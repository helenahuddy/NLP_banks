{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import conv\n",
    "#import data_helpers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "#import data_helpers\n",
    "import nltk\n",
    "import word2vec\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from gensim.models import word2vec\n",
    "from tensorflow.contrib import learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import data_helpers\n",
    "\n",
    "tweets='TREC_all.txt'\n",
    "labels='label_all.txt'\n",
    "\n",
    "X,y=data_helpers.load_tweets(tweets),data_helpers.load_1dsentiment(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words representing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in X])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x_bag = np.array(list(vocab_processor.fit_transform(X)))\n",
    "y_text = np.array([[sent, 1-sent] for sent in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class frequencies:\t [1639, 4653]\n",
      "Validation class frequencies:\t [782, 2318]\n",
      "Constant classifier's validation accuracy:\t 0.747741935483871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x_bag, y_text, test_size=0.33, random_state=42)\n",
    "\n",
    "print (\"Train class frequencies:\\t\", [col.nonzero()[0].shape[0] for col in y_train.transpose()])\n",
    "print (\"Validation class frequencies:\\t\", [col.nonzero()[0].shape[0] for col in y_dev.transpose()])\n",
    "print (\"Constant classifier's validation accuracy:\\t\", [col.nonzero()[0].shape[0] for col in y_dev.transpose()][1] * 1. / y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 50, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 20, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "BATCH_SIZE=50\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=20\n",
      "NUM_FILTERS=128\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/helenahuddy/CNN/runs/1508156672\n",
      "\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "WARNING:tensorflow:From <ipython-input-19-e877071d8246>:45: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "WARNING:tensorflow:From <ipython-input-19-e877071d8246>:48: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "2017-10-16T15:24:33.786149: step 1, loss 1.99794, acc 0.48\n",
      "2017-10-16T15:24:33.965246: step 2, loss 1.67133, acc 0.62\n",
      "2017-10-16T15:24:34.144599: step 3, loss 1.29331, acc 0.56\n",
      "2017-10-16T15:24:34.253115: step 4, loss 1.08593, acc 0.64\n",
      "2017-10-16T15:24:34.372892: step 5, loss 1.40711, acc 0.6\n",
      "2017-10-16T15:24:34.487709: step 6, loss 1.44321, acc 0.56\n",
      "2017-10-16T15:24:34.605171: step 7, loss 1.87714, acc 0.5\n",
      "2017-10-16T15:24:34.714060: step 8, loss 1.65081, acc 0.5\n",
      "2017-10-16T15:24:34.827519: step 9, loss 1.05365, acc 0.64\n",
      "2017-10-16T15:24:34.937492: step 10, loss 1.2275, acc 0.64\n",
      "2017-10-16T15:24:35.049588: step 11, loss 1.08502, acc 0.76\n",
      "2017-10-16T15:24:35.157937: step 12, loss 1.07223, acc 0.74\n",
      "2017-10-16T15:24:35.267158: step 13, loss 1.33893, acc 0.66\n",
      "2017-10-16T15:24:35.380297: step 14, loss 1.55071, acc 0.62\n",
      "2017-10-16T15:24:35.496079: step 15, loss 1.27821, acc 0.64\n",
      "2017-10-16T15:24:35.604068: step 16, loss 0.969285, acc 0.7\n",
      "2017-10-16T15:24:35.716521: step 17, loss 0.559891, acc 0.76\n",
      "2017-10-16T15:24:35.825691: step 18, loss 1.66961, acc 0.62\n",
      "2017-10-16T15:24:35.934814: step 19, loss 1.23276, acc 0.78\n",
      "2017-10-16T15:24:36.044077: step 20, loss 0.855393, acc 0.76\n",
      "2017-10-16T15:24:36.155419: step 21, loss 0.920216, acc 0.7\n",
      "2017-10-16T15:24:36.264961: step 22, loss 0.915392, acc 0.7\n",
      "2017-10-16T15:24:36.397108: step 23, loss 2.01224, acc 0.56\n",
      "2017-10-16T15:24:36.562837: step 24, loss 1.72798, acc 0.58\n",
      "2017-10-16T15:24:36.685649: step 25, loss 1.14111, acc 0.72\n",
      "2017-10-16T15:24:36.801967: step 26, loss 0.954013, acc 0.72\n",
      "2017-10-16T15:24:36.920183: step 27, loss 1.05216, acc 0.82\n",
      "2017-10-16T15:24:37.028733: step 28, loss 1.45927, acc 0.64\n",
      "2017-10-16T15:24:37.149150: step 29, loss 1.12722, acc 0.64\n",
      "2017-10-16T15:24:37.263954: step 30, loss 0.657383, acc 0.86\n",
      "2017-10-16T15:24:37.388451: step 31, loss 1.32622, acc 0.68\n",
      "2017-10-16T15:24:37.508189: step 32, loss 1.28871, acc 0.72\n",
      "2017-10-16T15:24:37.633764: step 33, loss 1.0843, acc 0.72\n",
      "2017-10-16T15:24:37.757378: step 34, loss 1.01209, acc 0.7\n",
      "2017-10-16T15:24:37.920188: step 35, loss 1.42348, acc 0.7\n",
      "2017-10-16T15:24:38.028297: step 36, loss 1.81093, acc 0.56\n",
      "2017-10-16T15:24:38.137310: step 37, loss 0.490391, acc 0.9\n",
      "2017-10-16T15:24:38.245018: step 38, loss 1.34668, acc 0.68\n",
      "2017-10-16T15:24:38.353829: step 39, loss 1.16049, acc 0.78\n",
      "2017-10-16T15:24:38.500440: step 40, loss 1.57254, acc 0.62\n",
      "2017-10-16T15:24:38.637934: step 41, loss 0.872911, acc 0.66\n",
      "2017-10-16T15:24:38.773472: step 42, loss 0.863366, acc 0.76\n",
      "2017-10-16T15:24:38.909106: step 43, loss 0.892325, acc 0.7\n",
      "2017-10-16T15:24:39.019151: step 44, loss 0.948031, acc 0.76\n",
      "2017-10-16T15:24:39.129265: step 45, loss 1.41468, acc 0.58\n",
      "2017-10-16T15:24:39.239039: step 46, loss 0.885288, acc 0.64\n",
      "2017-10-16T15:24:39.355297: step 47, loss 1.24189, acc 0.7\n",
      "2017-10-16T15:24:39.471179: step 48, loss 1.64165, acc 0.64\n",
      "2017-10-16T15:24:39.596240: step 49, loss 0.91538, acc 0.8\n",
      "2017-10-16T15:24:39.704634: step 50, loss 1.52458, acc 0.64\n",
      "2017-10-16T15:24:39.819209: step 51, loss 1.29343, acc 0.68\n",
      "2017-10-16T15:24:39.928293: step 52, loss 1.17646, acc 0.64\n",
      "2017-10-16T15:24:40.070903: step 53, loss 1.01457, acc 0.74\n",
      "2017-10-16T15:24:40.210489: step 54, loss 1.81464, acc 0.56\n",
      "2017-10-16T15:24:40.346538: step 55, loss 1.36758, acc 0.62\n",
      "2017-10-16T15:24:40.474081: step 56, loss 1.13781, acc 0.66\n",
      "2017-10-16T15:24:40.598920: step 57, loss 0.899814, acc 0.66\n",
      "2017-10-16T15:24:40.705532: step 58, loss 0.809883, acc 0.82\n",
      "2017-10-16T15:24:40.812283: step 59, loss 0.869265, acc 0.74\n",
      "2017-10-16T15:24:40.920072: step 60, loss 1.36844, acc 0.62\n",
      "2017-10-16T15:24:41.028919: step 61, loss 1.46335, acc 0.66\n",
      "2017-10-16T15:24:41.140871: step 62, loss 0.860601, acc 0.8\n",
      "2017-10-16T15:24:41.260361: step 63, loss 1.09246, acc 0.72\n",
      "2017-10-16T15:24:41.368809: step 64, loss 0.778851, acc 0.72\n",
      "2017-10-16T15:24:41.475766: step 65, loss 1.17653, acc 0.7\n",
      "2017-10-16T15:24:41.585520: step 66, loss 0.89923, acc 0.68\n",
      "2017-10-16T15:24:41.694887: step 67, loss 1.36048, acc 0.68\n",
      "2017-10-16T15:24:41.801111: step 68, loss 1.01978, acc 0.74\n",
      "2017-10-16T15:24:41.910596: step 69, loss 1.239, acc 0.7\n",
      "2017-10-16T15:24:42.029736: step 70, loss 1.52629, acc 0.64\n",
      "2017-10-16T15:24:42.169965: step 71, loss 1.17839, acc 0.68\n",
      "2017-10-16T15:24:42.300761: step 72, loss 0.86807, acc 0.64\n",
      "2017-10-16T15:24:42.452088: step 73, loss 1.21511, acc 0.68\n",
      "2017-10-16T15:24:42.637108: step 74, loss 0.970943, acc 0.72\n",
      "2017-10-16T15:24:42.783070: step 75, loss 0.917032, acc 0.76\n",
      "2017-10-16T15:24:42.890416: step 76, loss 0.763381, acc 0.72\n",
      "2017-10-16T15:24:43.045831: step 77, loss 1.41538, acc 0.68\n",
      "2017-10-16T15:24:43.188415: step 78, loss 1.08569, acc 0.74\n",
      "2017-10-16T15:24:43.345590: step 79, loss 0.561782, acc 0.82\n",
      "2017-10-16T15:24:43.485704: step 80, loss 1.59522, acc 0.62\n",
      "2017-10-16T15:24:43.638738: step 81, loss 1.05743, acc 0.72\n",
      "2017-10-16T15:24:43.764054: step 82, loss 0.520077, acc 0.8\n",
      "2017-10-16T15:24:43.877659: step 83, loss 1.03964, acc 0.7\n",
      "2017-10-16T15:24:43.986264: step 84, loss 1.33469, acc 0.7\n",
      "2017-10-16T15:24:44.096277: step 85, loss 0.811719, acc 0.8\n",
      "2017-10-16T15:24:44.205117: step 86, loss 1.27503, acc 0.68\n",
      "2017-10-16T15:24:44.322307: step 87, loss 1.09911, acc 0.7\n",
      "2017-10-16T15:24:44.438482: step 88, loss 1.22845, acc 0.8\n",
      "2017-10-16T15:24:44.561209: step 89, loss 1.29603, acc 0.64\n",
      "2017-10-16T15:24:44.672061: step 90, loss 0.658457, acc 0.72\n",
      "2017-10-16T15:24:44.783416: step 91, loss 1.15991, acc 0.7\n",
      "2017-10-16T15:24:44.893078: step 92, loss 1.16429, acc 0.76\n",
      "2017-10-16T15:24:45.010672: step 93, loss 0.604263, acc 0.78\n",
      "2017-10-16T15:24:45.118210: step 94, loss 1.04794, acc 0.72\n",
      "2017-10-16T15:24:45.228802: step 95, loss 1.09195, acc 0.72\n",
      "2017-10-16T15:24:45.335123: step 96, loss 1.18869, acc 0.66\n",
      "2017-10-16T15:24:45.443298: step 97, loss 1.12711, acc 0.72\n",
      "2017-10-16T15:24:45.558156: step 98, loss 0.798976, acc 0.72\n",
      "2017-10-16T15:24:45.671265: step 99, loss 1.31038, acc 0.66\n",
      "2017-10-16T15:24:45.778317: step 100, loss 1.00145, acc 0.72\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:24:48.308389: step 100, loss 0.460548, acc 0.802903\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-100\n",
      "\n",
      "2017-10-16T15:24:49.550547: step 101, loss 1.14263, acc 0.7\n",
      "2017-10-16T15:24:49.661028: step 102, loss 0.599638, acc 0.76\n",
      "2017-10-16T15:24:49.783538: step 103, loss 1.19978, acc 0.66\n",
      "2017-10-16T15:24:49.908055: step 104, loss 0.527551, acc 0.8\n",
      "2017-10-16T15:24:50.017354: step 105, loss 1.12629, acc 0.7\n",
      "2017-10-16T15:24:50.127004: step 106, loss 0.966808, acc 0.72\n",
      "2017-10-16T15:24:50.235570: step 107, loss 1.24191, acc 0.74\n",
      "2017-10-16T15:24:50.347149: step 108, loss 0.885377, acc 0.74\n",
      "2017-10-16T15:24:50.461340: step 109, loss 1.0472, acc 0.72\n",
      "2017-10-16T15:24:50.579880: step 110, loss 1.12781, acc 0.7\n",
      "2017-10-16T15:24:50.695079: step 111, loss 1.62159, acc 0.56\n",
      "2017-10-16T15:24:50.808615: step 112, loss 1.04024, acc 0.66\n",
      "2017-10-16T15:24:50.983305: step 113, loss 1.19171, acc 0.68\n",
      "2017-10-16T15:24:51.123556: step 114, loss 1.13719, acc 0.74\n",
      "2017-10-16T15:24:51.246033: step 115, loss 1.04687, acc 0.72\n",
      "2017-10-16T15:24:51.357255: step 116, loss 1.17724, acc 0.68\n",
      "2017-10-16T15:24:51.467402: step 117, loss 0.612316, acc 0.82\n",
      "2017-10-16T15:24:51.639143: step 118, loss 0.980313, acc 0.82\n",
      "2017-10-16T15:24:51.788416: step 119, loss 1.30696, acc 0.66\n",
      "2017-10-16T15:24:51.934012: step 120, loss 0.637189, acc 0.82\n",
      "2017-10-16T15:24:52.081699: step 121, loss 0.766424, acc 0.76\n",
      "2017-10-16T15:24:52.237345: step 122, loss 0.82648, acc 0.74\n",
      "2017-10-16T15:24:52.391242: step 123, loss 0.908644, acc 0.72\n",
      "2017-10-16T15:24:52.523532: step 124, loss 0.76304, acc 0.68\n",
      "2017-10-16T15:24:52.631941: step 125, loss 1.13288, acc 0.7\n",
      "2017-10-16T15:24:52.725555: step 126, loss 0.70088, acc 0.785714\n",
      "2017-10-16T15:24:52.840854: step 127, loss 0.842602, acc 0.76\n",
      "2017-10-16T15:24:52.964578: step 128, loss 1.00278, acc 0.76\n",
      "2017-10-16T15:24:53.121137: step 129, loss 0.998605, acc 0.66\n",
      "2017-10-16T15:24:53.252647: step 130, loss 0.856358, acc 0.74\n",
      "2017-10-16T15:24:53.371596: step 131, loss 1.18319, acc 0.72\n",
      "2017-10-16T15:24:53.502737: step 132, loss 1.03349, acc 0.66\n",
      "2017-10-16T15:24:53.636830: step 133, loss 0.90844, acc 0.74\n",
      "2017-10-16T15:24:53.761825: step 134, loss 0.654325, acc 0.82\n",
      "2017-10-16T15:24:53.901298: step 135, loss 0.978165, acc 0.68\n",
      "2017-10-16T15:24:54.045324: step 136, loss 0.801961, acc 0.7\n",
      "2017-10-16T15:24:54.175608: step 137, loss 0.847714, acc 0.72\n",
      "2017-10-16T15:24:54.301439: step 138, loss 1.21342, acc 0.64\n",
      "2017-10-16T15:24:54.428184: step 139, loss 1.33335, acc 0.72\n",
      "2017-10-16T15:24:54.548459: step 140, loss 0.84407, acc 0.72\n",
      "2017-10-16T15:24:54.693555: step 141, loss 0.807752, acc 0.78\n",
      "2017-10-16T15:24:54.818494: step 142, loss 0.914311, acc 0.76\n",
      "2017-10-16T15:24:54.980527: step 143, loss 0.802395, acc 0.8\n",
      "2017-10-16T15:24:55.107932: step 144, loss 0.881641, acc 0.68\n",
      "2017-10-16T15:24:55.247441: step 145, loss 0.720602, acc 0.8\n",
      "2017-10-16T15:24:55.406132: step 146, loss 0.657134, acc 0.82\n",
      "2017-10-16T15:24:55.551444: step 147, loss 1.26515, acc 0.74\n",
      "2017-10-16T15:24:55.671124: step 148, loss 1.02505, acc 0.8\n",
      "2017-10-16T15:24:55.785114: step 149, loss 0.745319, acc 0.78\n",
      "2017-10-16T15:24:55.952890: step 150, loss 0.665763, acc 0.8\n",
      "2017-10-16T15:24:56.079801: step 151, loss 1.41693, acc 0.72\n",
      "2017-10-16T15:24:56.190776: step 152, loss 0.987303, acc 0.74\n",
      "2017-10-16T15:24:56.323856: step 153, loss 0.60064, acc 0.88\n",
      "2017-10-16T15:24:56.460436: step 154, loss 1.21763, acc 0.66\n",
      "2017-10-16T15:24:56.588782: step 155, loss 0.831647, acc 0.78\n",
      "2017-10-16T15:24:56.763787: step 156, loss 1.22735, acc 0.66\n",
      "2017-10-16T15:24:56.908116: step 157, loss 1.41617, acc 0.68\n",
      "2017-10-16T15:24:57.017381: step 158, loss 1.07498, acc 0.7\n",
      "2017-10-16T15:24:57.129071: step 159, loss 0.96025, acc 0.74\n",
      "2017-10-16T15:24:57.236328: step 160, loss 0.676242, acc 0.8\n",
      "2017-10-16T15:24:57.378400: step 161, loss 0.760511, acc 0.76\n",
      "2017-10-16T15:24:57.501920: step 162, loss 0.818809, acc 0.74\n",
      "2017-10-16T15:24:57.620296: step 163, loss 0.459489, acc 0.82\n",
      "2017-10-16T15:24:57.731186: step 164, loss 0.77838, acc 0.74\n",
      "2017-10-16T15:24:57.873239: step 165, loss 0.664577, acc 0.8\n",
      "2017-10-16T15:24:58.012353: step 166, loss 1.13612, acc 0.72\n",
      "2017-10-16T15:24:58.187500: step 167, loss 0.974407, acc 0.78\n",
      "2017-10-16T15:24:58.345852: step 168, loss 1.30582, acc 0.7\n",
      "2017-10-16T15:24:58.469081: step 169, loss 0.752522, acc 0.78\n",
      "2017-10-16T15:24:58.588979: step 170, loss 0.643354, acc 0.76\n",
      "2017-10-16T15:24:58.728710: step 171, loss 0.930283, acc 0.74\n",
      "2017-10-16T15:24:58.860427: step 172, loss 1.16259, acc 0.76\n",
      "2017-10-16T15:24:59.007652: step 173, loss 1.53842, acc 0.64\n",
      "2017-10-16T15:24:59.189917: step 174, loss 0.672861, acc 0.8\n",
      "2017-10-16T15:24:59.344530: step 175, loss 0.997802, acc 0.76\n",
      "2017-10-16T15:24:59.454376: step 176, loss 1.13532, acc 0.7\n",
      "2017-10-16T15:24:59.569703: step 177, loss 1.01235, acc 0.76\n",
      "2017-10-16T15:24:59.710443: step 178, loss 0.533295, acc 0.84\n",
      "2017-10-16T15:24:59.831384: step 179, loss 1.54678, acc 0.64\n",
      "2017-10-16T15:24:59.946273: step 180, loss 0.652231, acc 0.82\n",
      "2017-10-16T15:25:00.094647: step 181, loss 0.711412, acc 0.82\n",
      "2017-10-16T15:25:00.242098: step 182, loss 1.40078, acc 0.72\n",
      "2017-10-16T15:25:00.421064: step 183, loss 0.825865, acc 0.8\n",
      "2017-10-16T15:25:00.589023: step 184, loss 1.01176, acc 0.74\n",
      "2017-10-16T15:25:00.724376: step 185, loss 0.713937, acc 0.8\n",
      "2017-10-16T15:25:00.855344: step 186, loss 0.640084, acc 0.78\n",
      "2017-10-16T15:25:00.968731: step 187, loss 1.65051, acc 0.6\n",
      "2017-10-16T15:25:01.078149: step 188, loss 0.797111, acc 0.8\n",
      "2017-10-16T15:25:01.216785: step 189, loss 0.760494, acc 0.78\n",
      "2017-10-16T15:25:01.370773: step 190, loss 1.05207, acc 0.7\n",
      "2017-10-16T15:25:01.534535: step 191, loss 1.18207, acc 0.7\n",
      "2017-10-16T15:25:01.681261: step 192, loss 0.829309, acc 0.78\n",
      "2017-10-16T15:25:01.822939: step 193, loss 1.19454, acc 0.78\n",
      "2017-10-16T15:25:01.979970: step 194, loss 0.961934, acc 0.7\n",
      "2017-10-16T15:25:02.130297: step 195, loss 1.07044, acc 0.74\n",
      "2017-10-16T15:25:02.275758: step 196, loss 0.457637, acc 0.84\n",
      "2017-10-16T15:25:02.444455: step 197, loss 1.15802, acc 0.7\n",
      "2017-10-16T15:25:02.573504: step 198, loss 0.905248, acc 0.72\n",
      "2017-10-16T15:25:02.709776: step 199, loss 1.43529, acc 0.68\n",
      "2017-10-16T15:25:02.855004: step 200, loss 0.753611, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:25:05.296804: step 200, loss 0.419539, acc 0.821935\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-200\n",
      "\n",
      "2017-10-16T15:25:05.955029: step 201, loss 0.922145, acc 0.7\n",
      "2017-10-16T15:25:06.070626: step 202, loss 0.825841, acc 0.8\n",
      "2017-10-16T15:25:06.178259: step 203, loss 0.645791, acc 0.76\n",
      "2017-10-16T15:25:06.288293: step 204, loss 1.513, acc 0.7\n",
      "2017-10-16T15:25:06.397316: step 205, loss 0.61091, acc 0.84\n",
      "2017-10-16T15:25:06.506619: step 206, loss 1.14868, acc 0.72\n",
      "2017-10-16T15:25:06.617481: step 207, loss 0.439656, acc 0.84\n",
      "2017-10-16T15:25:06.728372: step 208, loss 0.818278, acc 0.8\n",
      "2017-10-16T15:25:06.838314: step 209, loss 0.43988, acc 0.86\n",
      "2017-10-16T15:25:06.950852: step 210, loss 0.690758, acc 0.84\n",
      "2017-10-16T15:25:07.065673: step 211, loss 1.17363, acc 0.76\n",
      "2017-10-16T15:25:07.186591: step 212, loss 0.568409, acc 0.8\n",
      "2017-10-16T15:25:07.295080: step 213, loss 1.0816, acc 0.72\n",
      "2017-10-16T15:25:07.403673: step 214, loss 0.389193, acc 0.82\n",
      "2017-10-16T15:25:07.513989: step 215, loss 0.938375, acc 0.76\n",
      "2017-10-16T15:25:07.624699: step 216, loss 1.27806, acc 0.68\n",
      "2017-10-16T15:25:07.773015: step 217, loss 0.521841, acc 0.78\n",
      "2017-10-16T15:25:07.965564: step 218, loss 0.937612, acc 0.78\n",
      "2017-10-16T15:25:08.147880: step 219, loss 0.717294, acc 0.8\n",
      "2017-10-16T15:25:08.264171: step 220, loss 0.457547, acc 0.84\n",
      "2017-10-16T15:25:08.373129: step 221, loss 0.755847, acc 0.76\n",
      "2017-10-16T15:25:08.482251: step 222, loss 0.791015, acc 0.8\n",
      "2017-10-16T15:25:08.593512: step 223, loss 0.471278, acc 0.86\n",
      "2017-10-16T15:25:08.720372: step 224, loss 1.37236, acc 0.62\n",
      "2017-10-16T15:25:08.847312: step 225, loss 1.13473, acc 0.72\n",
      "2017-10-16T15:25:08.993186: step 226, loss 0.308094, acc 0.86\n",
      "2017-10-16T15:25:09.117013: step 227, loss 1.04364, acc 0.76\n",
      "2017-10-16T15:25:09.258685: step 228, loss 1.11796, acc 0.7\n",
      "2017-10-16T15:25:09.404593: step 229, loss 0.61442, acc 0.84\n",
      "2017-10-16T15:25:09.564026: step 230, loss 0.934603, acc 0.78\n",
      "2017-10-16T15:25:09.712444: step 231, loss 0.623907, acc 0.88\n",
      "2017-10-16T15:25:09.835378: step 232, loss 1, acc 0.72\n",
      "2017-10-16T15:25:09.947756: step 233, loss 1.47968, acc 0.64\n",
      "2017-10-16T15:25:10.057547: step 234, loss 0.899593, acc 0.78\n",
      "2017-10-16T15:25:10.167555: step 235, loss 0.603009, acc 0.82\n",
      "2017-10-16T15:25:10.280769: step 236, loss 1.05314, acc 0.78\n",
      "2017-10-16T15:25:10.392628: step 237, loss 0.599556, acc 0.82\n",
      "2017-10-16T15:25:10.505110: step 238, loss 1.0007, acc 0.72\n",
      "2017-10-16T15:25:10.613184: step 239, loss 0.601022, acc 0.8\n",
      "2017-10-16T15:25:10.726527: step 240, loss 0.727444, acc 0.8\n",
      "2017-10-16T15:25:10.838931: step 241, loss 1.20538, acc 0.76\n",
      "2017-10-16T15:25:10.968743: step 242, loss 1.36135, acc 0.66\n",
      "2017-10-16T15:25:11.084728: step 243, loss 0.936086, acc 0.76\n",
      "2017-10-16T15:25:11.196560: step 244, loss 0.765646, acc 0.72\n",
      "2017-10-16T15:25:11.304260: step 245, loss 0.650115, acc 0.8\n",
      "2017-10-16T15:25:11.411987: step 246, loss 0.844413, acc 0.82\n",
      "2017-10-16T15:25:11.524427: step 247, loss 1.33357, acc 0.72\n",
      "2017-10-16T15:25:11.662634: step 248, loss 1.22461, acc 0.72\n",
      "2017-10-16T15:25:11.774638: step 249, loss 0.371048, acc 0.84\n",
      "2017-10-16T15:25:11.894234: step 250, loss 0.821308, acc 0.78\n",
      "2017-10-16T15:25:12.005625: step 251, loss 0.910696, acc 0.66\n",
      "2017-10-16T15:25:12.100383: step 252, loss 0.486006, acc 0.809524\n",
      "2017-10-16T15:25:12.213148: step 253, loss 0.913345, acc 0.8\n",
      "2017-10-16T15:25:12.333221: step 254, loss 0.555504, acc 0.82\n",
      "2017-10-16T15:25:12.454866: step 255, loss 1.1232, acc 0.7\n",
      "2017-10-16T15:25:12.576480: step 256, loss 0.921086, acc 0.7\n",
      "2017-10-16T15:25:12.719800: step 257, loss 0.662955, acc 0.8\n",
      "2017-10-16T15:25:12.875888: step 258, loss 0.574233, acc 0.76\n",
      "2017-10-16T15:25:13.001809: step 259, loss 0.525779, acc 0.86\n",
      "2017-10-16T15:25:13.114163: step 260, loss 0.496652, acc 0.82\n",
      "2017-10-16T15:25:13.222074: step 261, loss 1.0362, acc 0.7\n",
      "2017-10-16T15:25:13.333955: step 262, loss 1.16697, acc 0.72\n",
      "2017-10-16T15:25:13.458891: step 263, loss 0.719587, acc 0.82\n",
      "2017-10-16T15:25:13.592504: step 264, loss 0.986517, acc 0.74\n",
      "2017-10-16T15:25:13.718842: step 265, loss 1.10221, acc 0.64\n",
      "2017-10-16T15:25:13.849518: step 266, loss 0.329993, acc 0.86\n",
      "2017-10-16T15:25:13.957718: step 267, loss 0.670984, acc 0.74\n",
      "2017-10-16T15:25:14.100525: step 268, loss 0.879382, acc 0.76\n",
      "2017-10-16T15:25:14.225626: step 269, loss 0.631273, acc 0.78\n",
      "2017-10-16T15:25:14.355702: step 270, loss 0.69016, acc 0.7\n",
      "2017-10-16T15:25:14.465678: step 271, loss 0.722425, acc 0.74\n",
      "2017-10-16T15:25:14.608436: step 272, loss 1.12864, acc 0.72\n",
      "2017-10-16T15:25:14.732988: step 273, loss 0.53321, acc 0.82\n",
      "2017-10-16T15:25:14.860432: step 274, loss 0.942799, acc 0.74\n",
      "2017-10-16T15:25:14.981241: step 275, loss 0.776376, acc 0.78\n",
      "2017-10-16T15:25:15.113479: step 276, loss 0.277435, acc 0.9\n",
      "2017-10-16T15:25:15.238486: step 277, loss 0.586088, acc 0.78\n",
      "2017-10-16T15:25:15.372594: step 278, loss 0.963403, acc 0.7\n",
      "2017-10-16T15:25:15.502122: step 279, loss 0.636072, acc 0.78\n",
      "2017-10-16T15:25:15.619103: step 280, loss 0.674153, acc 0.78\n",
      "2017-10-16T15:25:15.743356: step 281, loss 0.517099, acc 0.82\n",
      "2017-10-16T15:25:15.878983: step 282, loss 1.11674, acc 0.72\n",
      "2017-10-16T15:25:16.005375: step 283, loss 0.593464, acc 0.8\n",
      "2017-10-16T15:25:16.138299: step 284, loss 0.939528, acc 0.76\n",
      "2017-10-16T15:25:16.246709: step 285, loss 1.01529, acc 0.74\n",
      "2017-10-16T15:25:16.381667: step 286, loss 0.672701, acc 0.82\n",
      "2017-10-16T15:25:16.518749: step 287, loss 0.796176, acc 0.78\n",
      "2017-10-16T15:25:16.688997: step 288, loss 0.598496, acc 0.84\n",
      "2017-10-16T15:25:16.814455: step 289, loss 0.67003, acc 0.82\n",
      "2017-10-16T15:25:16.961200: step 290, loss 0.558772, acc 0.84\n",
      "2017-10-16T15:25:17.104922: step 291, loss 0.884472, acc 0.78\n",
      "2017-10-16T15:25:17.266878: step 292, loss 0.75588, acc 0.72\n",
      "2017-10-16T15:25:17.378252: step 293, loss 0.726066, acc 0.82\n",
      "2017-10-16T15:25:17.488159: step 294, loss 0.811455, acc 0.68\n",
      "2017-10-16T15:25:17.600086: step 295, loss 1.03216, acc 0.72\n",
      "2017-10-16T15:25:17.736577: step 296, loss 1.38903, acc 0.68\n",
      "2017-10-16T15:25:17.872800: step 297, loss 0.790082, acc 0.78\n",
      "2017-10-16T15:25:17.999546: step 298, loss 0.729337, acc 0.76\n",
      "2017-10-16T15:25:18.108294: step 299, loss 0.569699, acc 0.84\n",
      "2017-10-16T15:25:18.226051: step 300, loss 1.04598, acc 0.7\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:25:20.636746: step 300, loss 0.415684, acc 0.829677\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-300\n",
      "\n",
      "2017-10-16T15:25:21.571304: step 301, loss 0.643022, acc 0.78\n",
      "2017-10-16T15:25:21.688416: step 302, loss 1.17499, acc 0.66\n",
      "2017-10-16T15:25:21.821747: step 303, loss 0.979243, acc 0.72\n",
      "2017-10-16T15:25:21.953817: step 304, loss 0.736128, acc 0.78\n",
      "2017-10-16T15:25:22.110162: step 305, loss 1.07142, acc 0.74\n",
      "2017-10-16T15:25:22.243139: step 306, loss 0.613607, acc 0.8\n",
      "2017-10-16T15:25:22.356751: step 307, loss 0.686857, acc 0.74\n",
      "2017-10-16T15:25:22.475212: step 308, loss 0.621256, acc 0.8\n",
      "2017-10-16T15:25:22.585818: step 309, loss 0.872992, acc 0.74\n",
      "2017-10-16T15:25:22.702799: step 310, loss 0.954523, acc 0.76\n",
      "2017-10-16T15:25:22.831089: step 311, loss 0.751169, acc 0.82\n",
      "2017-10-16T15:25:22.956842: step 312, loss 0.857292, acc 0.76\n",
      "2017-10-16T15:25:23.080095: step 313, loss 0.655254, acc 0.8\n",
      "2017-10-16T15:25:23.199198: step 314, loss 0.619635, acc 0.82\n",
      "2017-10-16T15:25:23.307541: step 315, loss 0.556128, acc 0.84\n",
      "2017-10-16T15:25:23.416335: step 316, loss 0.775578, acc 0.8\n",
      "2017-10-16T15:25:23.524338: step 317, loss 1.18241, acc 0.78\n",
      "2017-10-16T15:25:23.635881: step 318, loss 0.470527, acc 0.76\n",
      "2017-10-16T15:25:23.745092: step 319, loss 0.468862, acc 0.8\n",
      "2017-10-16T15:25:23.854455: step 320, loss 0.834958, acc 0.82\n",
      "2017-10-16T15:25:23.966288: step 321, loss 0.51226, acc 0.86\n",
      "2017-10-16T15:25:24.078133: step 322, loss 0.645658, acc 0.72\n",
      "2017-10-16T15:25:24.186610: step 323, loss 0.836333, acc 0.8\n",
      "2017-10-16T15:25:24.298273: step 324, loss 0.813377, acc 0.78\n",
      "2017-10-16T15:25:24.406128: step 325, loss 0.592343, acc 0.84\n",
      "2017-10-16T15:25:24.515044: step 326, loss 0.511789, acc 0.82\n",
      "2017-10-16T15:25:24.626830: step 327, loss 0.941746, acc 0.72\n",
      "2017-10-16T15:25:24.737722: step 328, loss 0.628046, acc 0.82\n",
      "2017-10-16T15:25:24.846891: step 329, loss 0.720544, acc 0.8\n",
      "2017-10-16T15:25:24.958313: step 330, loss 0.796289, acc 0.78\n",
      "2017-10-16T15:25:25.067589: step 331, loss 0.713962, acc 0.8\n",
      "2017-10-16T15:25:25.178987: step 332, loss 0.43253, acc 0.86\n",
      "2017-10-16T15:25:25.287412: step 333, loss 0.680375, acc 0.78\n",
      "2017-10-16T15:25:25.395631: step 334, loss 0.667322, acc 0.78\n",
      "2017-10-16T15:25:25.505679: step 335, loss 0.327156, acc 0.9\n",
      "2017-10-16T15:25:25.621757: step 336, loss 0.389719, acc 0.88\n",
      "2017-10-16T15:25:25.732789: step 337, loss 0.452065, acc 0.86\n",
      "2017-10-16T15:25:25.843815: step 338, loss 0.666848, acc 0.8\n",
      "2017-10-16T15:25:25.952763: step 339, loss 0.676316, acc 0.8\n",
      "2017-10-16T15:25:26.062592: step 340, loss 0.882873, acc 0.8\n",
      "2017-10-16T15:25:26.170200: step 341, loss 0.518244, acc 0.78\n",
      "2017-10-16T15:25:26.278492: step 342, loss 0.846548, acc 0.74\n",
      "2017-10-16T15:25:26.390064: step 343, loss 0.638518, acc 0.8\n",
      "2017-10-16T15:25:26.502784: step 344, loss 0.770075, acc 0.74\n",
      "2017-10-16T15:25:26.611716: step 345, loss 0.872252, acc 0.78\n",
      "2017-10-16T15:25:26.721274: step 346, loss 1.37945, acc 0.68\n",
      "2017-10-16T15:25:26.844211: step 347, loss 0.955622, acc 0.76\n",
      "2017-10-16T15:25:26.983903: step 348, loss 0.695223, acc 0.8\n",
      "2017-10-16T15:25:27.107983: step 349, loss 0.414871, acc 0.84\n",
      "2017-10-16T15:25:27.222328: step 350, loss 0.629426, acc 0.82\n",
      "2017-10-16T15:25:27.331933: step 351, loss 0.827145, acc 0.78\n",
      "2017-10-16T15:25:27.495559: step 352, loss 0.696948, acc 0.82\n",
      "2017-10-16T15:25:27.654542: step 353, loss 0.693582, acc 0.8\n",
      "2017-10-16T15:25:27.820585: step 354, loss 0.66282, acc 0.74\n",
      "2017-10-16T15:25:27.957426: step 355, loss 0.718803, acc 0.74\n",
      "2017-10-16T15:25:28.102828: step 356, loss 1.0618, acc 0.84\n",
      "2017-10-16T15:25:28.241401: step 357, loss 0.523148, acc 0.82\n",
      "2017-10-16T15:25:28.362649: step 358, loss 0.657385, acc 0.82\n",
      "2017-10-16T15:25:28.471306: step 359, loss 0.640001, acc 0.88\n",
      "2017-10-16T15:25:28.639439: step 360, loss 0.424755, acc 0.84\n",
      "2017-10-16T15:25:28.778851: step 361, loss 0.833774, acc 0.72\n",
      "2017-10-16T15:25:28.909020: step 362, loss 0.640748, acc 0.82\n",
      "2017-10-16T15:25:29.026312: step 363, loss 0.641606, acc 0.76\n",
      "2017-10-16T15:25:29.140980: step 364, loss 0.658752, acc 0.82\n",
      "2017-10-16T15:25:29.251381: step 365, loss 0.721467, acc 0.84\n",
      "2017-10-16T15:25:29.360359: step 366, loss 0.827927, acc 0.7\n",
      "2017-10-16T15:25:29.468540: step 367, loss 0.531465, acc 0.8\n",
      "2017-10-16T15:25:29.587259: step 368, loss 0.559441, acc 0.8\n",
      "2017-10-16T15:25:29.701617: step 369, loss 0.824779, acc 0.78\n",
      "2017-10-16T15:25:29.815613: step 370, loss 0.537366, acc 0.78\n",
      "2017-10-16T15:25:29.922782: step 371, loss 0.821044, acc 0.8\n",
      "2017-10-16T15:25:30.030918: step 372, loss 0.754753, acc 0.78\n",
      "2017-10-16T15:25:30.138011: step 373, loss 0.566399, acc 0.76\n",
      "2017-10-16T15:25:30.247188: step 374, loss 0.39797, acc 0.8\n",
      "2017-10-16T15:25:30.355075: step 375, loss 0.551266, acc 0.82\n",
      "2017-10-16T15:25:30.461918: step 376, loss 0.934352, acc 0.72\n",
      "2017-10-16T15:25:30.569111: step 377, loss 0.723043, acc 0.72\n",
      "2017-10-16T15:25:30.661567: step 378, loss 0.725757, acc 0.761905\n",
      "2017-10-16T15:25:30.774664: step 379, loss 0.602338, acc 0.88\n",
      "2017-10-16T15:25:30.882602: step 380, loss 1.36763, acc 0.66\n",
      "2017-10-16T15:25:30.999092: step 381, loss 0.748557, acc 0.76\n",
      "2017-10-16T15:25:31.112596: step 382, loss 0.938598, acc 0.72\n",
      "2017-10-16T15:25:31.223276: step 383, loss 0.503059, acc 0.82\n",
      "2017-10-16T15:25:31.332846: step 384, loss 1.19603, acc 0.72\n",
      "2017-10-16T15:25:31.500491: step 385, loss 0.613817, acc 0.8\n",
      "2017-10-16T15:25:31.658290: step 386, loss 0.769161, acc 0.78\n",
      "2017-10-16T15:25:31.813963: step 387, loss 1.31661, acc 0.68\n",
      "2017-10-16T15:25:31.960587: step 388, loss 0.242035, acc 0.9\n",
      "2017-10-16T15:25:32.078173: step 389, loss 0.669156, acc 0.82\n",
      "2017-10-16T15:25:32.234599: step 390, loss 0.570907, acc 0.8\n",
      "2017-10-16T15:25:32.424677: step 391, loss 0.442357, acc 0.78\n",
      "2017-10-16T15:25:32.565967: step 392, loss 0.564772, acc 0.82\n",
      "2017-10-16T15:25:32.707622: step 393, loss 0.664936, acc 0.8\n",
      "2017-10-16T15:25:32.847594: step 394, loss 0.771669, acc 0.84\n",
      "2017-10-16T15:25:32.992033: step 395, loss 1.02929, acc 0.74\n",
      "2017-10-16T15:25:33.127612: step 396, loss 0.710407, acc 0.74\n",
      "2017-10-16T15:25:33.270797: step 397, loss 0.617168, acc 0.76\n",
      "2017-10-16T15:25:33.410355: step 398, loss 0.645754, acc 0.78\n",
      "2017-10-16T15:25:33.571117: step 399, loss 0.896636, acc 0.72\n",
      "2017-10-16T15:25:33.713715: step 400, loss 0.634295, acc 0.74\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:25:35.894921: step 400, loss 0.393495, acc 0.835806\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-400\n",
      "\n",
      "2017-10-16T15:25:36.706105: step 401, loss 0.41984, acc 0.9\n",
      "2017-10-16T15:25:36.816036: step 402, loss 0.569349, acc 0.84\n",
      "2017-10-16T15:25:36.923637: step 403, loss 0.73142, acc 0.78\n",
      "2017-10-16T15:25:37.032175: step 404, loss 0.562098, acc 0.86\n",
      "2017-10-16T15:25:37.138947: step 405, loss 1.06886, acc 0.72\n",
      "2017-10-16T15:25:37.249608: step 406, loss 0.450146, acc 0.84\n",
      "2017-10-16T15:25:37.359513: step 407, loss 0.548022, acc 0.8\n",
      "2017-10-16T15:25:37.470074: step 408, loss 0.377161, acc 0.84\n",
      "2017-10-16T15:25:37.582751: step 409, loss 0.907495, acc 0.78\n",
      "2017-10-16T15:25:37.692947: step 410, loss 0.851108, acc 0.74\n",
      "2017-10-16T15:25:37.801564: step 411, loss 0.785282, acc 0.7\n",
      "2017-10-16T15:25:37.910785: step 412, loss 0.614272, acc 0.88\n",
      "2017-10-16T15:25:38.023232: step 413, loss 0.533732, acc 0.8\n",
      "2017-10-16T15:25:38.135675: step 414, loss 1.14746, acc 0.74\n",
      "2017-10-16T15:25:38.242872: step 415, loss 0.693043, acc 0.78\n",
      "2017-10-16T15:25:38.352940: step 416, loss 0.85541, acc 0.8\n",
      "2017-10-16T15:25:38.459342: step 417, loss 1.04822, acc 0.7\n",
      "2017-10-16T15:25:38.568092: step 418, loss 0.246626, acc 0.9\n",
      "2017-10-16T15:25:38.675105: step 419, loss 0.847826, acc 0.7\n",
      "2017-10-16T15:25:38.786126: step 420, loss 0.384506, acc 0.86\n",
      "2017-10-16T15:25:38.894214: step 421, loss 0.526293, acc 0.84\n",
      "2017-10-16T15:25:39.005315: step 422, loss 0.42101, acc 0.9\n",
      "2017-10-16T15:25:39.114066: step 423, loss 0.674244, acc 0.82\n",
      "2017-10-16T15:25:39.224273: step 424, loss 0.515072, acc 0.82\n",
      "2017-10-16T15:25:39.331671: step 425, loss 0.643068, acc 0.8\n",
      "2017-10-16T15:25:39.439331: step 426, loss 0.423907, acc 0.9\n",
      "2017-10-16T15:25:39.546001: step 427, loss 0.40674, acc 0.86\n",
      "2017-10-16T15:25:39.656003: step 428, loss 0.268382, acc 0.9\n",
      "2017-10-16T15:25:39.763517: step 429, loss 0.860766, acc 0.78\n",
      "2017-10-16T15:25:39.872440: step 430, loss 0.996228, acc 0.76\n",
      "2017-10-16T15:25:39.978709: step 431, loss 0.94339, acc 0.72\n",
      "2017-10-16T15:25:40.085906: step 432, loss 0.872803, acc 0.74\n",
      "2017-10-16T15:25:40.192900: step 433, loss 0.652962, acc 0.84\n",
      "2017-10-16T15:25:40.301027: step 434, loss 0.413187, acc 0.88\n",
      "2017-10-16T15:25:40.408016: step 435, loss 0.780756, acc 0.8\n",
      "2017-10-16T15:25:40.517988: step 436, loss 0.716391, acc 0.76\n",
      "2017-10-16T15:25:40.625638: step 437, loss 0.86934, acc 0.74\n",
      "2017-10-16T15:25:40.733801: step 438, loss 0.713549, acc 0.76\n",
      "2017-10-16T15:25:40.841124: step 439, loss 0.996853, acc 0.7\n",
      "2017-10-16T15:25:40.950322: step 440, loss 0.670983, acc 0.78\n",
      "2017-10-16T15:25:41.059353: step 441, loss 0.744212, acc 0.78\n",
      "2017-10-16T15:25:41.170757: step 442, loss 0.917925, acc 0.76\n",
      "2017-10-16T15:25:41.276979: step 443, loss 0.793602, acc 0.8\n",
      "2017-10-16T15:25:41.385637: step 444, loss 0.932424, acc 0.8\n",
      "2017-10-16T15:25:41.493482: step 445, loss 0.588039, acc 0.8\n",
      "2017-10-16T15:25:41.602451: step 446, loss 0.373415, acc 0.8\n",
      "2017-10-16T15:25:41.709907: step 447, loss 1.18981, acc 0.72\n",
      "2017-10-16T15:25:41.816549: step 448, loss 0.721117, acc 0.76\n",
      "2017-10-16T15:25:41.925574: step 449, loss 0.709458, acc 0.8\n",
      "2017-10-16T15:25:42.044234: step 450, loss 1.04427, acc 0.68\n",
      "2017-10-16T15:25:42.152392: step 451, loss 0.82947, acc 0.76\n",
      "2017-10-16T15:25:42.261700: step 452, loss 0.834622, acc 0.76\n",
      "2017-10-16T15:25:42.379668: step 453, loss 1.06129, acc 0.76\n",
      "2017-10-16T15:25:42.515031: step 454, loss 0.799708, acc 0.8\n",
      "2017-10-16T15:25:42.622831: step 455, loss 1.02413, acc 0.72\n",
      "2017-10-16T15:25:42.749226: step 456, loss 0.745764, acc 0.82\n",
      "2017-10-16T15:25:42.867748: step 457, loss 0.898247, acc 0.68\n",
      "2017-10-16T15:25:42.979765: step 458, loss 0.712396, acc 0.82\n",
      "2017-10-16T15:25:43.086603: step 459, loss 0.534546, acc 0.82\n",
      "2017-10-16T15:25:43.195066: step 460, loss 0.643276, acc 0.78\n",
      "2017-10-16T15:25:43.303724: step 461, loss 0.560792, acc 0.78\n",
      "2017-10-16T15:25:43.417871: step 462, loss 1.0104, acc 0.74\n",
      "2017-10-16T15:25:43.525642: step 463, loss 0.504859, acc 0.88\n",
      "2017-10-16T15:25:43.634957: step 464, loss 1.05981, acc 0.76\n",
      "2017-10-16T15:25:43.741885: step 465, loss 0.885894, acc 0.68\n",
      "2017-10-16T15:25:43.851031: step 466, loss 0.505293, acc 0.8\n",
      "2017-10-16T15:25:43.958793: step 467, loss 1.08304, acc 0.7\n",
      "2017-10-16T15:25:44.069205: step 468, loss 0.258655, acc 0.86\n",
      "2017-10-16T15:25:44.186284: step 469, loss 1.05347, acc 0.72\n",
      "2017-10-16T15:25:44.318323: step 470, loss 0.569005, acc 0.78\n",
      "2017-10-16T15:25:44.449238: step 471, loss 0.514672, acc 0.9\n",
      "2017-10-16T15:25:44.591349: step 472, loss 0.914881, acc 0.76\n",
      "2017-10-16T15:25:44.731368: step 473, loss 0.755783, acc 0.82\n",
      "2017-10-16T15:25:44.873285: step 474, loss 0.789343, acc 0.82\n",
      "2017-10-16T15:25:44.995268: step 475, loss 0.420541, acc 0.84\n",
      "2017-10-16T15:25:45.108651: step 476, loss 0.858384, acc 0.8\n",
      "2017-10-16T15:25:45.219850: step 477, loss 0.880863, acc 0.78\n",
      "2017-10-16T15:25:45.330975: step 478, loss 0.586789, acc 0.84\n",
      "2017-10-16T15:25:45.438580: step 479, loss 0.66714, acc 0.82\n",
      "2017-10-16T15:25:45.547416: step 480, loss 0.993342, acc 0.68\n",
      "2017-10-16T15:25:45.656093: step 481, loss 0.695037, acc 0.78\n",
      "2017-10-16T15:25:45.765641: step 482, loss 0.416355, acc 0.86\n",
      "2017-10-16T15:25:45.873109: step 483, loss 0.718113, acc 0.78\n",
      "2017-10-16T15:25:45.980623: step 484, loss 0.252623, acc 0.9\n",
      "2017-10-16T15:25:46.087978: step 485, loss 0.562525, acc 0.8\n",
      "2017-10-16T15:25:46.195922: step 486, loss 1.1768, acc 0.74\n",
      "2017-10-16T15:25:46.303875: step 487, loss 0.869086, acc 0.68\n",
      "2017-10-16T15:25:46.411961: step 488, loss 0.601738, acc 0.88\n",
      "2017-10-16T15:25:46.519014: step 489, loss 1.10757, acc 0.74\n",
      "2017-10-16T15:25:46.634709: step 490, loss 1.00676, acc 0.72\n",
      "2017-10-16T15:25:46.749734: step 491, loss 0.904901, acc 0.78\n",
      "2017-10-16T15:25:46.858696: step 492, loss 0.653233, acc 0.84\n",
      "2017-10-16T15:25:46.967182: step 493, loss 0.823511, acc 0.8\n",
      "2017-10-16T15:25:47.075220: step 494, loss 0.37206, acc 0.88\n",
      "2017-10-16T15:25:47.182790: step 495, loss 0.379678, acc 0.82\n",
      "2017-10-16T15:25:47.291559: step 496, loss 0.746065, acc 0.78\n",
      "2017-10-16T15:25:47.399270: step 497, loss 0.918592, acc 0.76\n",
      "2017-10-16T15:25:47.509001: step 498, loss 0.825415, acc 0.7\n",
      "2017-10-16T15:25:47.617202: step 499, loss 0.997165, acc 0.68\n",
      "2017-10-16T15:25:47.731096: step 500, loss 0.517703, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:25:49.543335: step 500, loss 0.386594, acc 0.836129\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-500\n",
      "\n",
      "2017-10-16T15:25:50.328943: step 501, loss 0.726276, acc 0.78\n",
      "2017-10-16T15:25:50.441401: step 502, loss 0.771392, acc 0.74\n",
      "2017-10-16T15:25:50.549862: step 503, loss 0.418102, acc 0.86\n",
      "2017-10-16T15:25:50.643231: step 504, loss 0.418596, acc 0.833333\n",
      "2017-10-16T15:25:50.757976: step 505, loss 0.433871, acc 0.84\n",
      "2017-10-16T15:25:50.868613: step 506, loss 1.55628, acc 0.7\n",
      "2017-10-16T15:25:50.981286: step 507, loss 0.830138, acc 0.78\n",
      "2017-10-16T15:25:51.100323: step 508, loss 0.595374, acc 0.82\n",
      "2017-10-16T15:25:51.209947: step 509, loss 0.450653, acc 0.82\n",
      "2017-10-16T15:25:51.319174: step 510, loss 1.01692, acc 0.72\n",
      "2017-10-16T15:25:51.427505: step 511, loss 0.955547, acc 0.72\n",
      "2017-10-16T15:25:51.545294: step 512, loss 0.917289, acc 0.7\n",
      "2017-10-16T15:25:51.657346: step 513, loss 0.680844, acc 0.86\n",
      "2017-10-16T15:25:51.767889: step 514, loss 0.658032, acc 0.8\n",
      "2017-10-16T15:25:51.876050: step 515, loss 0.613441, acc 0.84\n",
      "2017-10-16T15:25:51.985954: step 516, loss 0.880454, acc 0.74\n",
      "2017-10-16T15:25:52.093483: step 517, loss 0.402158, acc 0.86\n",
      "2017-10-16T15:25:52.202751: step 518, loss 0.895965, acc 0.74\n",
      "2017-10-16T15:25:52.309715: step 519, loss 0.587596, acc 0.82\n",
      "2017-10-16T15:25:52.418345: step 520, loss 0.819476, acc 0.82\n",
      "2017-10-16T15:25:52.527700: step 521, loss 0.720266, acc 0.84\n",
      "2017-10-16T15:25:52.639867: step 522, loss 0.475741, acc 0.84\n",
      "2017-10-16T15:25:52.748339: step 523, loss 0.479906, acc 0.8\n",
      "2017-10-16T15:25:52.861061: step 524, loss 0.730632, acc 0.76\n",
      "2017-10-16T15:25:52.970448: step 525, loss 0.401794, acc 0.88\n",
      "2017-10-16T15:25:53.160003: step 526, loss 0.820728, acc 0.72\n",
      "2017-10-16T15:25:53.320925: step 527, loss 0.669653, acc 0.76\n",
      "2017-10-16T15:25:53.435106: step 528, loss 0.572012, acc 0.8\n",
      "2017-10-16T15:25:53.545275: step 529, loss 0.340791, acc 0.86\n",
      "2017-10-16T15:25:53.659605: step 530, loss 0.732624, acc 0.82\n",
      "2017-10-16T15:25:53.768908: step 531, loss 0.467329, acc 0.8\n",
      "2017-10-16T15:25:53.879825: step 532, loss 0.816999, acc 0.76\n",
      "2017-10-16T15:25:53.988994: step 533, loss 0.748773, acc 0.76\n",
      "2017-10-16T15:25:54.098384: step 534, loss 0.679145, acc 0.82\n",
      "2017-10-16T15:25:54.206658: step 535, loss 0.48158, acc 0.74\n",
      "2017-10-16T15:25:54.314364: step 536, loss 0.739377, acc 0.8\n",
      "2017-10-16T15:25:54.421945: step 537, loss 0.444655, acc 0.84\n",
      "2017-10-16T15:25:54.530388: step 538, loss 0.949066, acc 0.74\n",
      "2017-10-16T15:25:54.638580: step 539, loss 1.10184, acc 0.72\n",
      "2017-10-16T15:25:54.745692: step 540, loss 0.629835, acc 0.78\n",
      "2017-10-16T15:25:54.852974: step 541, loss 0.450778, acc 0.82\n",
      "2017-10-16T15:25:54.960110: step 542, loss 0.414911, acc 0.86\n",
      "2017-10-16T15:25:55.067424: step 543, loss 0.792983, acc 0.76\n",
      "2017-10-16T15:25:55.175265: step 544, loss 0.970312, acc 0.72\n",
      "2017-10-16T15:25:55.283213: step 545, loss 0.717371, acc 0.72\n",
      "2017-10-16T15:25:55.392775: step 546, loss 0.70414, acc 0.82\n",
      "2017-10-16T15:25:55.501667: step 547, loss 0.514919, acc 0.8\n",
      "2017-10-16T15:25:55.612461: step 548, loss 0.508396, acc 0.82\n",
      "2017-10-16T15:25:55.721098: step 549, loss 0.808818, acc 0.84\n",
      "2017-10-16T15:25:55.831555: step 550, loss 1.0069, acc 0.78\n",
      "2017-10-16T15:25:55.938504: step 551, loss 0.697695, acc 0.82\n",
      "2017-10-16T15:25:56.044734: step 552, loss 0.514394, acc 0.82\n",
      "2017-10-16T15:25:56.151926: step 553, loss 0.638622, acc 0.84\n",
      "2017-10-16T15:25:56.259669: step 554, loss 0.307527, acc 0.86\n",
      "2017-10-16T15:25:56.367677: step 555, loss 0.813829, acc 0.8\n",
      "2017-10-16T15:25:56.477891: step 556, loss 0.624042, acc 0.78\n",
      "2017-10-16T15:25:56.587602: step 557, loss 0.593382, acc 0.78\n",
      "2017-10-16T15:25:56.698909: step 558, loss 0.45641, acc 0.82\n",
      "2017-10-16T15:25:56.858163: step 559, loss 0.634877, acc 0.82\n",
      "2017-10-16T15:25:56.986264: step 560, loss 0.480677, acc 0.84\n",
      "2017-10-16T15:25:57.094014: step 561, loss 0.992378, acc 0.74\n",
      "2017-10-16T15:25:57.203651: step 562, loss 0.800419, acc 0.68\n",
      "2017-10-16T15:25:57.310755: step 563, loss 0.493444, acc 0.84\n",
      "2017-10-16T15:25:57.424812: step 564, loss 1.11601, acc 0.74\n",
      "2017-10-16T15:25:57.601270: step 565, loss 0.520947, acc 0.8\n",
      "2017-10-16T15:25:57.763190: step 566, loss 0.498841, acc 0.82\n",
      "2017-10-16T15:25:57.907165: step 567, loss 0.834406, acc 0.86\n",
      "2017-10-16T15:25:58.042239: step 568, loss 1.18414, acc 0.68\n",
      "2017-10-16T15:25:58.154031: step 569, loss 0.858388, acc 0.7\n",
      "2017-10-16T15:25:58.263114: step 570, loss 0.74104, acc 0.76\n",
      "2017-10-16T15:25:58.369920: step 571, loss 0.791682, acc 0.78\n",
      "2017-10-16T15:25:58.478067: step 572, loss 0.5947, acc 0.8\n",
      "2017-10-16T15:25:58.624276: step 573, loss 0.736176, acc 0.78\n",
      "2017-10-16T15:25:58.790376: step 574, loss 0.49451, acc 0.86\n",
      "2017-10-16T15:25:58.898506: step 575, loss 0.322012, acc 0.92\n",
      "2017-10-16T15:25:59.008291: step 576, loss 0.884549, acc 0.74\n",
      "2017-10-16T15:25:59.116755: step 577, loss 0.651069, acc 0.8\n",
      "2017-10-16T15:25:59.226568: step 578, loss 0.919907, acc 0.78\n",
      "2017-10-16T15:25:59.335479: step 579, loss 1.03239, acc 0.76\n",
      "2017-10-16T15:25:59.446313: step 580, loss 0.383744, acc 0.86\n",
      "2017-10-16T15:25:59.555864: step 581, loss 1.19261, acc 0.68\n",
      "2017-10-16T15:25:59.667453: step 582, loss 0.569472, acc 0.82\n",
      "2017-10-16T15:25:59.778310: step 583, loss 0.330735, acc 0.88\n",
      "2017-10-16T15:25:59.893859: step 584, loss 0.612459, acc 0.84\n",
      "2017-10-16T15:26:00.007247: step 585, loss 0.550353, acc 0.84\n",
      "2017-10-16T15:26:00.121859: step 586, loss 0.266753, acc 0.88\n",
      "2017-10-16T15:26:00.232337: step 587, loss 0.576308, acc 0.78\n",
      "2017-10-16T15:26:00.340702: step 588, loss 0.918252, acc 0.72\n",
      "2017-10-16T15:26:00.457526: step 589, loss 0.768616, acc 0.78\n",
      "2017-10-16T15:26:00.602908: step 590, loss 0.880384, acc 0.76\n",
      "2017-10-16T15:26:00.784177: step 591, loss 0.590884, acc 0.82\n",
      "2017-10-16T15:26:00.936291: step 592, loss 0.528458, acc 0.84\n",
      "2017-10-16T15:26:01.081401: step 593, loss 1.11113, acc 0.76\n",
      "2017-10-16T15:26:01.214774: step 594, loss 0.913074, acc 0.74\n",
      "2017-10-16T15:26:01.343161: step 595, loss 0.745794, acc 0.78\n",
      "2017-10-16T15:26:01.472794: step 596, loss 0.811364, acc 0.74\n",
      "2017-10-16T15:26:01.600228: step 597, loss 0.522753, acc 0.84\n",
      "2017-10-16T15:26:01.728073: step 598, loss 0.454403, acc 0.84\n",
      "2017-10-16T15:26:01.856354: step 599, loss 1.00669, acc 0.7\n",
      "2017-10-16T15:26:01.978277: step 600, loss 0.384007, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:26:04.141116: step 600, loss 0.386515, acc 0.834194\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-600\n",
      "\n",
      "2017-10-16T15:26:04.885341: step 601, loss 0.408749, acc 0.82\n",
      "2017-10-16T15:26:05.009222: step 602, loss 0.750098, acc 0.78\n",
      "2017-10-16T15:26:05.122675: step 603, loss 0.744834, acc 0.74\n",
      "2017-10-16T15:26:05.251729: step 604, loss 1.02564, acc 0.74\n",
      "2017-10-16T15:26:05.366735: step 605, loss 0.636451, acc 0.78\n",
      "2017-10-16T15:26:05.507790: step 606, loss 0.479762, acc 0.8\n",
      "2017-10-16T15:26:05.619550: step 607, loss 0.710805, acc 0.74\n",
      "2017-10-16T15:26:05.758135: step 608, loss 0.820922, acc 0.82\n",
      "2017-10-16T15:26:05.873239: step 609, loss 0.627737, acc 0.82\n",
      "2017-10-16T15:26:06.000062: step 610, loss 0.547188, acc 0.74\n",
      "2017-10-16T15:26:06.120389: step 611, loss 0.753436, acc 0.78\n",
      "2017-10-16T15:26:06.240350: step 612, loss 0.277968, acc 0.9\n",
      "2017-10-16T15:26:06.373815: step 613, loss 0.357135, acc 0.88\n",
      "2017-10-16T15:26:06.485875: step 614, loss 0.479129, acc 0.88\n",
      "2017-10-16T15:26:06.594210: step 615, loss 0.711668, acc 0.8\n",
      "2017-10-16T15:26:06.710727: step 616, loss 1.10971, acc 0.68\n",
      "2017-10-16T15:26:06.841991: step 617, loss 0.758402, acc 0.8\n",
      "2017-10-16T15:26:06.975005: step 618, loss 0.419969, acc 0.86\n",
      "2017-10-16T15:26:07.129397: step 619, loss 0.960887, acc 0.84\n",
      "2017-10-16T15:26:07.276714: step 620, loss 0.620759, acc 0.76\n",
      "2017-10-16T15:26:07.393535: step 621, loss 0.736094, acc 0.78\n",
      "2017-10-16T15:26:07.505478: step 622, loss 0.653149, acc 0.76\n",
      "2017-10-16T15:26:07.613750: step 623, loss 0.229868, acc 0.88\n",
      "2017-10-16T15:26:07.724082: step 624, loss 0.535786, acc 0.82\n",
      "2017-10-16T15:26:07.834058: step 625, loss 0.56078, acc 0.72\n",
      "2017-10-16T15:26:07.953696: step 626, loss 0.39009, acc 0.9\n",
      "2017-10-16T15:26:08.066656: step 627, loss 0.51289, acc 0.84\n",
      "2017-10-16T15:26:08.178020: step 628, loss 0.468528, acc 0.82\n",
      "2017-10-16T15:26:08.285959: step 629, loss 0.448741, acc 0.84\n",
      "2017-10-16T15:26:08.442152: step 630, loss 0.499957, acc 0.857143\n",
      "2017-10-16T15:26:08.607687: step 631, loss 0.95259, acc 0.82\n",
      "2017-10-16T15:26:08.784851: step 632, loss 0.624035, acc 0.9\n",
      "2017-10-16T15:26:08.934165: step 633, loss 1.06108, acc 0.72\n",
      "2017-10-16T15:26:09.086416: step 634, loss 0.60109, acc 0.76\n",
      "2017-10-16T15:26:09.212323: step 635, loss 0.885815, acc 0.8\n",
      "2017-10-16T15:26:09.325195: step 636, loss 0.957807, acc 0.74\n",
      "2017-10-16T15:26:09.436615: step 637, loss 1.12049, acc 0.66\n",
      "2017-10-16T15:26:09.549041: step 638, loss 0.679694, acc 0.78\n",
      "2017-10-16T15:26:09.658086: step 639, loss 0.47273, acc 0.8\n",
      "2017-10-16T15:26:09.773392: step 640, loss 0.454192, acc 0.8\n",
      "2017-10-16T15:26:09.890367: step 641, loss 0.465268, acc 0.84\n",
      "2017-10-16T15:26:09.999779: step 642, loss 0.962785, acc 0.7\n",
      "2017-10-16T15:26:10.108448: step 643, loss 0.254241, acc 0.92\n",
      "2017-10-16T15:26:10.215390: step 644, loss 0.35397, acc 0.88\n",
      "2017-10-16T15:26:10.324248: step 645, loss 0.512894, acc 0.78\n",
      "2017-10-16T15:26:10.432052: step 646, loss 0.428739, acc 0.88\n",
      "2017-10-16T15:26:10.543519: step 647, loss 1.09943, acc 0.78\n",
      "2017-10-16T15:26:10.654526: step 648, loss 0.527377, acc 0.84\n",
      "2017-10-16T15:26:10.762497: step 649, loss 0.368696, acc 0.84\n",
      "2017-10-16T15:26:10.872698: step 650, loss 0.896475, acc 0.72\n",
      "2017-10-16T15:26:10.979438: step 651, loss 0.671488, acc 0.76\n",
      "2017-10-16T15:26:11.090909: step 652, loss 0.551907, acc 0.72\n",
      "2017-10-16T15:26:11.199518: step 653, loss 0.717668, acc 0.84\n",
      "2017-10-16T15:26:11.381198: step 654, loss 0.71325, acc 0.74\n",
      "2017-10-16T15:26:11.532768: step 655, loss 0.569887, acc 0.76\n",
      "2017-10-16T15:26:11.681030: step 656, loss 0.47943, acc 0.8\n",
      "2017-10-16T15:26:11.818497: step 657, loss 0.560757, acc 0.84\n",
      "2017-10-16T15:26:11.933842: step 658, loss 0.264615, acc 0.9\n",
      "2017-10-16T15:26:12.041663: step 659, loss 0.929227, acc 0.62\n",
      "2017-10-16T15:26:12.156956: step 660, loss 0.630373, acc 0.8\n",
      "2017-10-16T15:26:12.265384: step 661, loss 0.989085, acc 0.72\n",
      "2017-10-16T15:26:12.382222: step 662, loss 0.452654, acc 0.86\n",
      "2017-10-16T15:26:12.496570: step 663, loss 1.12186, acc 0.78\n",
      "2017-10-16T15:26:12.611859: step 664, loss 1.19587, acc 0.64\n",
      "2017-10-16T15:26:12.719914: step 665, loss 0.618607, acc 0.84\n",
      "2017-10-16T15:26:12.830793: step 666, loss 0.607889, acc 0.8\n",
      "2017-10-16T15:26:12.939168: step 667, loss 0.278499, acc 0.92\n",
      "2017-10-16T15:26:13.047749: step 668, loss 0.921292, acc 0.74\n",
      "2017-10-16T15:26:13.157373: step 669, loss 0.826991, acc 0.74\n",
      "2017-10-16T15:26:13.267730: step 670, loss 0.524393, acc 0.8\n",
      "2017-10-16T15:26:13.375719: step 671, loss 0.744783, acc 0.8\n",
      "2017-10-16T15:26:13.483123: step 672, loss 0.876503, acc 0.74\n",
      "2017-10-16T15:26:13.595329: step 673, loss 1.05576, acc 0.66\n",
      "2017-10-16T15:26:13.706350: step 674, loss 0.645814, acc 0.78\n",
      "2017-10-16T15:26:13.815126: step 675, loss 0.297763, acc 0.88\n",
      "2017-10-16T15:26:13.925559: step 676, loss 0.826979, acc 0.8\n",
      "2017-10-16T15:26:14.032568: step 677, loss 0.628104, acc 0.76\n",
      "2017-10-16T15:26:14.139861: step 678, loss 0.675178, acc 0.82\n",
      "2017-10-16T15:26:14.247864: step 679, loss 0.57172, acc 0.8\n",
      "2017-10-16T15:26:14.355943: step 680, loss 0.743383, acc 0.76\n",
      "2017-10-16T15:26:14.464618: step 681, loss 0.756411, acc 0.74\n",
      "2017-10-16T15:26:14.574449: step 682, loss 0.675832, acc 0.76\n",
      "2017-10-16T15:26:14.756283: step 683, loss 0.70019, acc 0.8\n",
      "2017-10-16T15:26:14.906545: step 684, loss 0.495218, acc 0.82\n",
      "2017-10-16T15:26:15.048215: step 685, loss 0.622942, acc 0.8\n",
      "2017-10-16T15:26:15.197934: step 686, loss 1.01456, acc 0.82\n",
      "2017-10-16T15:26:15.330346: step 687, loss 0.622289, acc 0.78\n",
      "2017-10-16T15:26:15.446691: step 688, loss 0.679888, acc 0.72\n",
      "2017-10-16T15:26:15.555205: step 689, loss 0.985448, acc 0.68\n",
      "2017-10-16T15:26:15.674066: step 690, loss 0.321079, acc 0.9\n",
      "2017-10-16T15:26:15.782223: step 691, loss 0.461676, acc 0.82\n",
      "2017-10-16T15:26:15.898517: step 692, loss 0.3621, acc 0.84\n",
      "2017-10-16T15:26:16.017300: step 693, loss 1.22254, acc 0.68\n",
      "2017-10-16T15:26:16.132692: step 694, loss 0.46283, acc 0.86\n",
      "2017-10-16T15:26:16.240775: step 695, loss 0.613429, acc 0.8\n",
      "2017-10-16T15:26:16.351780: step 696, loss 0.603478, acc 0.8\n",
      "2017-10-16T15:26:16.459072: step 697, loss 0.574367, acc 0.84\n",
      "2017-10-16T15:26:16.565391: step 698, loss 0.920263, acc 0.74\n",
      "2017-10-16T15:26:16.681958: step 699, loss 0.440202, acc 0.88\n",
      "2017-10-16T15:26:16.791268: step 700, loss 0.640236, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:26:18.859891: step 700, loss 0.366449, acc 0.843871\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-700\n",
      "\n",
      "2017-10-16T15:26:19.675996: step 701, loss 0.317075, acc 0.92\n",
      "2017-10-16T15:26:19.782872: step 702, loss 0.897396, acc 0.8\n",
      "2017-10-16T15:26:19.891298: step 703, loss 0.381784, acc 0.88\n",
      "2017-10-16T15:26:19.998016: step 704, loss 0.364459, acc 0.9\n",
      "2017-10-16T15:26:20.106945: step 705, loss 0.760208, acc 0.82\n",
      "2017-10-16T15:26:20.212116: step 706, loss 0.487453, acc 0.88\n",
      "2017-10-16T15:26:20.321152: step 707, loss 0.764243, acc 0.76\n",
      "2017-10-16T15:26:20.428302: step 708, loss 0.74376, acc 0.76\n",
      "2017-10-16T15:26:20.534879: step 709, loss 0.509995, acc 0.84\n",
      "2017-10-16T15:26:20.643440: step 710, loss 0.394837, acc 0.86\n",
      "2017-10-16T15:26:20.751361: step 711, loss 1.11568, acc 0.68\n",
      "2017-10-16T15:26:20.859338: step 712, loss 0.398819, acc 0.86\n",
      "2017-10-16T15:26:20.967554: step 713, loss 0.18341, acc 0.92\n",
      "2017-10-16T15:26:21.073686: step 714, loss 0.684488, acc 0.78\n",
      "2017-10-16T15:26:21.181467: step 715, loss 0.449648, acc 0.84\n",
      "2017-10-16T15:26:21.289188: step 716, loss 0.452698, acc 0.86\n",
      "2017-10-16T15:26:21.396621: step 717, loss 0.368343, acc 0.88\n",
      "2017-10-16T15:26:21.503321: step 718, loss 1.20369, acc 0.8\n",
      "2017-10-16T15:26:21.612156: step 719, loss 0.585152, acc 0.82\n",
      "2017-10-16T15:26:21.720741: step 720, loss 0.790604, acc 0.8\n",
      "2017-10-16T15:26:21.827144: step 721, loss 0.531645, acc 0.88\n",
      "2017-10-16T15:26:21.933660: step 722, loss 0.559094, acc 0.82\n",
      "2017-10-16T15:26:22.042276: step 723, loss 0.513261, acc 0.84\n",
      "2017-10-16T15:26:22.151279: step 724, loss 0.537332, acc 0.84\n",
      "2017-10-16T15:26:22.257795: step 725, loss 0.258622, acc 0.9\n",
      "2017-10-16T15:26:22.366316: step 726, loss 0.862649, acc 0.74\n",
      "2017-10-16T15:26:22.475471: step 727, loss 0.563497, acc 0.86\n",
      "2017-10-16T15:26:22.585496: step 728, loss 0.636017, acc 0.8\n",
      "2017-10-16T15:26:22.691675: step 729, loss 0.403655, acc 0.9\n",
      "2017-10-16T15:26:22.798561: step 730, loss 0.510296, acc 0.86\n",
      "2017-10-16T15:26:22.908382: step 731, loss 0.67691, acc 0.78\n",
      "2017-10-16T15:26:23.015431: step 732, loss 0.970686, acc 0.66\n",
      "2017-10-16T15:26:23.124664: step 733, loss 1.06244, acc 0.76\n",
      "2017-10-16T15:26:23.232642: step 734, loss 0.958266, acc 0.78\n",
      "2017-10-16T15:26:23.343576: step 735, loss 0.405585, acc 0.84\n",
      "2017-10-16T15:26:23.452032: step 736, loss 0.685184, acc 0.74\n",
      "2017-10-16T15:26:23.562886: step 737, loss 0.493282, acc 0.82\n",
      "2017-10-16T15:26:23.681435: step 738, loss 0.506284, acc 0.84\n",
      "2017-10-16T15:26:23.789759: step 739, loss 0.547176, acc 0.9\n",
      "2017-10-16T15:26:23.964676: step 740, loss 0.666265, acc 0.78\n",
      "2017-10-16T15:26:24.093019: step 741, loss 0.67698, acc 0.76\n",
      "2017-10-16T15:26:24.211661: step 742, loss 0.586997, acc 0.82\n",
      "2017-10-16T15:26:24.319410: step 743, loss 0.604831, acc 0.8\n",
      "2017-10-16T15:26:24.506768: step 744, loss 0.815873, acc 0.74\n",
      "2017-10-16T15:26:24.640106: step 745, loss 0.486478, acc 0.84\n",
      "2017-10-16T15:26:24.793694: step 746, loss 0.885763, acc 0.78\n",
      "2017-10-16T15:26:24.941068: step 747, loss 0.591404, acc 0.8\n",
      "2017-10-16T15:26:25.088375: step 748, loss 0.698427, acc 0.78\n",
      "2017-10-16T15:26:25.198452: step 749, loss 0.713704, acc 0.84\n",
      "2017-10-16T15:26:25.306345: step 750, loss 0.719492, acc 0.74\n",
      "2017-10-16T15:26:25.417993: step 751, loss 0.311613, acc 0.86\n",
      "2017-10-16T15:26:25.550534: step 752, loss 0.382763, acc 0.84\n",
      "2017-10-16T15:26:25.734915: step 753, loss 1.06257, acc 0.72\n",
      "2017-10-16T15:26:25.866233: step 754, loss 0.484756, acc 0.8\n",
      "2017-10-16T15:26:25.999076: step 755, loss 1.08006, acc 0.66\n",
      "2017-10-16T15:26:26.121904: step 756, loss 0.651018, acc 0.809524\n",
      "2017-10-16T15:26:26.240903: step 757, loss 0.549984, acc 0.86\n",
      "2017-10-16T15:26:26.351447: step 758, loss 0.743977, acc 0.8\n",
      "2017-10-16T15:26:26.462926: step 759, loss 0.556128, acc 0.86\n",
      "2017-10-16T15:26:26.571901: step 760, loss 0.615964, acc 0.8\n",
      "2017-10-16T15:26:26.692808: step 761, loss 0.620283, acc 0.8\n",
      "2017-10-16T15:26:26.825984: step 762, loss 0.645752, acc 0.76\n",
      "2017-10-16T15:26:26.952624: step 763, loss 0.47955, acc 0.84\n",
      "2017-10-16T15:26:27.080735: step 764, loss 0.861441, acc 0.76\n",
      "2017-10-16T15:26:27.207168: step 765, loss 0.73213, acc 0.8\n",
      "2017-10-16T15:26:27.339541: step 766, loss 0.524996, acc 0.86\n",
      "2017-10-16T15:26:27.469270: step 767, loss 0.878209, acc 0.78\n",
      "2017-10-16T15:26:27.604634: step 768, loss 0.662395, acc 0.72\n",
      "2017-10-16T15:26:27.745351: step 769, loss 0.36512, acc 0.82\n",
      "2017-10-16T15:26:27.901535: step 770, loss 0.73712, acc 0.74\n",
      "2017-10-16T15:26:28.027716: step 771, loss 0.7175, acc 0.76\n",
      "2017-10-16T15:26:28.174180: step 772, loss 0.745192, acc 0.82\n",
      "2017-10-16T15:26:28.360312: step 773, loss 0.820681, acc 0.84\n",
      "2017-10-16T15:26:28.488034: step 774, loss 0.606854, acc 0.82\n",
      "2017-10-16T15:26:28.597387: step 775, loss 0.460597, acc 0.82\n",
      "2017-10-16T15:26:28.705239: step 776, loss 0.779209, acc 0.8\n",
      "2017-10-16T15:26:28.815997: step 777, loss 0.617032, acc 0.8\n",
      "2017-10-16T15:26:28.928202: step 778, loss 0.626901, acc 0.82\n",
      "2017-10-16T15:26:29.111355: step 779, loss 0.348938, acc 0.84\n",
      "2017-10-16T15:26:29.274279: step 780, loss 0.771166, acc 0.7\n",
      "2017-10-16T15:26:29.458194: step 781, loss 0.632112, acc 0.74\n",
      "2017-10-16T15:26:29.628321: step 782, loss 0.514425, acc 0.86\n",
      "2017-10-16T15:26:29.784349: step 783, loss 0.382498, acc 0.84\n",
      "2017-10-16T15:26:29.946311: step 784, loss 0.203693, acc 0.9\n",
      "2017-10-16T15:26:30.099295: step 785, loss 0.855225, acc 0.78\n",
      "2017-10-16T15:26:30.273754: step 786, loss 0.786584, acc 0.8\n",
      "2017-10-16T15:26:30.439369: step 787, loss 0.152713, acc 0.92\n",
      "2017-10-16T15:26:30.601700: step 788, loss 0.924946, acc 0.76\n",
      "2017-10-16T15:26:30.774324: step 789, loss 0.391024, acc 0.84\n",
      "2017-10-16T15:26:30.948326: step 790, loss 0.681711, acc 0.76\n",
      "2017-10-16T15:26:31.123168: step 791, loss 0.506006, acc 0.84\n",
      "2017-10-16T15:26:31.251170: step 792, loss 0.766879, acc 0.8\n",
      "2017-10-16T15:26:31.358617: step 793, loss 0.743817, acc 0.8\n",
      "2017-10-16T15:26:31.466232: step 794, loss 0.484244, acc 0.84\n",
      "2017-10-16T15:26:31.577582: step 795, loss 0.704281, acc 0.74\n",
      "2017-10-16T15:26:31.690763: step 796, loss 0.366803, acc 0.86\n",
      "2017-10-16T15:26:31.863312: step 797, loss 0.662199, acc 0.78\n",
      "2017-10-16T15:26:32.008490: step 798, loss 0.559551, acc 0.76\n",
      "2017-10-16T15:26:32.136677: step 799, loss 0.538967, acc 0.8\n",
      "2017-10-16T15:26:32.271368: step 800, loss 0.709877, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:26:34.703239: step 800, loss 0.355871, acc 0.847097\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-800\n",
      "\n",
      "2017-10-16T15:26:35.616225: step 801, loss 0.57299, acc 0.86\n",
      "2017-10-16T15:26:35.797422: step 802, loss 0.45143, acc 0.84\n",
      "2017-10-16T15:26:35.969121: step 803, loss 0.395368, acc 0.88\n",
      "2017-10-16T15:26:36.132455: step 804, loss 0.61687, acc 0.82\n",
      "2017-10-16T15:26:36.277647: step 805, loss 0.250745, acc 0.92\n",
      "2017-10-16T15:26:36.398149: step 806, loss 0.607278, acc 0.82\n",
      "2017-10-16T15:26:36.507894: step 807, loss 0.531569, acc 0.84\n",
      "2017-10-16T15:26:36.681705: step 808, loss 0.8005, acc 0.76\n",
      "2017-10-16T15:26:36.825655: step 809, loss 0.387296, acc 0.92\n",
      "2017-10-16T15:26:36.974470: step 810, loss 0.855769, acc 0.76\n",
      "2017-10-16T15:26:37.106221: step 811, loss 0.804339, acc 0.72\n",
      "2017-10-16T15:26:37.241590: step 812, loss 0.73127, acc 0.76\n",
      "2017-10-16T15:26:37.355098: step 813, loss 0.686624, acc 0.8\n",
      "2017-10-16T15:26:37.466349: step 814, loss 0.480996, acc 0.86\n",
      "2017-10-16T15:26:37.574452: step 815, loss 0.3766, acc 0.84\n",
      "2017-10-16T15:26:37.683083: step 816, loss 0.685637, acc 0.76\n",
      "2017-10-16T15:26:37.797175: step 817, loss 0.360427, acc 0.84\n",
      "2017-10-16T15:26:37.916155: step 818, loss 0.681179, acc 0.8\n",
      "2017-10-16T15:26:38.030231: step 819, loss 0.568405, acc 0.78\n",
      "2017-10-16T15:26:38.169125: step 820, loss 0.325325, acc 0.86\n",
      "2017-10-16T15:26:38.290007: step 821, loss 0.810235, acc 0.74\n",
      "2017-10-16T15:26:38.406235: step 822, loss 0.762819, acc 0.78\n",
      "2017-10-16T15:26:38.513863: step 823, loss 0.461921, acc 0.84\n",
      "2017-10-16T15:26:38.623218: step 824, loss 0.557178, acc 0.86\n",
      "2017-10-16T15:26:38.731957: step 825, loss 0.37704, acc 0.9\n",
      "2017-10-16T15:26:38.840664: step 826, loss 0.609551, acc 0.84\n",
      "2017-10-16T15:26:38.949996: step 827, loss 0.324234, acc 0.9\n",
      "2017-10-16T15:26:39.057851: step 828, loss 0.289713, acc 0.9\n",
      "2017-10-16T15:26:39.164178: step 829, loss 0.629658, acc 0.8\n",
      "2017-10-16T15:26:39.269893: step 830, loss 0.53357, acc 0.72\n",
      "2017-10-16T15:26:39.385091: step 831, loss 0.568341, acc 0.76\n",
      "2017-10-16T15:26:39.503550: step 832, loss 0.627178, acc 0.8\n",
      "2017-10-16T15:26:39.612788: step 833, loss 0.474687, acc 0.82\n",
      "2017-10-16T15:26:39.722765: step 834, loss 0.591027, acc 0.82\n",
      "2017-10-16T15:26:39.856270: step 835, loss 0.346323, acc 0.84\n",
      "2017-10-16T15:26:39.984798: step 836, loss 0.236359, acc 0.92\n",
      "2017-10-16T15:26:40.109720: step 837, loss 0.747124, acc 0.76\n",
      "2017-10-16T15:26:40.239005: step 838, loss 0.992185, acc 0.7\n",
      "2017-10-16T15:26:40.367255: step 839, loss 0.474982, acc 0.82\n",
      "2017-10-16T15:26:40.506156: step 840, loss 0.448874, acc 0.88\n",
      "2017-10-16T15:26:40.632986: step 841, loss 0.593285, acc 0.74\n",
      "2017-10-16T15:26:40.759958: step 842, loss 0.555917, acc 0.82\n",
      "2017-10-16T15:26:40.866924: step 843, loss 0.470488, acc 0.88\n",
      "2017-10-16T15:26:41.014088: step 844, loss 0.553589, acc 0.8\n",
      "2017-10-16T15:26:41.128962: step 845, loss 0.615056, acc 0.76\n",
      "2017-10-16T15:26:41.246660: step 846, loss 0.715329, acc 0.78\n",
      "2017-10-16T15:26:41.363096: step 847, loss 0.502903, acc 0.88\n",
      "2017-10-16T15:26:41.513091: step 848, loss 0.261854, acc 0.9\n",
      "2017-10-16T15:26:41.644400: step 849, loss 0.578849, acc 0.8\n",
      "2017-10-16T15:26:41.766731: step 850, loss 0.641731, acc 0.8\n",
      "2017-10-16T15:26:41.876037: step 851, loss 0.630407, acc 0.82\n",
      "2017-10-16T15:26:41.982384: step 852, loss 0.432994, acc 0.82\n",
      "2017-10-16T15:26:42.096426: step 853, loss 0.381167, acc 0.86\n",
      "2017-10-16T15:26:42.207594: step 854, loss 0.626192, acc 0.8\n",
      "2017-10-16T15:26:42.317143: step 855, loss 0.937112, acc 0.78\n",
      "2017-10-16T15:26:42.429260: step 856, loss 0.591723, acc 0.82\n",
      "2017-10-16T15:26:42.584505: step 857, loss 0.856538, acc 0.8\n",
      "2017-10-16T15:26:42.735720: step 858, loss 0.94659, acc 0.76\n",
      "2017-10-16T15:26:42.845382: step 859, loss 0.469702, acc 0.86\n",
      "2017-10-16T15:26:42.953077: step 860, loss 0.628689, acc 0.84\n",
      "2017-10-16T15:26:43.063282: step 861, loss 0.488876, acc 0.82\n",
      "2017-10-16T15:26:43.175292: step 862, loss 0.467805, acc 0.82\n",
      "2017-10-16T15:26:43.295880: step 863, loss 0.807243, acc 0.8\n",
      "2017-10-16T15:26:43.418510: step 864, loss 0.931674, acc 0.72\n",
      "2017-10-16T15:26:43.557758: step 865, loss 0.612323, acc 0.82\n",
      "2017-10-16T15:26:43.697216: step 866, loss 0.579209, acc 0.86\n",
      "2017-10-16T15:26:43.805478: step 867, loss 0.57593, acc 0.86\n",
      "2017-10-16T15:26:43.920125: step 868, loss 0.198615, acc 0.92\n",
      "2017-10-16T15:26:44.032820: step 869, loss 0.942991, acc 0.76\n",
      "2017-10-16T15:26:44.139175: step 870, loss 0.323053, acc 0.88\n",
      "2017-10-16T15:26:44.249304: step 871, loss 0.272827, acc 0.86\n",
      "2017-10-16T15:26:44.355439: step 872, loss 1.20994, acc 0.76\n",
      "2017-10-16T15:26:44.469920: step 873, loss 0.545621, acc 0.86\n",
      "2017-10-16T15:26:44.582383: step 874, loss 0.951511, acc 0.74\n",
      "2017-10-16T15:26:44.689656: step 875, loss 0.334162, acc 0.92\n",
      "2017-10-16T15:26:44.799219: step 876, loss 0.343998, acc 0.9\n",
      "2017-10-16T15:26:44.908327: step 877, loss 0.438663, acc 0.88\n",
      "2017-10-16T15:26:45.018981: step 878, loss 0.979858, acc 0.82\n",
      "2017-10-16T15:26:45.127507: step 879, loss 0.57059, acc 0.88\n",
      "2017-10-16T15:26:45.234603: step 880, loss 0.784246, acc 0.8\n",
      "2017-10-16T15:26:45.343749: step 881, loss 0.873067, acc 0.74\n",
      "2017-10-16T15:26:45.458250: step 882, loss 0.150652, acc 0.904762\n",
      "2017-10-16T15:26:45.584886: step 883, loss 0.341611, acc 0.88\n",
      "2017-10-16T15:26:45.716455: step 884, loss 0.563441, acc 0.8\n",
      "2017-10-16T15:26:45.834363: step 885, loss 0.802389, acc 0.68\n",
      "2017-10-16T15:26:45.944799: step 886, loss 0.484355, acc 0.84\n",
      "2017-10-16T15:26:46.057128: step 887, loss 0.318089, acc 0.8\n",
      "2017-10-16T15:26:46.196293: step 888, loss 0.644227, acc 0.8\n",
      "2017-10-16T15:26:46.320079: step 889, loss 0.695102, acc 0.76\n",
      "2017-10-16T15:26:46.450846: step 890, loss 0.511725, acc 0.8\n",
      "2017-10-16T15:26:46.570167: step 891, loss 0.802526, acc 0.78\n",
      "2017-10-16T15:26:46.705707: step 892, loss 0.277776, acc 0.9\n",
      "2017-10-16T15:26:46.829230: step 893, loss 0.92995, acc 0.78\n",
      "2017-10-16T15:26:46.941466: step 894, loss 0.610669, acc 0.8\n",
      "2017-10-16T15:26:47.047990: step 895, loss 0.383463, acc 0.9\n",
      "2017-10-16T15:26:47.155479: step 896, loss 0.425458, acc 0.9\n",
      "2017-10-16T15:26:47.275574: step 897, loss 0.725533, acc 0.82\n",
      "2017-10-16T15:26:47.422752: step 898, loss 0.334908, acc 0.82\n",
      "2017-10-16T15:26:47.566831: step 899, loss 0.421221, acc 0.84\n",
      "2017-10-16T15:26:47.718290: step 900, loss 0.425042, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:26:49.716643: step 900, loss 0.355324, acc 0.85\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-900\n",
      "\n",
      "2017-10-16T15:26:50.633767: step 901, loss 0.764321, acc 0.78\n",
      "2017-10-16T15:26:50.768775: step 902, loss 0.466413, acc 0.86\n",
      "2017-10-16T15:26:50.903855: step 903, loss 0.337249, acc 0.88\n",
      "2017-10-16T15:26:51.049862: step 904, loss 0.852396, acc 0.8\n",
      "2017-10-16T15:26:51.183780: step 905, loss 0.40408, acc 0.84\n",
      "2017-10-16T15:26:51.298501: step 906, loss 0.755025, acc 0.76\n",
      "2017-10-16T15:26:51.406939: step 907, loss 0.440173, acc 0.82\n",
      "2017-10-16T15:26:51.527229: step 908, loss 0.27026, acc 0.86\n",
      "2017-10-16T15:26:51.634084: step 909, loss 0.558747, acc 0.8\n",
      "2017-10-16T15:26:51.745048: step 910, loss 0.627564, acc 0.84\n",
      "2017-10-16T15:26:51.865376: step 911, loss 0.564436, acc 0.82\n",
      "2017-10-16T15:26:51.991575: step 912, loss 0.500986, acc 0.8\n",
      "2017-10-16T15:26:52.098183: step 913, loss 0.36888, acc 0.86\n",
      "2017-10-16T15:26:52.209770: step 914, loss 0.688344, acc 0.84\n",
      "2017-10-16T15:26:52.317633: step 915, loss 0.519758, acc 0.8\n",
      "2017-10-16T15:26:52.433983: step 916, loss 0.476016, acc 0.86\n",
      "2017-10-16T15:26:52.558900: step 917, loss 0.792366, acc 0.74\n",
      "2017-10-16T15:26:52.697028: step 918, loss 0.354567, acc 0.86\n",
      "2017-10-16T15:26:52.824875: step 919, loss 0.411575, acc 0.84\n",
      "2017-10-16T15:26:52.935997: step 920, loss 0.503557, acc 0.82\n",
      "2017-10-16T15:26:53.051967: step 921, loss 0.491269, acc 0.86\n",
      "2017-10-16T15:26:53.168990: step 922, loss 0.570044, acc 0.84\n",
      "2017-10-16T15:26:53.283083: step 923, loss 0.677129, acc 0.82\n",
      "2017-10-16T15:26:53.392025: step 924, loss 1.13677, acc 0.74\n",
      "2017-10-16T15:26:53.498153: step 925, loss 0.526424, acc 0.84\n",
      "2017-10-16T15:26:53.607606: step 926, loss 0.755301, acc 0.8\n",
      "2017-10-16T15:26:53.716529: step 927, loss 0.599602, acc 0.8\n",
      "2017-10-16T15:26:53.829064: step 928, loss 0.812973, acc 0.72\n",
      "2017-10-16T15:26:53.936442: step 929, loss 0.460193, acc 0.86\n",
      "2017-10-16T15:26:54.075133: step 930, loss 0.605096, acc 0.86\n",
      "2017-10-16T15:26:54.186836: step 931, loss 0.715117, acc 0.76\n",
      "2017-10-16T15:26:54.296595: step 932, loss 0.477093, acc 0.86\n",
      "2017-10-16T15:26:54.410205: step 933, loss 0.884338, acc 0.82\n",
      "2017-10-16T15:26:54.524552: step 934, loss 0.600064, acc 0.8\n",
      "2017-10-16T15:26:54.662886: step 935, loss 0.621808, acc 0.84\n",
      "2017-10-16T15:26:54.794828: step 936, loss 0.563369, acc 0.82\n",
      "2017-10-16T15:26:54.917339: step 937, loss 0.27856, acc 0.92\n",
      "2017-10-16T15:26:55.050336: step 938, loss 0.55811, acc 0.8\n",
      "2017-10-16T15:26:55.171912: step 939, loss 0.54276, acc 0.86\n",
      "2017-10-16T15:26:55.304591: step 940, loss 0.854215, acc 0.74\n",
      "2017-10-16T15:26:55.427211: step 941, loss 0.712752, acc 0.78\n",
      "2017-10-16T15:26:55.571246: step 942, loss 0.475873, acc 0.84\n",
      "2017-10-16T15:26:55.679589: step 943, loss 0.799673, acc 0.82\n",
      "2017-10-16T15:26:55.791423: step 944, loss 0.373113, acc 0.86\n",
      "2017-10-16T15:26:55.908375: step 945, loss 0.840005, acc 0.78\n",
      "2017-10-16T15:26:56.042149: step 946, loss 0.365075, acc 0.8\n",
      "2017-10-16T15:26:56.149695: step 947, loss 0.606849, acc 0.86\n",
      "2017-10-16T15:26:56.287302: step 948, loss 0.418134, acc 0.82\n",
      "2017-10-16T15:26:56.401409: step 949, loss 0.396812, acc 0.84\n",
      "2017-10-16T15:26:56.513009: step 950, loss 0.56263, acc 0.74\n",
      "2017-10-16T15:26:56.621440: step 951, loss 0.323438, acc 0.88\n",
      "2017-10-16T15:26:56.728002: step 952, loss 0.550396, acc 0.78\n",
      "2017-10-16T15:26:56.845755: step 953, loss 0.393741, acc 0.8\n",
      "2017-10-16T15:26:56.962628: step 954, loss 0.849439, acc 0.82\n",
      "2017-10-16T15:26:57.071640: step 955, loss 0.562904, acc 0.82\n",
      "2017-10-16T15:26:57.181753: step 956, loss 0.784206, acc 0.74\n",
      "2017-10-16T15:26:57.296425: step 957, loss 0.869667, acc 0.7\n",
      "2017-10-16T15:26:57.405264: step 958, loss 0.63516, acc 0.8\n",
      "2017-10-16T15:26:57.514647: step 959, loss 0.806757, acc 0.74\n",
      "2017-10-16T15:26:57.624616: step 960, loss 0.775448, acc 0.7\n",
      "2017-10-16T15:26:57.736035: step 961, loss 0.809924, acc 0.76\n",
      "2017-10-16T15:26:57.866536: step 962, loss 0.485487, acc 0.88\n",
      "2017-10-16T15:26:57.990771: step 963, loss 0.523029, acc 0.84\n",
      "2017-10-16T15:26:58.124003: step 964, loss 0.535914, acc 0.84\n",
      "2017-10-16T15:26:58.249096: step 965, loss 0.856718, acc 0.82\n",
      "2017-10-16T15:26:58.374717: step 966, loss 0.41145, acc 0.82\n",
      "2017-10-16T15:26:58.484187: step 967, loss 0.44389, acc 0.84\n",
      "2017-10-16T15:26:58.635946: step 968, loss 0.481236, acc 0.8\n",
      "2017-10-16T15:26:58.800420: step 969, loss 0.793779, acc 0.8\n",
      "2017-10-16T15:26:58.966526: step 970, loss 0.389769, acc 0.92\n",
      "2017-10-16T15:26:59.079141: step 971, loss 0.559081, acc 0.8\n",
      "2017-10-16T15:26:59.189495: step 972, loss 0.492112, acc 0.84\n",
      "2017-10-16T15:26:59.300716: step 973, loss 0.46503, acc 0.84\n",
      "2017-10-16T15:26:59.466670: step 974, loss 0.390886, acc 0.92\n",
      "2017-10-16T15:26:59.618902: step 975, loss 0.335219, acc 0.86\n",
      "2017-10-16T15:26:59.767030: step 976, loss 0.561333, acc 0.78\n",
      "2017-10-16T15:26:59.903579: step 977, loss 0.487235, acc 0.78\n",
      "2017-10-16T15:27:00.067176: step 978, loss 0.395935, acc 0.84\n",
      "2017-10-16T15:27:00.234699: step 979, loss 0.449141, acc 0.86\n",
      "2017-10-16T15:27:00.384758: step 980, loss 0.514346, acc 0.78\n",
      "2017-10-16T15:27:00.495631: step 981, loss 0.73956, acc 0.76\n",
      "2017-10-16T15:27:00.608833: step 982, loss 0.498613, acc 0.82\n",
      "2017-10-16T15:27:00.717818: step 983, loss 0.433559, acc 0.88\n",
      "2017-10-16T15:27:00.828124: step 984, loss 0.384779, acc 0.88\n",
      "2017-10-16T15:27:00.946125: step 985, loss 0.375833, acc 0.82\n",
      "2017-10-16T15:27:01.067081: step 986, loss 0.560443, acc 0.8\n",
      "2017-10-16T15:27:01.175670: step 987, loss 0.276996, acc 0.9\n",
      "2017-10-16T15:27:01.282969: step 988, loss 0.470561, acc 0.8\n",
      "2017-10-16T15:27:01.394662: step 989, loss 0.548627, acc 0.84\n",
      "2017-10-16T15:27:01.505207: step 990, loss 0.525653, acc 0.88\n",
      "2017-10-16T15:27:01.616226: step 991, loss 0.493276, acc 0.88\n",
      "2017-10-16T15:27:01.725952: step 992, loss 0.844136, acc 0.78\n",
      "2017-10-16T15:27:01.836240: step 993, loss 0.50501, acc 0.78\n",
      "2017-10-16T15:27:01.947815: step 994, loss 0.275737, acc 0.92\n",
      "2017-10-16T15:27:02.056891: step 995, loss 0.48686, acc 0.8\n",
      "2017-10-16T15:27:02.167708: step 996, loss 0.327181, acc 0.88\n",
      "2017-10-16T15:27:02.278166: step 997, loss 0.271604, acc 0.92\n",
      "2017-10-16T15:27:02.390685: step 998, loss 0.786241, acc 0.74\n",
      "2017-10-16T15:27:02.499539: step 999, loss 0.494269, acc 0.82\n",
      "2017-10-16T15:27:02.609819: step 1000, loss 0.461609, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:27:04.900774: step 1000, loss 0.3464, acc 0.854839\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-1000\n",
      "\n",
      "2017-10-16T15:27:05.630570: step 1001, loss 0.75085, acc 0.74\n",
      "2017-10-16T15:27:05.750506: step 1002, loss 0.557934, acc 0.8\n",
      "2017-10-16T15:27:05.870263: step 1003, loss 0.366203, acc 0.9\n",
      "2017-10-16T15:27:05.995146: step 1004, loss 0.791522, acc 0.76\n",
      "2017-10-16T15:27:06.106060: step 1005, loss 0.567938, acc 0.88\n",
      "2017-10-16T15:27:06.215665: step 1006, loss 0.398041, acc 0.86\n",
      "2017-10-16T15:27:06.328959: step 1007, loss 0.758907, acc 0.8\n",
      "2017-10-16T15:27:06.423698: step 1008, loss 0.593473, acc 0.738095\n",
      "2017-10-16T15:27:06.538195: step 1009, loss 0.886371, acc 0.72\n",
      "2017-10-16T15:27:06.654893: step 1010, loss 0.359485, acc 0.86\n",
      "2017-10-16T15:27:06.766232: step 1011, loss 0.575774, acc 0.76\n",
      "2017-10-16T15:27:06.878204: step 1012, loss 0.506752, acc 0.8\n",
      "2017-10-16T15:27:06.992144: step 1013, loss 0.86178, acc 0.7\n",
      "2017-10-16T15:27:07.107217: step 1014, loss 0.463917, acc 0.76\n",
      "2017-10-16T15:27:07.216536: step 1015, loss 0.401817, acc 0.9\n",
      "2017-10-16T15:27:07.324731: step 1016, loss 0.628802, acc 0.76\n",
      "2017-10-16T15:27:07.436721: step 1017, loss 0.586778, acc 0.74\n",
      "2017-10-16T15:27:07.546516: step 1018, loss 0.440353, acc 0.88\n",
      "2017-10-16T15:27:07.656746: step 1019, loss 0.430042, acc 0.84\n",
      "2017-10-16T15:27:07.767201: step 1020, loss 0.508594, acc 0.76\n",
      "2017-10-16T15:27:07.886192: step 1021, loss 0.380617, acc 0.86\n",
      "2017-10-16T15:27:08.001935: step 1022, loss 0.25274, acc 0.9\n",
      "2017-10-16T15:27:08.108904: step 1023, loss 0.567213, acc 0.86\n",
      "2017-10-16T15:27:08.218999: step 1024, loss 0.42931, acc 0.88\n",
      "2017-10-16T15:27:08.329908: step 1025, loss 0.453884, acc 0.78\n",
      "2017-10-16T15:27:08.439618: step 1026, loss 0.80535, acc 0.78\n",
      "2017-10-16T15:27:08.548605: step 1027, loss 0.413469, acc 0.88\n",
      "2017-10-16T15:27:08.662727: step 1028, loss 0.554003, acc 0.82\n",
      "2017-10-16T15:27:08.782899: step 1029, loss 0.625928, acc 0.78\n",
      "2017-10-16T15:27:08.894369: step 1030, loss 0.327151, acc 0.86\n",
      "2017-10-16T15:27:09.001665: step 1031, loss 0.649836, acc 0.74\n",
      "2017-10-16T15:27:09.111068: step 1032, loss 0.51629, acc 0.86\n",
      "2017-10-16T15:27:09.223918: step 1033, loss 0.32489, acc 0.88\n",
      "2017-10-16T15:27:09.334219: step 1034, loss 0.410053, acc 0.84\n",
      "2017-10-16T15:27:09.446268: step 1035, loss 0.351385, acc 0.86\n",
      "2017-10-16T15:27:09.557650: step 1036, loss 0.432851, acc 0.86\n",
      "2017-10-16T15:27:09.677568: step 1037, loss 0.43562, acc 0.88\n",
      "2017-10-16T15:27:09.792145: step 1038, loss 0.466301, acc 0.86\n",
      "2017-10-16T15:27:09.901030: step 1039, loss 0.697095, acc 0.8\n",
      "2017-10-16T15:27:10.009875: step 1040, loss 0.681537, acc 0.8\n",
      "2017-10-16T15:27:10.121563: step 1041, loss 0.668401, acc 0.8\n",
      "2017-10-16T15:27:10.249324: step 1042, loss 0.45278, acc 0.86\n",
      "2017-10-16T15:27:10.366375: step 1043, loss 0.367007, acc 0.86\n",
      "2017-10-16T15:27:10.489332: step 1044, loss 1.04195, acc 0.74\n",
      "2017-10-16T15:27:10.598087: step 1045, loss 0.320949, acc 0.92\n",
      "2017-10-16T15:27:10.709847: step 1046, loss 0.514706, acc 0.78\n",
      "2017-10-16T15:27:10.817634: step 1047, loss 0.676111, acc 0.82\n",
      "2017-10-16T15:27:10.931149: step 1048, loss 0.696119, acc 0.82\n",
      "2017-10-16T15:27:11.063760: step 1049, loss 0.509304, acc 0.84\n",
      "2017-10-16T15:27:11.177618: step 1050, loss 0.414259, acc 0.82\n",
      "2017-10-16T15:27:11.295606: step 1051, loss 0.542625, acc 0.84\n",
      "2017-10-16T15:27:11.407420: step 1052, loss 0.356338, acc 0.88\n",
      "2017-10-16T15:27:11.513068: step 1053, loss 0.513002, acc 0.84\n",
      "2017-10-16T15:27:11.619059: step 1054, loss 0.480948, acc 0.86\n",
      "2017-10-16T15:27:11.730988: step 1055, loss 0.713899, acc 0.84\n",
      "2017-10-16T15:27:11.840186: step 1056, loss 0.55299, acc 0.9\n",
      "2017-10-16T15:27:11.947923: step 1057, loss 0.401402, acc 0.88\n",
      "2017-10-16T15:27:12.056043: step 1058, loss 0.438696, acc 0.84\n",
      "2017-10-16T15:27:12.167897: step 1059, loss 0.531256, acc 0.84\n",
      "2017-10-16T15:27:12.277696: step 1060, loss 0.633954, acc 0.76\n",
      "2017-10-16T15:27:12.388382: step 1061, loss 0.493762, acc 0.82\n",
      "2017-10-16T15:27:12.509035: step 1062, loss 0.567129, acc 0.82\n",
      "2017-10-16T15:27:12.618910: step 1063, loss 0.343777, acc 0.88\n",
      "2017-10-16T15:27:12.738742: step 1064, loss 0.564314, acc 0.84\n",
      "2017-10-16T15:27:12.851637: step 1065, loss 0.664639, acc 0.76\n",
      "2017-10-16T15:27:12.961461: step 1066, loss 0.506486, acc 0.86\n",
      "2017-10-16T15:27:13.070410: step 1067, loss 0.537953, acc 0.84\n",
      "2017-10-16T15:27:13.181811: step 1068, loss 0.641354, acc 0.86\n",
      "2017-10-16T15:27:13.298204: step 1069, loss 0.552116, acc 0.78\n",
      "2017-10-16T15:27:13.434834: step 1070, loss 0.439738, acc 0.82\n",
      "2017-10-16T15:27:13.552500: step 1071, loss 0.460443, acc 0.9\n",
      "2017-10-16T15:27:13.665341: step 1072, loss 0.758398, acc 0.82\n",
      "2017-10-16T15:27:13.772793: step 1073, loss 0.601641, acc 0.84\n",
      "2017-10-16T15:27:13.882722: step 1074, loss 0.806478, acc 0.84\n",
      "2017-10-16T15:27:13.996141: step 1075, loss 0.431935, acc 0.86\n",
      "2017-10-16T15:27:14.126515: step 1076, loss 0.320408, acc 0.9\n",
      "2017-10-16T15:27:14.266649: step 1077, loss 0.452424, acc 0.82\n",
      "2017-10-16T15:27:14.396873: step 1078, loss 0.807585, acc 0.82\n",
      "2017-10-16T15:27:14.510308: step 1079, loss 0.448182, acc 0.84\n",
      "2017-10-16T15:27:14.619671: step 1080, loss 0.905055, acc 0.78\n",
      "2017-10-16T15:27:14.730323: step 1081, loss 0.706701, acc 0.82\n",
      "2017-10-16T15:27:14.841105: step 1082, loss 0.488002, acc 0.84\n",
      "2017-10-16T15:27:14.952258: step 1083, loss 0.76375, acc 0.76\n",
      "2017-10-16T15:27:15.073789: step 1084, loss 0.176054, acc 0.94\n",
      "2017-10-16T15:27:15.191760: step 1085, loss 0.728279, acc 0.74\n",
      "2017-10-16T15:27:15.309416: step 1086, loss 0.371591, acc 0.92\n",
      "2017-10-16T15:27:15.427275: step 1087, loss 0.697461, acc 0.82\n",
      "2017-10-16T15:27:15.547912: step 1088, loss 0.445537, acc 0.84\n",
      "2017-10-16T15:27:15.681816: step 1089, loss 0.773626, acc 0.82\n",
      "2017-10-16T15:27:15.808716: step 1090, loss 0.458208, acc 0.78\n",
      "2017-10-16T15:27:15.920543: step 1091, loss 0.574446, acc 0.76\n",
      "2017-10-16T15:27:16.037602: step 1092, loss 0.498742, acc 0.78\n",
      "2017-10-16T15:27:16.147445: step 1093, loss 0.421688, acc 0.8\n",
      "2017-10-16T15:27:16.256690: step 1094, loss 0.528601, acc 0.84\n",
      "2017-10-16T15:27:16.400312: step 1095, loss 0.654764, acc 0.8\n",
      "2017-10-16T15:27:16.533881: step 1096, loss 0.447987, acc 0.86\n",
      "2017-10-16T15:27:16.646414: step 1097, loss 0.250237, acc 0.9\n",
      "2017-10-16T15:27:16.762300: step 1098, loss 0.451954, acc 0.84\n",
      "2017-10-16T15:27:16.869383: step 1099, loss 0.212852, acc 0.9\n",
      "2017-10-16T15:27:16.987261: step 1100, loss 0.30102, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:27:19.389262: step 1100, loss 0.342211, acc 0.850968\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-1100\n",
      "\n",
      "2017-10-16T15:27:20.209154: step 1101, loss 0.27559, acc 0.88\n",
      "2017-10-16T15:27:20.322937: step 1102, loss 0.257977, acc 0.88\n",
      "2017-10-16T15:27:20.443328: step 1103, loss 0.395916, acc 0.84\n",
      "2017-10-16T15:27:20.554707: step 1104, loss 0.50872, acc 0.86\n",
      "2017-10-16T15:27:20.663611: step 1105, loss 0.465318, acc 0.84\n",
      "2017-10-16T15:27:20.780105: step 1106, loss 0.414875, acc 0.86\n",
      "2017-10-16T15:27:20.891778: step 1107, loss 0.639965, acc 0.82\n",
      "2017-10-16T15:27:21.011501: step 1108, loss 0.330157, acc 0.88\n",
      "2017-10-16T15:27:21.142925: step 1109, loss 0.655168, acc 0.84\n",
      "2017-10-16T15:27:21.265758: step 1110, loss 0.490513, acc 0.84\n",
      "2017-10-16T15:27:21.373301: step 1111, loss 0.300367, acc 0.9\n",
      "2017-10-16T15:27:21.550045: step 1112, loss 0.530733, acc 0.82\n",
      "2017-10-16T15:27:21.678322: step 1113, loss 0.204165, acc 0.88\n",
      "2017-10-16T15:27:21.795030: step 1114, loss 0.578778, acc 0.8\n",
      "2017-10-16T15:27:21.905084: step 1115, loss 0.593815, acc 0.8\n",
      "2017-10-16T15:27:22.014491: step 1116, loss 0.431232, acc 0.84\n",
      "2017-10-16T15:27:22.164518: step 1117, loss 0.4144, acc 0.86\n",
      "2017-10-16T15:27:22.315207: step 1118, loss 0.355037, acc 0.82\n",
      "2017-10-16T15:27:22.459400: step 1119, loss 0.535172, acc 0.8\n",
      "2017-10-16T15:27:22.589804: step 1120, loss 0.713211, acc 0.8\n",
      "2017-10-16T15:27:22.698168: step 1121, loss 0.500727, acc 0.88\n",
      "2017-10-16T15:27:22.806186: step 1122, loss 0.960406, acc 0.82\n",
      "2017-10-16T15:27:22.917655: step 1123, loss 0.7809, acc 0.76\n",
      "2017-10-16T15:27:23.029120: step 1124, loss 0.219067, acc 0.94\n",
      "2017-10-16T15:27:23.144593: step 1125, loss 0.476609, acc 0.78\n",
      "2017-10-16T15:27:23.262281: step 1126, loss 0.391132, acc 0.84\n",
      "2017-10-16T15:27:23.423155: step 1127, loss 0.417497, acc 0.8\n",
      "2017-10-16T15:27:23.573172: step 1128, loss 0.290261, acc 0.94\n",
      "2017-10-16T15:27:23.708802: step 1129, loss 0.442646, acc 0.9\n",
      "2017-10-16T15:27:23.843998: step 1130, loss 0.495375, acc 0.8\n",
      "2017-10-16T15:27:23.971518: step 1131, loss 0.355592, acc 0.88\n",
      "2017-10-16T15:27:24.113069: step 1132, loss 0.199841, acc 0.92\n",
      "2017-10-16T15:27:24.251392: step 1133, loss 0.217824, acc 0.9\n",
      "2017-10-16T15:27:24.372060: step 1134, loss 0.548564, acc 0.809524\n",
      "2017-10-16T15:27:24.515420: step 1135, loss 0.569989, acc 0.78\n",
      "2017-10-16T15:27:24.661715: step 1136, loss 0.533818, acc 0.72\n",
      "2017-10-16T15:27:24.801994: step 1137, loss 0.57338, acc 0.78\n",
      "2017-10-16T15:27:24.949760: step 1138, loss 0.572936, acc 0.8\n",
      "2017-10-16T15:27:25.059378: step 1139, loss 0.458528, acc 0.86\n",
      "2017-10-16T15:27:25.169417: step 1140, loss 0.360116, acc 0.86\n",
      "2017-10-16T15:27:25.283542: step 1141, loss 0.700131, acc 0.78\n",
      "2017-10-16T15:27:25.395564: step 1142, loss 0.679526, acc 0.82\n",
      "2017-10-16T15:27:25.519677: step 1143, loss 0.628918, acc 0.78\n",
      "2017-10-16T15:27:25.663415: step 1144, loss 1.0409, acc 0.7\n",
      "2017-10-16T15:27:25.770649: step 1145, loss 0.839464, acc 0.72\n",
      "2017-10-16T15:27:25.897853: step 1146, loss 0.324684, acc 0.82\n",
      "2017-10-16T15:27:26.034538: step 1147, loss 0.723435, acc 0.84\n",
      "2017-10-16T15:27:26.171147: step 1148, loss 0.682196, acc 0.82\n",
      "2017-10-16T15:27:26.297462: step 1149, loss 0.344367, acc 0.9\n",
      "2017-10-16T15:27:26.427037: step 1150, loss 0.430084, acc 0.88\n",
      "2017-10-16T15:27:26.559305: step 1151, loss 0.412848, acc 0.82\n",
      "2017-10-16T15:27:26.695351: step 1152, loss 0.314015, acc 0.9\n",
      "2017-10-16T15:27:26.815602: step 1153, loss 0.416104, acc 0.86\n",
      "2017-10-16T15:27:26.954836: step 1154, loss 0.61834, acc 0.84\n",
      "2017-10-16T15:27:27.084208: step 1155, loss 0.398646, acc 0.82\n",
      "2017-10-16T15:27:27.218762: step 1156, loss 0.545422, acc 0.8\n",
      "2017-10-16T15:27:27.337885: step 1157, loss 0.209339, acc 0.92\n",
      "2017-10-16T15:27:27.483650: step 1158, loss 0.318662, acc 0.8\n",
      "2017-10-16T15:27:27.609303: step 1159, loss 0.524704, acc 0.78\n",
      "2017-10-16T15:27:27.743801: step 1160, loss 0.472484, acc 0.82\n",
      "2017-10-16T15:27:27.871592: step 1161, loss 0.494448, acc 0.84\n",
      "2017-10-16T15:27:28.003940: step 1162, loss 0.766868, acc 0.74\n",
      "2017-10-16T15:27:28.111976: step 1163, loss 0.35335, acc 0.86\n",
      "2017-10-16T15:27:28.235909: step 1164, loss 0.719724, acc 0.8\n",
      "2017-10-16T15:27:28.383590: step 1165, loss 0.294957, acc 0.9\n",
      "2017-10-16T15:27:28.525932: step 1166, loss 0.512365, acc 0.82\n",
      "2017-10-16T15:27:28.636023: step 1167, loss 0.581326, acc 0.74\n",
      "2017-10-16T15:27:28.745381: step 1168, loss 0.306934, acc 0.84\n",
      "2017-10-16T15:27:28.859849: step 1169, loss 0.508919, acc 0.8\n",
      "2017-10-16T15:27:28.994423: step 1170, loss 0.660165, acc 0.78\n",
      "2017-10-16T15:27:29.129221: step 1171, loss 0.379273, acc 0.86\n",
      "2017-10-16T15:27:29.266519: step 1172, loss 0.307636, acc 0.88\n",
      "2017-10-16T15:27:29.385335: step 1173, loss 0.416476, acc 0.76\n",
      "2017-10-16T15:27:29.512624: step 1174, loss 0.364653, acc 0.9\n",
      "2017-10-16T15:27:29.647612: step 1175, loss 0.437097, acc 0.86\n",
      "2017-10-16T15:27:29.781545: step 1176, loss 0.307036, acc 0.92\n",
      "2017-10-16T15:27:29.904757: step 1177, loss 0.458466, acc 0.82\n",
      "2017-10-16T15:27:30.036943: step 1178, loss 0.345132, acc 0.9\n",
      "2017-10-16T15:27:30.169514: step 1179, loss 0.304633, acc 0.92\n",
      "2017-10-16T15:27:30.299758: step 1180, loss 0.304021, acc 0.86\n",
      "2017-10-16T15:27:30.426484: step 1181, loss 0.308268, acc 0.88\n",
      "2017-10-16T15:27:30.546096: step 1182, loss 0.432522, acc 0.86\n",
      "2017-10-16T15:27:30.660386: step 1183, loss 0.41631, acc 0.9\n",
      "2017-10-16T15:27:30.808040: step 1184, loss 0.590075, acc 0.88\n",
      "2017-10-16T15:27:30.935601: step 1185, loss 0.473992, acc 0.82\n",
      "2017-10-16T15:27:31.068089: step 1186, loss 0.78448, acc 0.74\n",
      "2017-10-16T15:27:31.197758: step 1187, loss 0.229279, acc 0.9\n",
      "2017-10-16T15:27:31.323289: step 1188, loss 0.297562, acc 0.9\n",
      "2017-10-16T15:27:31.436576: step 1189, loss 0.156026, acc 0.94\n",
      "2017-10-16T15:27:31.545718: step 1190, loss 0.305657, acc 0.9\n",
      "2017-10-16T15:27:31.686775: step 1191, loss 0.538945, acc 0.86\n",
      "2017-10-16T15:27:31.824928: step 1192, loss 0.57317, acc 0.8\n",
      "2017-10-16T15:27:31.949448: step 1193, loss 0.368002, acc 0.84\n",
      "2017-10-16T15:27:32.080450: step 1194, loss 0.646906, acc 0.74\n",
      "2017-10-16T15:27:32.193294: step 1195, loss 0.602199, acc 0.86\n",
      "2017-10-16T15:27:32.337711: step 1196, loss 0.510563, acc 0.86\n",
      "2017-10-16T15:27:32.464312: step 1197, loss 0.541829, acc 0.86\n",
      "2017-10-16T15:27:32.594115: step 1198, loss 0.375747, acc 0.8\n",
      "2017-10-16T15:27:32.718247: step 1199, loss 0.398183, acc 0.82\n",
      "2017-10-16T15:27:32.836266: step 1200, loss 0.767842, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:27:35.008834: step 1200, loss 0.337564, acc 0.856452\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-1200\n",
      "\n",
      "2017-10-16T15:27:35.819618: step 1201, loss 0.582327, acc 0.86\n",
      "2017-10-16T15:27:35.933332: step 1202, loss 0.382575, acc 0.84\n",
      "2017-10-16T15:27:36.043867: step 1203, loss 0.550445, acc 0.78\n",
      "2017-10-16T15:27:36.151129: step 1204, loss 0.46415, acc 0.84\n",
      "2017-10-16T15:27:36.266682: step 1205, loss 0.70348, acc 0.8\n",
      "2017-10-16T15:27:36.376614: step 1206, loss 0.55199, acc 0.8\n",
      "2017-10-16T15:27:36.490434: step 1207, loss 0.291529, acc 0.9\n",
      "2017-10-16T15:27:36.612522: step 1208, loss 0.623081, acc 0.8\n",
      "2017-10-16T15:27:36.744726: step 1209, loss 0.346815, acc 0.88\n",
      "2017-10-16T15:27:36.901416: step 1210, loss 0.379151, acc 0.84\n",
      "2017-10-16T15:27:37.026730: step 1211, loss 0.693348, acc 0.78\n",
      "2017-10-16T15:27:37.164224: step 1212, loss 0.418392, acc 0.86\n",
      "2017-10-16T15:27:37.284535: step 1213, loss 0.360744, acc 0.9\n",
      "2017-10-16T15:27:37.412970: step 1214, loss 0.362541, acc 0.88\n",
      "2017-10-16T15:27:37.541570: step 1215, loss 0.412213, acc 0.86\n",
      "2017-10-16T15:27:37.679108: step 1216, loss 0.372586, acc 0.78\n",
      "2017-10-16T15:27:37.807191: step 1217, loss 0.440002, acc 0.86\n",
      "2017-10-16T15:27:37.928758: step 1218, loss 0.561872, acc 0.82\n",
      "2017-10-16T15:27:38.062210: step 1219, loss 0.652543, acc 0.86\n",
      "2017-10-16T15:27:38.193967: step 1220, loss 0.31737, acc 0.86\n",
      "2017-10-16T15:27:38.323576: step 1221, loss 0.414402, acc 0.86\n",
      "2017-10-16T15:27:38.458280: step 1222, loss 0.532292, acc 0.84\n",
      "2017-10-16T15:27:38.574735: step 1223, loss 0.691849, acc 0.78\n",
      "2017-10-16T15:27:38.689042: step 1224, loss 0.205175, acc 0.9\n",
      "2017-10-16T15:27:38.805331: step 1225, loss 0.530116, acc 0.76\n",
      "2017-10-16T15:27:38.916882: step 1226, loss 0.47934, acc 0.84\n",
      "2017-10-16T15:27:39.037376: step 1227, loss 0.320788, acc 0.9\n",
      "2017-10-16T15:27:39.156865: step 1228, loss 0.51499, acc 0.84\n",
      "2017-10-16T15:27:39.271564: step 1229, loss 0.558362, acc 0.78\n",
      "2017-10-16T15:27:39.383462: step 1230, loss 0.394787, acc 0.88\n",
      "2017-10-16T15:27:39.501559: step 1231, loss 0.458875, acc 0.78\n",
      "2017-10-16T15:27:39.651483: step 1232, loss 0.578867, acc 0.84\n",
      "2017-10-16T15:27:39.779480: step 1233, loss 0.546517, acc 0.88\n",
      "2017-10-16T15:27:39.917546: step 1234, loss 0.433262, acc 0.84\n",
      "2017-10-16T15:27:40.029203: step 1235, loss 0.353706, acc 0.82\n",
      "2017-10-16T15:27:40.158097: step 1236, loss 0.361999, acc 0.84\n",
      "2017-10-16T15:27:40.290336: step 1237, loss 0.464629, acc 0.9\n",
      "2017-10-16T15:27:40.423360: step 1238, loss 0.61151, acc 0.82\n",
      "2017-10-16T15:27:40.540839: step 1239, loss 0.735653, acc 0.78\n",
      "2017-10-16T15:27:40.656014: step 1240, loss 0.498963, acc 0.84\n",
      "2017-10-16T15:27:40.772225: step 1241, loss 0.885612, acc 0.78\n",
      "2017-10-16T15:27:40.883366: step 1242, loss 0.87237, acc 0.78\n",
      "2017-10-16T15:27:41.039831: step 1243, loss 0.455026, acc 0.86\n",
      "2017-10-16T15:27:41.192947: step 1244, loss 0.461826, acc 0.9\n",
      "2017-10-16T15:27:41.318302: step 1245, loss 0.602129, acc 0.82\n",
      "2017-10-16T15:27:41.432343: step 1246, loss 0.566487, acc 0.9\n",
      "2017-10-16T15:27:41.542756: step 1247, loss 0.480131, acc 0.8\n",
      "2017-10-16T15:27:41.651223: step 1248, loss 0.38177, acc 0.86\n",
      "2017-10-16T15:27:41.767952: step 1249, loss 0.273149, acc 0.9\n",
      "2017-10-16T15:27:41.879515: step 1250, loss 0.381312, acc 0.86\n",
      "2017-10-16T15:27:41.994573: step 1251, loss 0.54389, acc 0.88\n",
      "2017-10-16T15:27:42.115704: step 1252, loss 0.460994, acc 0.84\n",
      "2017-10-16T15:27:42.226428: step 1253, loss 0.374852, acc 0.78\n",
      "2017-10-16T15:27:42.339433: step 1254, loss 0.361066, acc 0.88\n",
      "2017-10-16T15:27:42.454231: step 1255, loss 0.397835, acc 0.88\n",
      "2017-10-16T15:27:42.573960: step 1256, loss 0.435202, acc 0.88\n",
      "2017-10-16T15:27:42.686539: step 1257, loss 0.335166, acc 0.9\n",
      "2017-10-16T15:27:42.796120: step 1258, loss 0.758654, acc 0.78\n",
      "2017-10-16T15:27:42.911721: step 1259, loss 0.39342, acc 0.84\n",
      "2017-10-16T15:27:43.006311: step 1260, loss 0.451999, acc 0.857143\n",
      "2017-10-16T15:27:43.125516: step 1261, loss 0.475477, acc 0.84\n",
      "2017-10-16T15:27:43.239305: step 1262, loss 0.506312, acc 0.84\n",
      "2017-10-16T15:27:43.354106: step 1263, loss 0.678055, acc 0.8\n",
      "2017-10-16T15:27:43.494671: step 1264, loss 0.57545, acc 0.76\n",
      "2017-10-16T15:27:43.649288: step 1265, loss 0.578262, acc 0.82\n",
      "2017-10-16T15:27:43.817510: step 1266, loss 0.551988, acc 0.84\n",
      "2017-10-16T15:27:43.953300: step 1267, loss 0.603099, acc 0.78\n",
      "2017-10-16T15:27:44.085562: step 1268, loss 0.811775, acc 0.76\n",
      "2017-10-16T15:27:44.218271: step 1269, loss 0.245695, acc 0.88\n",
      "2017-10-16T15:27:44.334796: step 1270, loss 0.401483, acc 0.8\n",
      "2017-10-16T15:27:44.448363: step 1271, loss 0.523494, acc 0.82\n",
      "2017-10-16T15:27:44.561426: step 1272, loss 0.231606, acc 0.9\n",
      "2017-10-16T15:27:44.694431: step 1273, loss 0.276386, acc 0.9\n",
      "2017-10-16T15:27:44.836638: step 1274, loss 0.491742, acc 0.86\n",
      "2017-10-16T15:27:44.966290: step 1275, loss 0.602178, acc 0.74\n",
      "2017-10-16T15:27:45.103140: step 1276, loss 0.315844, acc 0.9\n",
      "2017-10-16T15:27:45.215548: step 1277, loss 0.248415, acc 0.9\n",
      "2017-10-16T15:27:45.340588: step 1278, loss 0.43736, acc 0.82\n",
      "2017-10-16T15:27:45.472476: step 1279, loss 0.444006, acc 0.88\n",
      "2017-10-16T15:27:45.588790: step 1280, loss 0.453977, acc 0.82\n",
      "2017-10-16T15:27:45.699222: step 1281, loss 0.484204, acc 0.84\n",
      "2017-10-16T15:27:45.809745: step 1282, loss 0.642084, acc 0.74\n",
      "2017-10-16T15:27:45.945473: step 1283, loss 0.693845, acc 0.76\n",
      "2017-10-16T15:27:46.081237: step 1284, loss 0.40593, acc 0.86\n",
      "2017-10-16T15:27:46.189239: step 1285, loss 0.194246, acc 0.9\n",
      "2017-10-16T15:27:46.327061: step 1286, loss 0.490682, acc 0.86\n",
      "2017-10-16T15:27:46.486329: step 1287, loss 0.178175, acc 0.94\n",
      "2017-10-16T15:27:46.639716: step 1288, loss 0.59797, acc 0.8\n",
      "2017-10-16T15:27:46.769836: step 1289, loss 0.41372, acc 0.9\n",
      "2017-10-16T15:27:46.909391: step 1290, loss 0.396425, acc 0.8\n",
      "2017-10-16T15:27:47.041527: step 1291, loss 0.517678, acc 0.84\n",
      "2017-10-16T15:27:47.168269: step 1292, loss 0.211639, acc 0.92\n",
      "2017-10-16T15:27:47.283281: step 1293, loss 0.995315, acc 0.76\n",
      "2017-10-16T15:27:47.405894: step 1294, loss 0.318669, acc 0.86\n",
      "2017-10-16T15:27:47.546861: step 1295, loss 0.605382, acc 0.84\n",
      "2017-10-16T15:27:47.666101: step 1296, loss 0.674523, acc 0.78\n",
      "2017-10-16T15:27:47.773116: step 1297, loss 0.500716, acc 0.82\n",
      "2017-10-16T15:27:47.883815: step 1298, loss 0.631413, acc 0.82\n",
      "2017-10-16T15:27:47.999794: step 1299, loss 0.818632, acc 0.72\n",
      "2017-10-16T15:27:48.116744: step 1300, loss 0.790989, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:27:50.196673: step 1300, loss 0.332874, acc 0.852581\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-1300\n",
      "\n",
      "2017-10-16T15:27:51.246853: step 1301, loss 0.374507, acc 0.8\n",
      "2017-10-16T15:27:51.405400: step 1302, loss 0.53279, acc 0.86\n",
      "2017-10-16T15:27:51.521431: step 1303, loss 0.244213, acc 0.9\n",
      "2017-10-16T15:27:51.631372: step 1304, loss 0.392716, acc 0.84\n",
      "2017-10-16T15:27:51.742187: step 1305, loss 0.366011, acc 0.82\n",
      "2017-10-16T15:27:51.857147: step 1306, loss 0.243226, acc 0.92\n",
      "2017-10-16T15:27:51.986890: step 1307, loss 0.398813, acc 0.86\n",
      "2017-10-16T15:27:52.115807: step 1308, loss 0.244591, acc 0.9\n",
      "2017-10-16T15:27:52.224048: step 1309, loss 0.604804, acc 0.78\n",
      "2017-10-16T15:27:52.333938: step 1310, loss 0.75528, acc 0.8\n",
      "2017-10-16T15:27:52.453044: step 1311, loss 0.454764, acc 0.86\n",
      "2017-10-16T15:27:52.566297: step 1312, loss 0.476758, acc 0.86\n",
      "2017-10-16T15:27:52.677232: step 1313, loss 0.300103, acc 0.88\n",
      "2017-10-16T15:27:52.789178: step 1314, loss 0.484478, acc 0.84\n",
      "2017-10-16T15:27:52.901302: step 1315, loss 0.41562, acc 0.86\n",
      "2017-10-16T15:27:53.015074: step 1316, loss 0.248047, acc 0.9\n",
      "2017-10-16T15:27:53.126966: step 1317, loss 0.579692, acc 0.76\n",
      "2017-10-16T15:27:53.238653: step 1318, loss 0.621228, acc 0.76\n",
      "2017-10-16T15:27:53.352585: step 1319, loss 0.506017, acc 0.82\n",
      "2017-10-16T15:27:53.462045: step 1320, loss 0.317329, acc 0.88\n",
      "2017-10-16T15:27:53.572681: step 1321, loss 0.387617, acc 0.86\n",
      "2017-10-16T15:27:53.683976: step 1322, loss 0.626173, acc 0.82\n",
      "2017-10-16T15:27:53.797563: step 1323, loss 0.20823, acc 0.92\n",
      "2017-10-16T15:27:53.914457: step 1324, loss 0.445755, acc 0.84\n",
      "2017-10-16T15:27:54.037266: step 1325, loss 0.586853, acc 0.78\n",
      "2017-10-16T15:27:54.165404: step 1326, loss 0.288154, acc 0.88\n",
      "2017-10-16T15:27:54.276913: step 1327, loss 0.190569, acc 0.94\n",
      "2017-10-16T15:27:54.391324: step 1328, loss 0.501613, acc 0.84\n",
      "2017-10-16T15:27:54.505010: step 1329, loss 0.503931, acc 0.82\n",
      "2017-10-16T15:27:54.629909: step 1330, loss 0.639745, acc 0.76\n",
      "2017-10-16T15:27:54.772113: step 1331, loss 0.364113, acc 0.82\n",
      "2017-10-16T15:27:54.891683: step 1332, loss 0.60371, acc 0.84\n",
      "2017-10-16T15:27:55.001213: step 1333, loss 0.426736, acc 0.84\n",
      "2017-10-16T15:27:55.114575: step 1334, loss 0.957548, acc 0.78\n",
      "2017-10-16T15:27:55.279151: step 1335, loss 0.531434, acc 0.8\n",
      "2017-10-16T15:27:55.434256: step 1336, loss 0.323235, acc 0.9\n",
      "2017-10-16T15:27:55.585114: step 1337, loss 0.49894, acc 0.84\n",
      "2017-10-16T15:27:55.765310: step 1338, loss 0.33994, acc 0.82\n",
      "2017-10-16T15:27:55.927902: step 1339, loss 0.881672, acc 0.68\n",
      "2017-10-16T15:27:56.101324: step 1340, loss 0.343124, acc 0.92\n",
      "2017-10-16T15:27:56.258681: step 1341, loss 0.39988, acc 0.84\n",
      "2017-10-16T15:27:56.408884: step 1342, loss 0.54338, acc 0.78\n",
      "2017-10-16T15:27:56.522387: step 1343, loss 0.380982, acc 0.86\n",
      "2017-10-16T15:27:56.676951: step 1344, loss 0.560008, acc 0.78\n",
      "2017-10-16T15:27:56.839977: step 1345, loss 0.62598, acc 0.82\n",
      "2017-10-16T15:27:56.995666: step 1346, loss 0.481126, acc 0.84\n",
      "2017-10-16T15:27:57.156868: step 1347, loss 0.221049, acc 0.92\n",
      "2017-10-16T15:27:57.306415: step 1348, loss 0.436493, acc 0.86\n",
      "2017-10-16T15:27:57.431079: step 1349, loss 0.515187, acc 0.78\n",
      "2017-10-16T15:27:57.543695: step 1350, loss 0.469342, acc 0.86\n",
      "2017-10-16T15:27:57.653034: step 1351, loss 0.643135, acc 0.76\n",
      "2017-10-16T15:27:57.759947: step 1352, loss 0.40063, acc 0.84\n",
      "2017-10-16T15:27:57.875188: step 1353, loss 0.183628, acc 0.92\n",
      "2017-10-16T15:27:57.998895: step 1354, loss 0.257963, acc 0.9\n",
      "2017-10-16T15:27:58.116111: step 1355, loss 0.450331, acc 0.84\n",
      "2017-10-16T15:27:58.240723: step 1356, loss 0.487576, acc 0.82\n",
      "2017-10-16T15:27:58.392511: step 1357, loss 0.273809, acc 0.88\n",
      "2017-10-16T15:27:58.519015: step 1358, loss 0.404501, acc 0.84\n",
      "2017-10-16T15:27:58.626110: step 1359, loss 0.487785, acc 0.88\n",
      "2017-10-16T15:27:58.742178: step 1360, loss 0.302328, acc 0.86\n",
      "2017-10-16T15:27:58.855051: step 1361, loss 0.566795, acc 0.8\n",
      "2017-10-16T15:27:58.969108: step 1362, loss 0.37618, acc 0.86\n",
      "2017-10-16T15:27:59.079308: step 1363, loss 0.283632, acc 0.84\n",
      "2017-10-16T15:27:59.193481: step 1364, loss 0.355071, acc 0.86\n",
      "2017-10-16T15:27:59.305904: step 1365, loss 0.812054, acc 0.8\n",
      "2017-10-16T15:27:59.417295: step 1366, loss 0.213601, acc 0.92\n",
      "2017-10-16T15:27:59.527431: step 1367, loss 0.391989, acc 0.8\n",
      "2017-10-16T15:27:59.635646: step 1368, loss 0.906434, acc 0.76\n",
      "2017-10-16T15:27:59.747641: step 1369, loss 0.460005, acc 0.8\n",
      "2017-10-16T15:27:59.854468: step 1370, loss 0.417605, acc 0.84\n",
      "2017-10-16T15:27:59.976993: step 1371, loss 0.476812, acc 0.86\n",
      "2017-10-16T15:28:00.100113: step 1372, loss 0.357574, acc 0.86\n",
      "2017-10-16T15:28:00.209877: step 1373, loss 0.253758, acc 0.88\n",
      "2017-10-16T15:28:00.315897: step 1374, loss 0.490815, acc 0.8\n",
      "2017-10-16T15:28:00.428002: step 1375, loss 0.275271, acc 0.88\n",
      "2017-10-16T15:28:00.537717: step 1376, loss 0.4838, acc 0.84\n",
      "2017-10-16T15:28:00.649283: step 1377, loss 0.37644, acc 0.84\n",
      "2017-10-16T15:28:00.759126: step 1378, loss 0.291621, acc 0.86\n",
      "2017-10-16T15:28:00.870899: step 1379, loss 0.238996, acc 0.86\n",
      "2017-10-16T15:28:00.978118: step 1380, loss 0.370414, acc 0.86\n",
      "2017-10-16T15:28:01.090584: step 1381, loss 0.410162, acc 0.92\n",
      "2017-10-16T15:28:01.199707: step 1382, loss 0.539155, acc 0.88\n",
      "2017-10-16T15:28:01.309751: step 1383, loss 0.618929, acc 0.84\n",
      "2017-10-16T15:28:01.418555: step 1384, loss 0.604153, acc 0.82\n",
      "2017-10-16T15:28:01.529326: step 1385, loss 0.476809, acc 0.84\n",
      "2017-10-16T15:28:01.621360: step 1386, loss 0.583338, acc 0.833333\n",
      "2017-10-16T15:28:01.745047: step 1387, loss 0.251174, acc 0.88\n",
      "2017-10-16T15:28:01.858287: step 1388, loss 0.482929, acc 0.88\n",
      "2017-10-16T15:28:02.022274: step 1389, loss 0.248095, acc 0.86\n",
      "2017-10-16T15:28:02.180709: step 1390, loss 0.189294, acc 0.9\n",
      "2017-10-16T15:28:02.346084: step 1391, loss 0.427264, acc 0.86\n",
      "2017-10-16T15:28:02.506414: step 1392, loss 0.386415, acc 0.84\n",
      "2017-10-16T15:28:02.649593: step 1393, loss 0.382735, acc 0.84\n",
      "2017-10-16T15:28:02.809269: step 1394, loss 0.348738, acc 0.88\n",
      "2017-10-16T15:28:02.954219: step 1395, loss 0.43244, acc 0.84\n",
      "2017-10-16T15:28:03.073731: step 1396, loss 0.563848, acc 0.82\n",
      "2017-10-16T15:28:03.242205: step 1397, loss 0.481092, acc 0.82\n",
      "2017-10-16T15:28:03.385551: step 1398, loss 0.664645, acc 0.78\n",
      "2017-10-16T15:28:03.553148: step 1399, loss 0.662702, acc 0.78\n",
      "2017-10-16T15:28:03.676466: step 1400, loss 0.340023, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:28:05.863438: step 1400, loss 0.327453, acc 0.858387\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-1400\n",
      "\n",
      "2017-10-16T15:28:06.539530: step 1401, loss 0.536694, acc 0.84\n",
      "2017-10-16T15:28:06.649737: step 1402, loss 0.41831, acc 0.82\n",
      "2017-10-16T15:28:06.758931: step 1403, loss 0.360496, acc 0.84\n",
      "2017-10-16T15:28:06.866057: step 1404, loss 0.177062, acc 0.94\n",
      "2017-10-16T15:28:06.977896: step 1405, loss 0.310528, acc 0.86\n",
      "2017-10-16T15:28:07.087272: step 1406, loss 0.38867, acc 0.86\n",
      "2017-10-16T15:28:07.197948: step 1407, loss 0.40386, acc 0.82\n",
      "2017-10-16T15:28:07.306142: step 1408, loss 0.199748, acc 0.92\n",
      "2017-10-16T15:28:07.418872: step 1409, loss 0.464779, acc 0.82\n",
      "2017-10-16T15:28:07.527921: step 1410, loss 0.577068, acc 0.8\n",
      "2017-10-16T15:28:07.640591: step 1411, loss 0.468677, acc 0.84\n",
      "2017-10-16T15:28:07.756206: step 1412, loss 0.467927, acc 0.8\n",
      "2017-10-16T15:28:07.869424: step 1413, loss 0.432327, acc 0.84\n",
      "2017-10-16T15:28:07.980130: step 1414, loss 0.423238, acc 0.8\n",
      "2017-10-16T15:28:08.093353: step 1415, loss 0.530159, acc 0.78\n",
      "2017-10-16T15:28:08.205188: step 1416, loss 0.323366, acc 0.88\n",
      "2017-10-16T15:28:08.315896: step 1417, loss 0.491709, acc 0.8\n",
      "2017-10-16T15:28:08.423466: step 1418, loss 0.329999, acc 0.86\n",
      "2017-10-16T15:28:08.537606: step 1419, loss 0.218061, acc 0.9\n",
      "2017-10-16T15:28:08.649494: step 1420, loss 0.379177, acc 0.84\n",
      "2017-10-16T15:28:08.760703: step 1421, loss 0.667551, acc 0.76\n",
      "2017-10-16T15:28:08.870658: step 1422, loss 0.431486, acc 0.86\n",
      "2017-10-16T15:28:08.985126: step 1423, loss 0.357068, acc 0.88\n",
      "2017-10-16T15:28:09.132426: step 1424, loss 0.915632, acc 0.8\n",
      "2017-10-16T15:28:09.248852: step 1425, loss 0.154396, acc 0.92\n",
      "2017-10-16T15:28:09.397911: step 1426, loss 0.455001, acc 0.8\n",
      "2017-10-16T15:28:09.537420: step 1427, loss 0.660579, acc 0.7\n",
      "2017-10-16T15:28:09.676104: step 1428, loss 0.532688, acc 0.84\n",
      "2017-10-16T15:28:09.796028: step 1429, loss 0.230192, acc 0.86\n",
      "2017-10-16T15:28:09.911496: step 1430, loss 0.430681, acc 0.82\n",
      "2017-10-16T15:28:10.021081: step 1431, loss 0.255183, acc 0.92\n",
      "2017-10-16T15:28:10.186251: step 1432, loss 0.480055, acc 0.84\n",
      "2017-10-16T15:28:10.335009: step 1433, loss 0.448065, acc 0.86\n",
      "2017-10-16T15:28:10.485190: step 1434, loss 0.492349, acc 0.84\n",
      "2017-10-16T15:28:10.623134: step 1435, loss 0.496204, acc 0.82\n",
      "2017-10-16T15:28:10.797792: step 1436, loss 0.549984, acc 0.84\n",
      "2017-10-16T15:28:10.955870: step 1437, loss 0.560077, acc 0.8\n",
      "2017-10-16T15:28:11.134807: step 1438, loss 0.395494, acc 0.82\n",
      "2017-10-16T15:28:11.286313: step 1439, loss 0.514359, acc 0.86\n",
      "2017-10-16T15:28:11.468169: step 1440, loss 0.475063, acc 0.84\n",
      "2017-10-16T15:28:11.600589: step 1441, loss 0.172062, acc 0.94\n",
      "2017-10-16T15:28:11.718543: step 1442, loss 0.46538, acc 0.86\n",
      "2017-10-16T15:28:11.829570: step 1443, loss 0.567669, acc 0.84\n",
      "2017-10-16T15:28:11.941942: step 1444, loss 0.268371, acc 0.86\n",
      "2017-10-16T15:28:12.079077: step 1445, loss 0.578888, acc 0.74\n",
      "2017-10-16T15:28:12.210338: step 1446, loss 0.378823, acc 0.86\n",
      "2017-10-16T15:28:12.327139: step 1447, loss 0.260757, acc 0.9\n",
      "2017-10-16T15:28:12.443708: step 1448, loss 0.732161, acc 0.74\n",
      "2017-10-16T15:28:12.569565: step 1449, loss 0.415911, acc 0.82\n",
      "2017-10-16T15:28:12.692122: step 1450, loss 0.35796, acc 0.88\n",
      "2017-10-16T15:28:12.801573: step 1451, loss 0.610922, acc 0.78\n",
      "2017-10-16T15:28:12.926640: step 1452, loss 0.366014, acc 0.86\n",
      "2017-10-16T15:28:13.061487: step 1453, loss 0.238137, acc 0.88\n",
      "2017-10-16T15:28:13.177098: step 1454, loss 0.558034, acc 0.86\n",
      "2017-10-16T15:28:13.359298: step 1455, loss 0.817528, acc 0.8\n",
      "2017-10-16T15:28:13.506928: step 1456, loss 0.820742, acc 0.78\n",
      "2017-10-16T15:28:13.621762: step 1457, loss 0.470795, acc 0.82\n",
      "2017-10-16T15:28:13.740148: step 1458, loss 0.381143, acc 0.88\n",
      "2017-10-16T15:28:13.867601: step 1459, loss 0.420341, acc 0.86\n",
      "2017-10-16T15:28:13.989641: step 1460, loss 0.445524, acc 0.9\n",
      "2017-10-16T15:28:14.103982: step 1461, loss 0.284544, acc 0.92\n",
      "2017-10-16T15:28:14.224992: step 1462, loss 0.50387, acc 0.84\n",
      "2017-10-16T15:28:14.414590: step 1463, loss 0.679463, acc 0.82\n",
      "2017-10-16T15:28:14.619967: step 1464, loss 0.347385, acc 0.84\n",
      "2017-10-16T15:28:14.736667: step 1465, loss 0.540684, acc 0.78\n",
      "2017-10-16T15:28:14.863346: step 1466, loss 0.46941, acc 0.88\n",
      "2017-10-16T15:28:14.978780: step 1467, loss 0.528797, acc 0.78\n",
      "2017-10-16T15:28:15.098372: step 1468, loss 0.372391, acc 0.88\n",
      "2017-10-16T15:28:15.213633: step 1469, loss 0.374182, acc 0.82\n",
      "2017-10-16T15:28:15.331653: step 1470, loss 0.423848, acc 0.78\n",
      "2017-10-16T15:28:15.445734: step 1471, loss 0.656834, acc 0.74\n",
      "2017-10-16T15:28:15.556005: step 1472, loss 0.167716, acc 0.92\n",
      "2017-10-16T15:28:15.666500: step 1473, loss 0.235666, acc 0.92\n",
      "2017-10-16T15:28:15.778661: step 1474, loss 0.278205, acc 0.92\n",
      "2017-10-16T15:28:15.892673: step 1475, loss 0.261755, acc 0.92\n",
      "2017-10-16T15:28:16.002929: step 1476, loss 0.44771, acc 0.86\n",
      "2017-10-16T15:28:16.121433: step 1477, loss 0.478841, acc 0.76\n",
      "2017-10-16T15:28:16.235286: step 1478, loss 0.410972, acc 0.86\n",
      "2017-10-16T15:28:16.345011: step 1479, loss 0.0797858, acc 0.98\n",
      "2017-10-16T15:28:16.454828: step 1480, loss 0.627468, acc 0.84\n",
      "2017-10-16T15:28:16.571544: step 1481, loss 0.515519, acc 0.78\n",
      "2017-10-16T15:28:16.701377: step 1482, loss 0.277267, acc 0.88\n",
      "2017-10-16T15:28:16.826263: step 1483, loss 0.510304, acc 0.84\n",
      "2017-10-16T15:28:16.938016: step 1484, loss 0.974145, acc 0.76\n",
      "2017-10-16T15:28:17.049457: step 1485, loss 0.703937, acc 0.76\n",
      "2017-10-16T15:28:17.167531: step 1486, loss 0.448905, acc 0.88\n",
      "2017-10-16T15:28:17.283917: step 1487, loss 0.735234, acc 0.78\n",
      "2017-10-16T15:28:17.394101: step 1488, loss 0.526412, acc 0.84\n",
      "2017-10-16T15:28:17.514139: step 1489, loss 0.416069, acc 0.84\n",
      "2017-10-16T15:28:17.687804: step 1490, loss 0.487303, acc 0.82\n",
      "2017-10-16T15:28:17.887809: step 1491, loss 0.481693, acc 0.9\n",
      "2017-10-16T15:28:18.019985: step 1492, loss 0.521414, acc 0.86\n",
      "2017-10-16T15:28:18.139602: step 1493, loss 0.549946, acc 0.82\n",
      "2017-10-16T15:28:18.261116: step 1494, loss 0.284297, acc 0.9\n",
      "2017-10-16T15:28:18.376507: step 1495, loss 0.567043, acc 0.84\n",
      "2017-10-16T15:28:18.496892: step 1496, loss 0.607164, acc 0.78\n",
      "2017-10-16T15:28:18.613216: step 1497, loss 0.411984, acc 0.86\n",
      "2017-10-16T15:28:18.727495: step 1498, loss 0.699958, acc 0.78\n",
      "2017-10-16T15:28:18.837441: step 1499, loss 0.339573, acc 0.84\n",
      "2017-10-16T15:28:18.944701: step 1500, loss 0.349059, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:28:21.755771: step 1500, loss 0.324225, acc 0.857097\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-1500\n",
      "\n",
      "2017-10-16T15:28:22.590864: step 1501, loss 0.302352, acc 0.92\n",
      "2017-10-16T15:28:22.708187: step 1502, loss 0.7513, acc 0.82\n",
      "2017-10-16T15:28:22.821432: step 1503, loss 0.473174, acc 0.78\n",
      "2017-10-16T15:28:22.934453: step 1504, loss 0.269113, acc 0.88\n",
      "2017-10-16T15:28:23.045602: step 1505, loss 0.377978, acc 0.84\n",
      "2017-10-16T15:28:23.156927: step 1506, loss 0.449334, acc 0.78\n",
      "2017-10-16T15:28:23.273507: step 1507, loss 0.433993, acc 0.78\n",
      "2017-10-16T15:28:23.384970: step 1508, loss 0.215887, acc 0.88\n",
      "2017-10-16T15:28:23.495770: step 1509, loss 0.52206, acc 0.82\n",
      "2017-10-16T15:28:23.605904: step 1510, loss 0.398834, acc 0.86\n",
      "2017-10-16T15:28:23.719731: step 1511, loss 0.432367, acc 0.8\n",
      "2017-10-16T15:28:23.827153: step 1512, loss 0.293889, acc 0.857143\n",
      "2017-10-16T15:28:23.973635: step 1513, loss 0.362552, acc 0.88\n",
      "2017-10-16T15:28:24.119636: step 1514, loss 0.365335, acc 0.88\n",
      "2017-10-16T15:28:24.231005: step 1515, loss 0.437237, acc 0.9\n",
      "2017-10-16T15:28:24.344599: step 1516, loss 0.280168, acc 0.9\n",
      "2017-10-16T15:28:24.464017: step 1517, loss 0.180544, acc 0.9\n",
      "2017-10-16T15:28:24.634419: step 1518, loss 0.284039, acc 0.94\n",
      "2017-10-16T15:28:24.779122: step 1519, loss 0.531306, acc 0.84\n",
      "2017-10-16T15:28:24.902020: step 1520, loss 0.0995417, acc 0.94\n",
      "2017-10-16T15:28:25.012187: step 1521, loss 0.48134, acc 0.82\n",
      "2017-10-16T15:28:25.121300: step 1522, loss 0.422648, acc 0.86\n",
      "2017-10-16T15:28:25.235666: step 1523, loss 0.375082, acc 0.84\n",
      "2017-10-16T15:28:25.346107: step 1524, loss 0.541925, acc 0.88\n",
      "2017-10-16T15:28:25.461056: step 1525, loss 0.401492, acc 0.88\n",
      "2017-10-16T15:28:25.572592: step 1526, loss 0.194953, acc 0.86\n",
      "2017-10-16T15:28:25.686381: step 1527, loss 0.546511, acc 0.84\n",
      "2017-10-16T15:28:25.796177: step 1528, loss 0.245271, acc 0.88\n",
      "2017-10-16T15:28:25.912422: step 1529, loss 0.303352, acc 0.82\n",
      "2017-10-16T15:28:26.024322: step 1530, loss 0.0999281, acc 0.98\n",
      "2017-10-16T15:28:26.195583: step 1531, loss 0.187745, acc 0.94\n",
      "2017-10-16T15:28:26.329331: step 1532, loss 0.386576, acc 0.88\n",
      "2017-10-16T15:28:26.440075: step 1533, loss 0.534307, acc 0.8\n",
      "2017-10-16T15:28:26.565839: step 1534, loss 0.307152, acc 0.88\n",
      "2017-10-16T15:28:26.695312: step 1535, loss 0.454578, acc 0.88\n",
      "2017-10-16T15:28:26.840434: step 1536, loss 0.37688, acc 0.86\n",
      "2017-10-16T15:28:26.963325: step 1537, loss 0.605141, acc 0.76\n",
      "2017-10-16T15:28:27.108648: step 1538, loss 0.459385, acc 0.8\n",
      "2017-10-16T15:28:27.227207: step 1539, loss 0.361092, acc 0.84\n",
      "2017-10-16T15:28:27.355814: step 1540, loss 0.429771, acc 0.82\n",
      "2017-10-16T15:28:27.475849: step 1541, loss 0.325579, acc 0.84\n",
      "2017-10-16T15:28:27.597516: step 1542, loss 0.293087, acc 0.9\n",
      "2017-10-16T15:28:27.716174: step 1543, loss 0.533589, acc 0.86\n",
      "2017-10-16T15:28:27.833048: step 1544, loss 0.612373, acc 0.8\n",
      "2017-10-16T15:28:27.943299: step 1545, loss 0.349535, acc 0.92\n",
      "2017-10-16T15:28:28.056904: step 1546, loss 0.580163, acc 0.76\n",
      "2017-10-16T15:28:28.214179: step 1547, loss 0.621559, acc 0.82\n",
      "2017-10-16T15:28:28.346278: step 1548, loss 0.197785, acc 0.94\n",
      "2017-10-16T15:28:28.454008: step 1549, loss 0.294445, acc 0.88\n",
      "2017-10-16T15:28:28.564123: step 1550, loss 0.519043, acc 0.88\n",
      "2017-10-16T15:28:28.678563: step 1551, loss 0.598461, acc 0.76\n",
      "2017-10-16T15:28:28.795390: step 1552, loss 0.284551, acc 0.86\n",
      "2017-10-16T15:28:28.906198: step 1553, loss 0.635032, acc 0.76\n",
      "2017-10-16T15:28:29.042400: step 1554, loss 0.278006, acc 0.92\n",
      "2017-10-16T15:28:29.161686: step 1555, loss 0.508548, acc 0.82\n",
      "2017-10-16T15:28:29.284271: step 1556, loss 0.370534, acc 0.86\n",
      "2017-10-16T15:28:29.401754: step 1557, loss 0.576478, acc 0.82\n",
      "2017-10-16T15:28:29.533308: step 1558, loss 0.170005, acc 0.92\n",
      "2017-10-16T15:28:29.662457: step 1559, loss 0.362222, acc 0.82\n",
      "2017-10-16T15:28:29.780082: step 1560, loss 0.495891, acc 0.86\n",
      "2017-10-16T15:28:29.917622: step 1561, loss 0.356532, acc 0.88\n",
      "2017-10-16T15:28:30.055265: step 1562, loss 0.386701, acc 0.88\n",
      "2017-10-16T15:28:30.176605: step 1563, loss 0.545373, acc 0.82\n",
      "2017-10-16T15:28:30.294809: step 1564, loss 0.756224, acc 0.8\n",
      "2017-10-16T15:28:30.415903: step 1565, loss 0.491276, acc 0.88\n",
      "2017-10-16T15:28:30.529015: step 1566, loss 0.313447, acc 0.86\n",
      "2017-10-16T15:28:30.677866: step 1567, loss 0.391755, acc 0.78\n",
      "2017-10-16T15:28:30.818518: step 1568, loss 0.464217, acc 0.8\n",
      "2017-10-16T15:28:30.926485: step 1569, loss 0.427886, acc 0.84\n",
      "2017-10-16T15:28:31.035914: step 1570, loss 0.690714, acc 0.8\n",
      "2017-10-16T15:28:31.149975: step 1571, loss 0.251329, acc 0.86\n",
      "2017-10-16T15:28:31.259791: step 1572, loss 0.423625, acc 0.84\n",
      "2017-10-16T15:28:31.372092: step 1573, loss 0.406678, acc 0.86\n",
      "2017-10-16T15:28:31.482341: step 1574, loss 0.470897, acc 0.82\n",
      "2017-10-16T15:28:31.593891: step 1575, loss 0.548575, acc 0.86\n",
      "2017-10-16T15:28:31.715017: step 1576, loss 0.328192, acc 0.9\n",
      "2017-10-16T15:28:31.827277: step 1577, loss 0.274052, acc 0.84\n",
      "2017-10-16T15:28:31.935559: step 1578, loss 0.40643, acc 0.9\n",
      "2017-10-16T15:28:32.047634: step 1579, loss 0.310832, acc 0.9\n",
      "2017-10-16T15:28:32.154946: step 1580, loss 0.418513, acc 0.88\n",
      "2017-10-16T15:28:32.268088: step 1581, loss 0.39581, acc 0.86\n",
      "2017-10-16T15:28:32.380368: step 1582, loss 0.647821, acc 0.76\n",
      "2017-10-16T15:28:32.494444: step 1583, loss 0.453886, acc 0.86\n",
      "2017-10-16T15:28:32.621633: step 1584, loss 0.542031, acc 0.78\n",
      "2017-10-16T15:28:32.758855: step 1585, loss 0.50019, acc 0.78\n",
      "2017-10-16T15:28:32.875397: step 1586, loss 0.186839, acc 0.9\n",
      "2017-10-16T15:28:32.980722: step 1587, loss 0.170843, acc 0.94\n",
      "2017-10-16T15:28:33.089860: step 1588, loss 0.489793, acc 0.84\n",
      "2017-10-16T15:28:33.209153: step 1589, loss 0.678362, acc 0.76\n",
      "2017-10-16T15:28:33.319466: step 1590, loss 0.566163, acc 0.76\n",
      "2017-10-16T15:28:33.432938: step 1591, loss 0.301102, acc 0.9\n",
      "2017-10-16T15:28:33.545994: step 1592, loss 0.146013, acc 0.94\n",
      "2017-10-16T15:28:33.660731: step 1593, loss 0.57889, acc 0.74\n",
      "2017-10-16T15:28:33.787975: step 1594, loss 0.507314, acc 0.86\n",
      "2017-10-16T15:28:33.911511: step 1595, loss 0.279055, acc 0.88\n",
      "2017-10-16T15:28:34.023657: step 1596, loss 0.451209, acc 0.82\n",
      "2017-10-16T15:28:34.132923: step 1597, loss 0.499154, acc 0.78\n",
      "2017-10-16T15:28:34.241233: step 1598, loss 0.463323, acc 0.8\n",
      "2017-10-16T15:28:34.368566: step 1599, loss 0.296585, acc 0.88\n",
      "2017-10-16T15:28:34.480826: step 1600, loss 0.588076, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:28:36.527068: step 1600, loss 0.322211, acc 0.857419\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-1600\n",
      "\n",
      "2017-10-16T15:28:37.362133: step 1601, loss 0.461591, acc 0.84\n",
      "2017-10-16T15:28:37.510471: step 1602, loss 0.258852, acc 0.88\n",
      "2017-10-16T15:28:37.638108: step 1603, loss 0.373057, acc 0.84\n",
      "2017-10-16T15:28:37.773020: step 1604, loss 0.417399, acc 0.86\n",
      "2017-10-16T15:28:37.917139: step 1605, loss 0.293086, acc 0.88\n",
      "2017-10-16T15:28:38.081044: step 1606, loss 0.512089, acc 0.84\n",
      "2017-10-16T15:28:38.228712: step 1607, loss 0.508987, acc 0.8\n",
      "2017-10-16T15:28:38.364636: step 1608, loss 0.567781, acc 0.82\n",
      "2017-10-16T15:28:38.475130: step 1609, loss 0.199109, acc 0.9\n",
      "2017-10-16T15:28:38.587310: step 1610, loss 0.251567, acc 0.92\n",
      "2017-10-16T15:28:38.700688: step 1611, loss 0.239009, acc 0.9\n",
      "2017-10-16T15:28:38.818914: step 1612, loss 0.696291, acc 0.82\n",
      "2017-10-16T15:28:38.960972: step 1613, loss 0.578012, acc 0.82\n",
      "2017-10-16T15:28:39.112964: step 1614, loss 0.733169, acc 0.76\n",
      "2017-10-16T15:28:39.232264: step 1615, loss 0.495575, acc 0.84\n",
      "2017-10-16T15:28:39.345759: step 1616, loss 0.551784, acc 0.8\n",
      "2017-10-16T15:28:39.457870: step 1617, loss 0.595264, acc 0.8\n",
      "2017-10-16T15:28:39.612831: step 1618, loss 0.507703, acc 0.76\n",
      "2017-10-16T15:28:39.740644: step 1619, loss 0.625632, acc 0.8\n",
      "2017-10-16T15:28:39.874712: step 1620, loss 0.208601, acc 0.88\n",
      "2017-10-16T15:28:40.002622: step 1621, loss 0.444374, acc 0.86\n",
      "2017-10-16T15:28:40.122245: step 1622, loss 0.462591, acc 0.86\n",
      "2017-10-16T15:28:40.229610: step 1623, loss 0.695647, acc 0.74\n",
      "2017-10-16T15:28:40.361214: step 1624, loss 0.568072, acc 0.78\n",
      "2017-10-16T15:28:40.476647: step 1625, loss 0.303821, acc 0.9\n",
      "2017-10-16T15:28:40.596260: step 1626, loss 0.289878, acc 0.9\n",
      "2017-10-16T15:28:40.728386: step 1627, loss 0.485272, acc 0.82\n",
      "2017-10-16T15:28:40.890297: step 1628, loss 0.567151, acc 0.84\n",
      "2017-10-16T15:28:41.028024: step 1629, loss 0.306332, acc 0.86\n",
      "2017-10-16T15:28:41.174160: step 1630, loss 0.409684, acc 0.8\n",
      "2017-10-16T15:28:41.308927: step 1631, loss 0.371823, acc 0.88\n",
      "2017-10-16T15:28:41.464655: step 1632, loss 0.286238, acc 0.9\n",
      "2017-10-16T15:28:41.602031: step 1633, loss 0.471164, acc 0.88\n",
      "2017-10-16T15:28:41.725384: step 1634, loss 0.423522, acc 0.86\n",
      "2017-10-16T15:28:41.833816: step 1635, loss 0.263904, acc 0.88\n",
      "2017-10-16T15:28:41.965041: step 1636, loss 0.406875, acc 0.88\n",
      "2017-10-16T15:28:42.088609: step 1637, loss 0.455168, acc 0.8\n",
      "2017-10-16T15:28:42.189566: step 1638, loss 0.14048, acc 0.928571\n",
      "2017-10-16T15:28:42.311091: step 1639, loss 0.357351, acc 0.86\n",
      "2017-10-16T15:28:42.429633: step 1640, loss 0.147837, acc 0.94\n",
      "2017-10-16T15:28:42.564769: step 1641, loss 0.401671, acc 0.82\n",
      "2017-10-16T15:28:42.697986: step 1642, loss 0.337983, acc 0.9\n",
      "2017-10-16T15:28:42.808044: step 1643, loss 0.347386, acc 0.82\n",
      "2017-10-16T15:28:42.918227: step 1644, loss 0.57199, acc 0.78\n",
      "2017-10-16T15:28:43.034287: step 1645, loss 0.422197, acc 0.88\n",
      "2017-10-16T15:28:43.172589: step 1646, loss 0.290107, acc 0.86\n",
      "2017-10-16T15:28:43.287774: step 1647, loss 0.528135, acc 0.76\n",
      "2017-10-16T15:28:43.411265: step 1648, loss 0.500416, acc 0.82\n",
      "2017-10-16T15:28:43.528517: step 1649, loss 0.219537, acc 0.88\n",
      "2017-10-16T15:28:43.650707: step 1650, loss 0.446283, acc 0.8\n",
      "2017-10-16T15:28:43.783861: step 1651, loss 0.36888, acc 0.86\n",
      "2017-10-16T15:28:43.915670: step 1652, loss 0.561918, acc 0.8\n",
      "2017-10-16T15:28:44.058622: step 1653, loss 0.464133, acc 0.84\n",
      "2017-10-16T15:28:44.193652: step 1654, loss 0.419263, acc 0.88\n",
      "2017-10-16T15:28:44.319865: step 1655, loss 0.415151, acc 0.84\n",
      "2017-10-16T15:28:44.447671: step 1656, loss 0.447212, acc 0.84\n",
      "2017-10-16T15:28:44.561288: step 1657, loss 0.393876, acc 0.88\n",
      "2017-10-16T15:28:44.688733: step 1658, loss 0.498996, acc 0.82\n",
      "2017-10-16T15:28:44.798025: step 1659, loss 0.351694, acc 0.82\n",
      "2017-10-16T15:28:44.933730: step 1660, loss 0.348292, acc 0.86\n",
      "2017-10-16T15:28:45.045753: step 1661, loss 0.307147, acc 0.86\n",
      "2017-10-16T15:28:45.160964: step 1662, loss 0.378642, acc 0.86\n",
      "2017-10-16T15:28:45.280109: step 1663, loss 0.356285, acc 0.86\n",
      "2017-10-16T15:28:45.398166: step 1664, loss 0.704937, acc 0.78\n",
      "2017-10-16T15:28:45.524400: step 1665, loss 0.340145, acc 0.88\n",
      "2017-10-16T15:28:45.673649: step 1666, loss 0.372787, acc 0.9\n",
      "2017-10-16T15:28:45.788001: step 1667, loss 0.423593, acc 0.82\n",
      "2017-10-16T15:28:45.921527: step 1668, loss 0.365958, acc 0.86\n",
      "2017-10-16T15:28:46.032127: step 1669, loss 0.330208, acc 0.92\n",
      "2017-10-16T15:28:46.144023: step 1670, loss 0.38571, acc 0.9\n",
      "2017-10-16T15:28:46.256545: step 1671, loss 0.341548, acc 0.9\n",
      "2017-10-16T15:28:46.374692: step 1672, loss 0.499696, acc 0.78\n",
      "2017-10-16T15:28:46.528218: step 1673, loss 0.122465, acc 0.92\n",
      "2017-10-16T15:28:46.675675: step 1674, loss 0.620671, acc 0.8\n",
      "2017-10-16T15:28:46.793494: step 1675, loss 0.349254, acc 0.86\n",
      "2017-10-16T15:28:46.911686: step 1676, loss 0.417863, acc 0.86\n",
      "2017-10-16T15:28:47.074282: step 1677, loss 0.45398, acc 0.82\n",
      "2017-10-16T15:28:47.225207: step 1678, loss 0.408735, acc 0.84\n",
      "2017-10-16T15:28:47.346646: step 1679, loss 0.179893, acc 0.9\n",
      "2017-10-16T15:28:47.499923: step 1680, loss 0.387204, acc 0.84\n",
      "2017-10-16T15:28:47.663499: step 1681, loss 0.244399, acc 0.9\n",
      "2017-10-16T15:28:47.800679: step 1682, loss 0.328765, acc 0.9\n",
      "2017-10-16T15:28:47.910312: step 1683, loss 0.250027, acc 0.86\n",
      "2017-10-16T15:28:48.020440: step 1684, loss 0.33386, acc 0.86\n",
      "2017-10-16T15:28:48.168009: step 1685, loss 0.424999, acc 0.84\n",
      "2017-10-16T15:28:48.294864: step 1686, loss 0.45664, acc 0.86\n",
      "2017-10-16T15:28:48.403167: step 1687, loss 0.219259, acc 0.92\n",
      "2017-10-16T15:28:48.515441: step 1688, loss 0.160545, acc 0.94\n",
      "2017-10-16T15:28:48.646047: step 1689, loss 0.324471, acc 0.86\n",
      "2017-10-16T15:28:48.765028: step 1690, loss 0.909594, acc 0.76\n",
      "2017-10-16T15:28:48.877407: step 1691, loss 0.747232, acc 0.76\n",
      "2017-10-16T15:28:48.987571: step 1692, loss 0.395484, acc 0.88\n",
      "2017-10-16T15:28:49.104314: step 1693, loss 0.379579, acc 0.86\n",
      "2017-10-16T15:28:49.217489: step 1694, loss 0.193937, acc 0.94\n",
      "2017-10-16T15:28:49.352444: step 1695, loss 0.329969, acc 0.88\n",
      "2017-10-16T15:28:49.485335: step 1696, loss 0.364112, acc 0.86\n",
      "2017-10-16T15:28:49.593424: step 1697, loss 0.287889, acc 0.92\n",
      "2017-10-16T15:28:49.766501: step 1698, loss 0.3418, acc 0.86\n",
      "2017-10-16T15:28:49.905360: step 1699, loss 0.381508, acc 0.8\n",
      "2017-10-16T15:28:50.026674: step 1700, loss 0.352377, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:28:52.379031: step 1700, loss 0.317809, acc 0.861935\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-1700\n",
      "\n",
      "2017-10-16T15:28:53.378567: step 1701, loss 0.703527, acc 0.82\n",
      "2017-10-16T15:28:53.514578: step 1702, loss 0.407874, acc 0.82\n",
      "2017-10-16T15:28:53.643775: step 1703, loss 0.620841, acc 0.8\n",
      "2017-10-16T15:28:53.780579: step 1704, loss 0.190151, acc 0.9\n",
      "2017-10-16T15:28:53.892187: step 1705, loss 0.237995, acc 0.88\n",
      "2017-10-16T15:28:54.008149: step 1706, loss 0.334122, acc 0.86\n",
      "2017-10-16T15:28:54.128582: step 1707, loss 0.663129, acc 0.74\n",
      "2017-10-16T15:28:54.241680: step 1708, loss 0.445197, acc 0.86\n",
      "2017-10-16T15:28:54.373755: step 1709, loss 0.406131, acc 0.86\n",
      "2017-10-16T15:28:54.488404: step 1710, loss 0.505282, acc 0.84\n",
      "2017-10-16T15:28:54.600813: step 1711, loss 0.394777, acc 0.82\n",
      "2017-10-16T15:28:54.712912: step 1712, loss 0.421708, acc 0.88\n",
      "2017-10-16T15:28:54.831342: step 1713, loss 0.235973, acc 0.92\n",
      "2017-10-16T15:28:54.942988: step 1714, loss 0.554797, acc 0.76\n",
      "2017-10-16T15:28:55.075032: step 1715, loss 0.204789, acc 0.9\n",
      "2017-10-16T15:28:55.187763: step 1716, loss 0.401192, acc 0.82\n",
      "2017-10-16T15:28:55.300262: step 1717, loss 0.211279, acc 0.92\n",
      "2017-10-16T15:28:55.408826: step 1718, loss 0.461327, acc 0.8\n",
      "2017-10-16T15:28:55.525916: step 1719, loss 0.261358, acc 0.88\n",
      "2017-10-16T15:28:55.637002: step 1720, loss 0.260557, acc 0.88\n",
      "2017-10-16T15:28:55.750983: step 1721, loss 0.518001, acc 0.86\n",
      "2017-10-16T15:28:55.858388: step 1722, loss 0.222788, acc 0.92\n",
      "2017-10-16T15:28:55.974844: step 1723, loss 0.606626, acc 0.8\n",
      "2017-10-16T15:28:56.083484: step 1724, loss 0.320808, acc 0.92\n",
      "2017-10-16T15:28:56.199785: step 1725, loss 0.190672, acc 0.96\n",
      "2017-10-16T15:28:56.308191: step 1726, loss 0.264205, acc 0.86\n",
      "2017-10-16T15:28:56.423699: step 1727, loss 0.492819, acc 0.76\n",
      "2017-10-16T15:28:56.541285: step 1728, loss 0.619845, acc 0.8\n",
      "2017-10-16T15:28:56.658987: step 1729, loss 0.293975, acc 0.9\n",
      "2017-10-16T15:28:56.779361: step 1730, loss 0.18911, acc 0.92\n",
      "2017-10-16T15:28:56.917114: step 1731, loss 0.392929, acc 0.86\n",
      "2017-10-16T15:28:57.050516: step 1732, loss 0.59272, acc 0.78\n",
      "2017-10-16T15:28:57.159182: step 1733, loss 0.348519, acc 0.88\n",
      "2017-10-16T15:28:57.274239: step 1734, loss 0.283955, acc 0.82\n",
      "2017-10-16T15:28:57.389606: step 1735, loss 0.319769, acc 0.86\n",
      "2017-10-16T15:28:57.497934: step 1736, loss 0.543348, acc 0.8\n",
      "2017-10-16T15:28:57.626116: step 1737, loss 0.559173, acc 0.88\n",
      "2017-10-16T15:28:57.802357: step 1738, loss 0.432125, acc 0.78\n",
      "2017-10-16T15:28:57.910472: step 1739, loss 0.411483, acc 0.84\n",
      "2017-10-16T15:28:58.017864: step 1740, loss 0.174867, acc 0.94\n",
      "2017-10-16T15:28:58.137524: step 1741, loss 0.469344, acc 0.8\n",
      "2017-10-16T15:28:58.256275: step 1742, loss 0.425605, acc 0.82\n",
      "2017-10-16T15:28:58.376478: step 1743, loss 0.672989, acc 0.8\n",
      "2017-10-16T15:28:58.488308: step 1744, loss 0.72071, acc 0.8\n",
      "2017-10-16T15:28:58.602802: step 1745, loss 0.227987, acc 0.9\n",
      "2017-10-16T15:28:58.716603: step 1746, loss 0.476379, acc 0.84\n",
      "2017-10-16T15:28:58.840598: step 1747, loss 0.302963, acc 0.9\n",
      "2017-10-16T15:28:58.953214: step 1748, loss 0.616848, acc 0.78\n",
      "2017-10-16T15:28:59.069852: step 1749, loss 0.496744, acc 0.8\n",
      "2017-10-16T15:28:59.181734: step 1750, loss 0.310618, acc 0.88\n",
      "2017-10-16T15:28:59.298420: step 1751, loss 0.588742, acc 0.8\n",
      "2017-10-16T15:28:59.407465: step 1752, loss 0.667852, acc 0.74\n",
      "2017-10-16T15:28:59.526173: step 1753, loss 0.189801, acc 0.92\n",
      "2017-10-16T15:28:59.682268: step 1754, loss 0.527059, acc 0.78\n",
      "2017-10-16T15:28:59.847801: step 1755, loss 0.315709, acc 0.84\n",
      "2017-10-16T15:29:00.001232: step 1756, loss 0.433154, acc 0.82\n",
      "2017-10-16T15:29:00.133680: step 1757, loss 0.386057, acc 0.88\n",
      "2017-10-16T15:29:00.250789: step 1758, loss 0.609399, acc 0.78\n",
      "2017-10-16T15:29:00.367790: step 1759, loss 0.325813, acc 0.86\n",
      "2017-10-16T15:29:00.507062: step 1760, loss 0.256713, acc 0.9\n",
      "2017-10-16T15:29:00.651622: step 1761, loss 0.41821, acc 0.84\n",
      "2017-10-16T15:29:00.780640: step 1762, loss 0.18123, acc 0.9\n",
      "2017-10-16T15:29:00.916514: step 1763, loss 0.142042, acc 0.96\n",
      "2017-10-16T15:29:01.032943: step 1764, loss 0.287428, acc 0.904762\n",
      "2017-10-16T15:29:01.169422: step 1765, loss 0.346473, acc 0.88\n",
      "2017-10-16T15:29:01.291157: step 1766, loss 0.458105, acc 0.86\n",
      "2017-10-16T15:29:01.419985: step 1767, loss 0.297424, acc 0.92\n",
      "2017-10-16T15:29:01.558373: step 1768, loss 0.28108, acc 0.9\n",
      "2017-10-16T15:29:01.689957: step 1769, loss 0.339777, acc 0.88\n",
      "2017-10-16T15:29:01.814473: step 1770, loss 0.483604, acc 0.82\n",
      "2017-10-16T15:29:01.950627: step 1771, loss 0.576466, acc 0.82\n",
      "2017-10-16T15:29:02.080124: step 1772, loss 0.313808, acc 0.88\n",
      "2017-10-16T15:29:02.200603: step 1773, loss 0.291279, acc 0.94\n",
      "2017-10-16T15:29:02.332401: step 1774, loss 0.320459, acc 0.9\n",
      "2017-10-16T15:29:02.463118: step 1775, loss 0.274309, acc 0.9\n",
      "2017-10-16T15:29:02.591608: step 1776, loss 0.327644, acc 0.86\n",
      "2017-10-16T15:29:02.704569: step 1777, loss 0.401067, acc 0.86\n",
      "2017-10-16T15:29:02.820746: step 1778, loss 0.301811, acc 0.84\n",
      "2017-10-16T15:29:02.959152: step 1779, loss 0.231097, acc 0.92\n",
      "2017-10-16T15:29:03.093356: step 1780, loss 0.178124, acc 0.92\n",
      "2017-10-16T15:29:03.221747: step 1781, loss 0.284167, acc 0.86\n",
      "2017-10-16T15:29:03.352097: step 1782, loss 0.571128, acc 0.76\n",
      "2017-10-16T15:29:03.472772: step 1783, loss 0.556364, acc 0.8\n",
      "2017-10-16T15:29:03.626069: step 1784, loss 0.517679, acc 0.76\n",
      "2017-10-16T15:29:03.739720: step 1785, loss 0.465874, acc 0.86\n",
      "2017-10-16T15:29:03.850812: step 1786, loss 0.430449, acc 0.84\n",
      "2017-10-16T15:29:03.972967: step 1787, loss 0.527014, acc 0.84\n",
      "2017-10-16T15:29:04.084197: step 1788, loss 0.489102, acc 0.78\n",
      "2017-10-16T15:29:04.200494: step 1789, loss 0.401034, acc 0.92\n",
      "2017-10-16T15:29:04.311499: step 1790, loss 0.296834, acc 0.86\n",
      "2017-10-16T15:29:04.427136: step 1791, loss 0.380774, acc 0.86\n",
      "2017-10-16T15:29:04.537704: step 1792, loss 0.309149, acc 0.88\n",
      "2017-10-16T15:29:04.654516: step 1793, loss 0.30757, acc 0.88\n",
      "2017-10-16T15:29:04.768406: step 1794, loss 0.353605, acc 0.88\n",
      "2017-10-16T15:29:04.884955: step 1795, loss 0.252413, acc 0.92\n",
      "2017-10-16T15:29:04.993798: step 1796, loss 0.557108, acc 0.74\n",
      "2017-10-16T15:29:05.109453: step 1797, loss 0.37998, acc 0.9\n",
      "2017-10-16T15:29:05.218557: step 1798, loss 0.560347, acc 0.78\n",
      "2017-10-16T15:29:05.333800: step 1799, loss 0.217097, acc 0.9\n",
      "2017-10-16T15:29:05.450183: step 1800, loss 0.515548, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2017-10-16T15:29:07.499051: step 1800, loss 0.321703, acc 0.858065\n",
      "\n",
      "Saved model checkpoint to /Users/helenahuddy/CNN/runs/1508156672/checkpoints/model-1800\n",
      "\n",
      "2017-10-16T15:29:08.393843: step 1801, loss 0.572745, acc 0.8\n",
      "2017-10-16T15:29:08.529279: step 1802, loss 0.234613, acc 0.94\n",
      "2017-10-16T15:29:08.658154: step 1803, loss 0.525649, acc 0.82\n",
      "2017-10-16T15:29:08.796828: step 1804, loss 0.495127, acc 0.82\n",
      "2017-10-16T15:29:08.924385: step 1805, loss 0.15618, acc 0.94\n",
      "2017-10-16T15:29:09.050322: step 1806, loss 0.347866, acc 0.82\n",
      "2017-10-16T15:29:09.165334: step 1807, loss 0.430545, acc 0.82\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e877071d8246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-e877071d8246>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch)\u001b[0m\n\u001b[1;32m     60\u001b[0m             _, step, summaries, loss, accuracy = sess.run(\n\u001b[1;32m     61\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        \n",
    "        cnn = conv.TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters)\n",
    "        \n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        \n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    " \n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    " \n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph_def)\n",
    " \n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph_def)\n",
    "        \n",
    "        \n",
    "        # Checkpointing\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        # Tensorflow assumes this directory already exists so we need to create it\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        \n",
    "        \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        \n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x_batch,\n",
    "                cnn.input_y: y_batch,\n",
    "                cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "            \n",
    "            \n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "                \n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            zip(x_train, y_train), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMBED_DIM = 300\n",
    "HIDDEN_SIZE = 300\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "batch_ph   = tf.placeholder(tf.int32, [None, None])\n",
    "target_ph  = tf.placeholder(tf.float32, [None, NUM_CLASSES])\n",
    "seq_len_ph = tf.placeholder(tf.int32, [None])\n",
    "keep_prob_ph = tf.placeholder(tf.float32)\n",
    "\n",
    "embeddings_ph = tf.placeholder(tf.float32, [len(vocab_processor.vocabulary_), EMBED_DIM])\n",
    "embeddings_var = tf.Variable(tf.constant(0., shape=[len(vocab_processor.vocabulary_), EMBED_DIM]), trainable=False)\n",
    "init_embeddings = embeddings_var.assign(embeddings_ph)\n",
    "batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)\n",
    "    \n",
    "# Bi-RNN layers\n",
    "outputs,_ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE),\n",
    "                   inputs=batch_embedded,sequence_length=seq_len_ph, dtype=tf.float32, scope=\"bi_rnn1\")  \n",
    "outputs = tf.concat(outputs, 2)\n",
    "# outputs,_ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE),\n",
    "#                          inputs=outputs,sequence_length=seq_len_ph, dtype=tf.float32, scope=\"bi_rnn2\")\n",
    "# outputs = tf.concat(outputs, 2)\n",
    "\n",
    "# Last output of Bi-RNN\n",
    "output = outputs[:,0,:]\n",
    "\n",
    "# Dropout\n",
    "drop = tf.nn.dropout(output, keep_prob_ph)\n",
    "\n",
    "# Fully connected layer\n",
    "W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE * 2, NUM_CLASSES], stddev=0.1), name=\"W\")\n",
    "b = tf.Variable(tf.constant(0., shape=[NUM_CLASSES]), name=\"b\")\n",
    "y_hat = tf.nn.xw_plus_b(drop, W, b, name=\"scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adam parameters\n",
    "LEARNING_RATE = 1e-3\n",
    "EPSILON = 1e-5\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.9\n",
    "# L2 regularization coefficient\n",
    "BETA = 0\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat, labels=target_ph),\n",
    "                               name=\"cross_entropy\")\n",
    "l2_loss = tf.nn.l2_loss(W, name=\"l2_loss\")\n",
    "loss = cross_entropy + l2_loss * BETA\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE, beta1=BETA1, beta2=BETA2,\n",
    "                                   epsilon=EPSILON).minimize(loss)\n",
    "# optimizer = tf.train.MomentumOptimizer(learning_rate=1e-1, momentum=0.1).minimize(loss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(target_ph, 1), tf.argmax(y_hat, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size):\n",
    "    size = X.shape[0]\n",
    "    X_copy = X.copy()\n",
    "    y_copy = y.copy()\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "    X_copy = X_copy[indices]\n",
    "    y_copy = y_copy[indices]\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i + batch_size <= size:\n",
    "            yield X_copy[i:i + batch_size], y_copy[i:i + batch_size]\n",
    "            i += batch_size\n",
    "        else:\n",
    "            i = 0\n",
    "            indices = np.arange(size)\n",
    "            np.random.shuffle(indices)\n",
    "            X_copy = X_copy[indices]\n",
    "            y_copy = y_copy[indices]\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'eos_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-eff0bf26e501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mseq_len_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meos_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 sess.run(optimizer, feed_dict={batch_ph: x_batch, target_ph: y_batch,\n\u001b[1;32m     43\u001b[0m                                                seq_len_ph: seq_len_tr, keep_prob_ph: DROPOUT})\n",
      "\u001b[0;32m<ipython-input-76-eff0bf26e501>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mseq_len_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meos_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 sess.run(optimizer, feed_dict={batch_ph: x_batch, target_ph: y_batch,\n\u001b[1;32m     43\u001b[0m                                                seq_len_ph: seq_len_tr, keep_prob_ph: DROPOUT})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eos_id' is not defined"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "FOLDS = 10\n",
    "\n",
    "DROPOUT = 0.5  # Probability of keeping a neuron\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "skf = KFold(FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "results = []\n",
    "\n",
    "for train_index, test_index in skf.split(x_bag, y_text):\n",
    "    X_tr, X_val = np.array([x_bag[i] for i in train_index]), np.array([x_bag[i] for i in test_index])\n",
    "    y_tr, y_val = np.array([y_text[i] for i in train_index]), np.array([y_text[i] for i in test_index])\n",
    "    \n",
    "    train_batch_generator = batch_generator(X_tr, y_tr, BATCH_SIZE)\n",
    "\n",
    "    loss_tr_l = []\n",
    "    loss_val_l = []\n",
    "    ce_tr_l = []  # Cross-entropy\n",
    "    ce_val_l = []\n",
    "    acc_tr_l = []  # Accuracy\n",
    "    acc_val_l = []\n",
    "    f_macro_tr_l = []\n",
    "    f_macro_val_l = []\n",
    "    f_fair_tr_l = []\n",
    "    f_fair_val_l = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #sess.run(init_embeddings, feed_dict={embeddings_ph: embeddings})\n",
    "        print (\"Start learning...\")\n",
    "        for epoch in range(EPOCHS):\n",
    "            for i in range(int(X_tr.shape[0] / BATCH_SIZE)):\n",
    "                x_batch, y_batch = next(train_batch_generator)\n",
    "                seq_len_tr = np.array([list(x).index(eos_id) + 1 for x in x_batch])\n",
    "                sess.run(optimizer, feed_dict={batch_ph: x_batch, target_ph: y_batch,\n",
    "                                               seq_len_ph: seq_len_tr, keep_prob_ph: DROPOUT})\n",
    "\n",
    "            y_pred_tr, ce_tr, loss_tr, acc_tr = sess.run([y_hat, cross_entropy, loss, accuracy],\n",
    "                                                  feed_dict={batch_ph: x_batch, target_ph: y_batch, \n",
    "                                                             seq_len_ph: seq_len_tr, keep_prob_ph: 1.0})\n",
    "            \n",
    "            y_pred_val, ce_val, loss_val, acc_val = [], 0, 0, 0\n",
    "            num_val_batches = X_val.shape[0] / BATCH_SIZE\n",
    "            for i in range(num_val_batches):\n",
    "                x_batch_val, y_batch_val = X_val[i * BATCH_SIZE : (i + 1) * BATCH_SIZE],\\\n",
    "                                           y_val[i * BATCH_SIZE : (i + 1) * BATCH_SIZE]\n",
    "                seq_len_val = np.array([list(x).index(eos_id) + 1 for x in x_batch_val])\n",
    "                y_pred_val_, ce_val_, loss_val_, acc_val_ = sess.run([y_hat, cross_entropy, loss, accuracy],\n",
    "                                                             feed_dict={batch_ph: x_batch_val, target_ph: y_batch_val,\n",
    "                                                                        seq_len_ph: seq_len_val, keep_prob_ph: 1.0})\n",
    "                y_pred_val += list(y_pred_val_)\n",
    "                ce_val += ce_val_\n",
    "                loss_val += loss_val_\n",
    "                acc_val += acc_val_\n",
    "            \n",
    "            y_pred_val = np.array(y_pred_val)\n",
    "            ce_val /= num_val_batches\n",
    "            loss_val /= num_val_batches\n",
    "            acc_val /= num_val_batches\n",
    "\n",
    "            y_pred_tr = np.array([cls2probs(cls) for cls in np.argmax(y_pred_tr, 1) - 1])\n",
    "            y_pred_val = np.array([cls2probs(cls) for cls in np.argmax(y_pred_val, 1) - 1])\n",
    "            f_macro_tr, f_micro_tr = f_macro(y_batch, y_pred_tr), f_micro(y_batch, y_pred_tr)\n",
    "            f_macro_val, f_micro_val = f_macro(y_val[:num_val_batches * BATCH_SIZE], y_pred_val),\\\n",
    "                                       f_micro(y_val[:num_val_batches * BATCH_SIZE], y_pred_val)\n",
    "        \n",
    "\n",
    "            loss_tr_l.append(loss_tr)\n",
    "            loss_val_l.append(loss_val)\n",
    "            ce_tr_l.append(ce_tr)\n",
    "            ce_val_l.append(ce_val)\n",
    "            acc_tr_l.append(acc_tr)\n",
    "            acc_val_l.append(acc_val)\n",
    "            f_macro_tr_l.append(f_macro_tr)\n",
    "            f_macro_val_l.append(f_macro_val)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            print (\"epoch: {}\".format(epoch))\n",
    "            print (\"\\t Train loss: {:.3f}\\t ce: {:.3f}\\t acc: {:.3f}\\t f_macro: {:.3f}\".format(\n",
    "                loss_tr, ce_tr, acc_tr, f_macro_tr))\n",
    "            print (\"\\t Valid loss: {:.3f}\\t ce: {:.3f}\\t acc: {:.3f}\\t f_macro: {:.3f}\".format(\n",
    "                loss_val, ce_val, acc_val, f_macro_val))\n",
    "\n",
    "            \"\"\"plt.figure(figsize=(15,10))\n",
    "            plt.plot(ce_tr_l, color='blue', label='ce_tr')\n",
    "            plt.plot(ce_val_l, color='red', label='ce_val')        \n",
    "            plt.plot(f_macro_val_l, color='green', label='f_macro_val')\n",
    "            plt.xlim(0, EPOCHS - 1)\n",
    "            plt.ylim(0, 1)\n",
    "            plt.legend()\n",
    "            plt.show()\"\"\"\n",
    "            \n",
    "    results.append([max(acc_val_l), max(f_macro_val_l)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_steps = 10000\n",
    "batch_size = 50\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 39 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 39 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 2 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
    "    'out': tf.Variable(tf.random_normal([2*num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BiRNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, num_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, num_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    try:\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                              dtype=tf.float32)\n",
    "    except Exception: # Old TensorFlow version only returns outputs not states\n",
    "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                        dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'bidirectional_rnn/fw/basic_lstm_cell'; and the cell was not constructed as BasicLSTMCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-21b310ac2e8f>\u001b[0m in \u001b[0;36mBiRNN\u001b[0;34m(x, weights, biases)\u001b[0m\n\u001b[1;32m     18\u001b[0m         outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n\u001b[0;32m---> 19\u001b[0;31m                                               dtype=tf.float32)\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Old TensorFlow version only returns outputs not states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\u001b[0m in \u001b[0;36mstatic_bidirectional_rnn\u001b[0;34m(cell_fw, cell_bw, inputs, initial_state_fw, initial_state_bw, dtype, sequence_length, scope)\u001b[0m\n\u001b[1;32m    336\u001b[0m           \u001b[0mcell_fw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state_fw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m           sequence_length, scope=fw_scope)\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\u001b[0m in \u001b[0;36mstatic_rnn\u001b[0;34m(cell, inputs, initial_state, dtype, sequence_length, scope)\u001b[0m\n\u001b[1;32m    196\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# pylint: disable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m       \u001b[0;31m# pylint: enable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;34m\"\"\"Long short-term memory cell (LSTM).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_checked_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"basic_lstm_cell\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m       \u001b[0;31m# Parameters of gates are concatenated into one multiply for efficiency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m_checked_scope\u001b[0;34m(cell, scope, reuse, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;34m\"reuse it in your second calculation, or create a new one with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \"the argument reuse=True.\" % (scope_name, type(cell).__name__))\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'bidirectional_rnn/fw/basic_lstm_cell'; and the cell was not constructed as BasicLSTMCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-f44aa07a31c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define loss and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
      "\u001b[0;32m<ipython-input-107-21b310ac2e8f>\u001b[0m in \u001b[0;36mBiRNN\u001b[0;34m(x, weights, biases)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Old TensorFlow version only returns outputs not states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n\u001b[0;32m---> 22\u001b[0;31m                                         dtype=tf.float32)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Linear activation, using rnn inner loop last output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\u001b[0m in \u001b[0;36mstatic_bidirectional_rnn\u001b[0;34m(cell_fw, cell_bw, inputs, initial_state_fw, initial_state_bw, dtype, sequence_length, scope)\u001b[0m\n\u001b[1;32m    335\u001b[0m       output_fw, output_state_fw = static_rnn(\n\u001b[1;32m    336\u001b[0m           \u001b[0mcell_fw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state_fw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m           sequence_length, scope=fw_scope)\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;31m# Backward direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\u001b[0m in \u001b[0;36mstatic_rnn\u001b[0;34m(cell, inputs, initial_state, dtype, sequence_length, scope)\u001b[0m\n\u001b[1;32m    195\u001b[0m             state_size=cell.state_size)\n\u001b[1;32m    196\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvarscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# pylint: disable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m       \u001b[0;31m# pylint: enable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;34m\"\"\"Long short-term memory cell (LSTM).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_checked_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"basic_lstm_cell\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m       \u001b[0;31m# Parameters of gates are concatenated into one multiply for efficiency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_is_tuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m_checked_scope\u001b[0;34m(cell, scope, reuse, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;34m\"To share the weights of an RNNCell, simply \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;34m\"reuse it in your second calculation, or create a new one with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \"the argument reuse=True.\" % (scope_name, type(cell).__name__))\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# Everything is OK.  Update the cell's scope and yield it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'bidirectional_rnn/fw/basic_lstm_cell'; and the cell was not constructed as BasicLSTMCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True."
     ]
    }
   ],
   "source": [
    "logits = BiRNN(X, weights, biases)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_7' with dtype float\n\t [[Node: Placeholder_7 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Placeholder_7', defined at:\n  File \"/Applications/anaconda/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Applications/anaconda/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Applications/anaconda/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Applications/anaconda/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Applications/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Applications/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-85-57127e53078f>\", line 13, in <module>\n    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1507, in placeholder\n    name=name)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1997, in _placeholder\n    name=name)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_7' with dtype float\n\t [[Node: Placeholder_7 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_7' with dtype float\n\t [[Node: Placeholder_7 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-a9987985d5c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#batch_x = batch_x.reshape((batch_size, timesteps, num_input))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Run optimization op (backprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# Calculate batch loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_7' with dtype float\n\t [[Node: Placeholder_7 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Placeholder_7', defined at:\n  File \"/Applications/anaconda/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Applications/anaconda/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Applications/anaconda/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Applications/anaconda/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Applications/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Applications/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-85-57127e53078f>\", line 13, in <module>\n    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1507, in placeholder\n    name=name)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1997, in _placeholder\n    name=name)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_7' with dtype float\n\t [[Node: Placeholder_7 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    train_batch_generator = batch_generator(X_tr, y_tr, BATCH_SIZE)\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        \n",
    "        batch_x, batch_y = next(train_batch_generator)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = len(x_dev)\n",
    "    test_data = x_dev.reshape((-1, timesteps, num_input))\n",
    "    test_label = y_dev\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6292"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Stemmer\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "#stemmer.stemWord('')\n",
    "\n",
    "lists=[]\n",
    "for i in all_examples:\n",
    "    lists.append([stemmer.stemWord(word) for word in i.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train the model Word2Vec\n",
    "num_features = 300                  \n",
    "min_word_count = 40                          \n",
    "num_workers = 4       \n",
    "context = 10                                                                                             \n",
    "downsampling = 1e-3   \n",
    "\n",
    "\n",
    "model = word2vec.Word2Vec(lists, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 0.9579096436500549),\n",
       " ('', 0.9547955989837646),\n",
       " ('', 0.9546328186988831),\n",
       " ('', 0.952491044998169),\n",
       " ('', 0.9520655274391174),\n",
       " ('', 0.9492961168289185),\n",
       " ('', 0.9450693130493164),\n",
       " ('', 0.9447919130325317),\n",
       " ('', 0.9425774812698364),\n",
       " ('', 0.9424964189529419)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeVec(words, model, num_features):   \n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#   \n",
    "\"\"\"\"class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    " \n",
    "sentences = MySentences('123') # a memory-friendly iterator\n",
    "model2 = word2vec.Word2Vec(sentences)\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\"from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('web_russe.model.bin', binary=True) \n",
    "word_vectors.vocab\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_text=[makeVec(i,model, num_features) for i in all_examples]\n",
    "x_text=np.array(x_text)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
