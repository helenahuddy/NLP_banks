{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 21 µs\n"
     ]
    }
   ],
   "source": [
    "%%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/Lasagne/Lasagne/archive/master.zip\n",
      "  Downloading https://github.com/Lasagne/Lasagne/archive/master.zip\n",
      "\u001b[K     | 235kB 769kB/s\n",
      "Requirement already up-to-date: numpy in /Applications/anaconda/lib/python3.5/site-packages (from Lasagne==0.2.dev1)\n",
      "Installing collected packages: Lasagne\n",
      "  Found existing installation: Lasagne 0.1\n",
      "    Uninstalling Lasagne-0.1:\n",
      "      Successfully uninstalled Lasagne-0.1\n",
      "  Running setup.py install for Lasagne ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25hSuccessfully installed Lasagne-0.2.dev1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# theano teaser\n",
    "\n",
    "Doing the very same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import data_helpers\n",
    "\n",
    "tweets='TREC_all.txt'\n",
    "labels='label_all.txt'\n",
    "\n",
    "X,y=data_helpers.load_tweets(tweets),data_helpers.load_2dsentiment(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_check=data_helpers.load_1dsentiment(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2421 6971\n"
     ]
    }
   ],
   "source": [
    "a=0\n",
    "b=0\n",
    "for i in y_check:\n",
    "    if i:\n",
    "        a+=1\n",
    "    else:\n",
    "        b+=1\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.contrib import learn\n",
    "import lasagne\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_document_length = max([len(x.split(\" \")) for x in X])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x_bag = np.array(list(vocab_processor.fit_transform(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13028"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class frequencies:\t [1639, 4653]\n",
      "Validation class frequencies:\t [782, 2318]\n",
      "Constant classifier's validation accuracy:\t 0.747741935483871\n"
     ]
    }
   ],
   "source": [
    "#Check if balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_bag, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print (\"Train class frequencies:\\t\", [col.nonzero()[0].shape[0] for col in y_train.transpose()])\n",
    "print (\"Validation class frequencies:\\t\", [col.nonzero()[0].shape[0] for col in y_test.transpose()])\n",
    "print (\"Constant classifier's validation accuracy:\\t\", [col.nonzero()[0].shape[0] for col in y_test.transpose()][1] * 1. / y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val = X_train[:-5500], X_train[-5500:]\n",
    "y_train, y_val = y_train[:-5500], y_train[-5500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's all, now let's train it!\n",
    "* We got a lot of data, so it's recommended that you use SGD\n",
    "* So let's implement a function that splits the training sample into minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An auxilary function that returns mini-batches for neural network training\n",
    "\n",
    "#Parameters\n",
    "# X - a tensor of images with shape (many, 1, 28, 28), e.g. X_train\n",
    "# y - a vector of answers for corresponding images e.g. Y_train\n",
    "#batch_size - a single number - the intended size of each batches\n",
    "\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A better network \n",
    " \n",
    "\n",
    "## Tips on what can be done:\n",
    "\n",
    "\n",
    "\n",
    " * Network size\n",
    "   * MOAR neurons, \n",
    "   * MOAR layers, \n",
    "   * Convolutions are almost imperative\n",
    "   * Пх'нглуи мглв'нафх Ктулху Р'льех вгах'нагл фхтагн! \n",
    "   \n",
    "   \n",
    "   \n",
    " * Regularize to prevent overfitting\n",
    "   * Add some L2 weight norm to the loss function, theano will do the rest\n",
    "   * Can be done manually or via - http://lasagne.readthedocs.org/en/latest/modules/regularization.html\n",
    "   \n",
    "   \n",
    "   \n",
    " * Better optimization - rmsprop, nesterov_momentum, adadelta, adagrad and so on.\n",
    "   * Converge faster and sometimes reach better optima\n",
    "   * It might make sense to tweak learning rate, other learning parameters, batch size and number of epochs\n",
    "   \n",
    "   \n",
    "   \n",
    " * Dropout - to prevent overfitting\n",
    "   * `lasagne.layers.DropoutLayer(prev_layer, p=probability_to_zero_out)`\n",
    "   \n",
    "   \n",
    "   \n",
    " * Convolution layers\n",
    "   * `network = lasagne.layers.Conv2DLayer(prev_layer,`\n",
    "    `                       num_filters = n_neurons,`\n",
    "    `                        filter_size = (filter width, filter height),`\n",
    "    `                        nonlinearity = some_nonlinearity)`\n",
    "   * Warning! Training convolutional networks can take long without GPU.\n",
    "     * If you are CPU-only, we still recomment to try a simple convolutional architecture\n",
    "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
    " \n",
    " * Plenty other layers and architectures\n",
    "   * http://lasagne.readthedocs.org/en/latest/modules/layers.html\n",
    "   * batch normalization, pooling, etc\n",
    "   \n",
    "   \n",
    " * Nonlinearities in the hidden layers\n",
    "   * tanh, relu, leaky relu, etc\n",
    "   \n",
    " \n",
    " \n",
    "   \n",
    "There is a template for your solution below that you can opt to use or throw away and write it your way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_X = T.lmatrix(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = (None, 39)\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int64')\n",
    "\n",
    "\n",
    "\n",
    "#Input layer (auxilary)\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "emb_layer=lasagne.layers.EmbeddingLayer(input_layer,len(vocab_processor.vocabulary_),50)\n",
    "\n",
    "dim_layer=lasagne.layers.DimshuffleLayer(emb_layer, [0,2,1])\n",
    "\n",
    "#lstm=lasagne.layers.LSTMLayer(dense_1,num_units=4)\n",
    "\n",
    "\n",
    "conv_1=lasagne.layers.Conv1DLayer(dim_layer, num_filters=3, filter_size=5)#,nonlinearity=lasagne.nonlinearities.softmax)\n",
    "pool_1=lasagne.layers.MaxPool1DLayer(conv_1, pool_size=2)\n",
    "\n",
    "\n",
    "\n",
    "#drop_1=lasagne.layers.DropoutLayer(input_layer,p=0.5)\n",
    "dense_1 = lasagne.layers.DenseLayer(pool_1,num_units=50)#,\n",
    "                                  #nonlinearity = lasagne.nonlinearities.softmax,\n",
    "#                                 name = \"selu\")\n",
    "\n",
    "#conv_2=lasagne.layers.Conv1DLayer(input_layer, num_filters=3, filter_size=5)#,nonlinearity=lasagne.nonlinearities.softmax)\n",
    "#pool_2=lasagne.layers.Pool2DLayer(conv_2, pool_size=2)\n",
    "\n",
    "\n",
    "#drop_1=lasagne.layers.DropoutLayer(dense_1,p=0.5)\n",
    "\n",
    "#dense_2 = lasagne.layers.DenseLayer(dense_1,num_units=100,\n",
    "#                                   nonlinearity = lasagne.nonlinearities.selu,\n",
    "   #                               name = \"selu\")\n",
    "\n",
    "#dense_3 = lasagne.layers.DenseLayer(dense_2,num_units=100,\n",
    "#                                   nonlinearity = lasagne.nonlinearities.selu,\n",
    "#                                  name = \"selu\")\n",
    "\n",
    "\n",
    "#fully connected output layer that takes dense_1 as input and has 10 neurons (1 for each digit)\n",
    "#We use softmax nonlinearity to make probabilities add up to 1\n",
    "dense_output = lasagne.layers.DenseLayer(dense_1,num_units = 2,\n",
    "                                        nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='output')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, W, b, W, b, output.W, output.b]\n"
     ]
    }
   ],
   "source": [
    "# Network predictions (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(dense_output,input_X)\n",
    "\n",
    "\n",
    "#All weights (shared-varaibles)\n",
    "# \"trainable\" flag means not to return auxilary params like batch mean (for batch normalization)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output,trainable=True)\n",
    "print (all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "\n",
    "\n",
    "\n",
    "from lasagne.regularization import regularize_layer_params_weighted, l2, l1\n",
    "from lasagne.regularization import regularize_layer_params\n",
    "layers = {conv_1: 0.1, dense_1: 0.5}\n",
    "l2_penalty = regularize_layer_params_weighted(layers, l2)\n",
    "l1_penalty = regularize_layer_params(dense_1, l1) * 1e-4\n",
    "#loss=loss+l2_penalty#+l1_penalty\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.adagrad(loss, all_weights,learning_rate=0.01)\n",
    "\n",
    "\n",
    "#A function that accepts X and y, returns loss functions and performs weight updates\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#A function that just computes accuracy given X and y\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 0.725s\n",
      "  training loss (in-iteration):\t\t0.575040\n",
      "  train accuracy:\t\t70.67 %\n",
      "  validation accuracy:\t\t75.95 %\n",
      "Epoch 2 of 100 took 0.502s\n",
      "  training loss (in-iteration):\t\t0.332779\n",
      "  train accuracy:\t\t87.07 %\n",
      "  validation accuracy:\t\t83.62 %\n",
      "Epoch 3 of 100 took 0.647s\n",
      "  training loss (in-iteration):\t\t0.166882\n",
      "  train accuracy:\t\t95.07 %\n",
      "  validation accuracy:\t\t84.42 %\n",
      "Epoch 4 of 100 took 0.464s\n",
      "  training loss (in-iteration):\t\t0.080947\n",
      "  train accuracy:\t\t98.13 %\n",
      "  validation accuracy:\t\t84.33 %\n",
      "Epoch 5 of 100 took 0.544s\n",
      "  training loss (in-iteration):\t\t0.042614\n",
      "  train accuracy:\t\t99.20 %\n",
      "  validation accuracy:\t\t84.18 %\n",
      "Epoch 6 of 100 took 0.462s\n",
      "  training loss (in-iteration):\t\t0.025954\n",
      "  train accuracy:\t\t99.47 %\n",
      "  validation accuracy:\t\t84.20 %\n",
      "Epoch 7 of 100 took 0.475s\n",
      "  training loss (in-iteration):\t\t0.017441\n",
      "  train accuracy:\t\t99.60 %\n",
      "  validation accuracy:\t\t84.07 %\n",
      "Epoch 8 of 100 took 0.466s\n",
      "  training loss (in-iteration):\t\t0.012304\n",
      "  train accuracy:\t\t99.73 %\n",
      "  validation accuracy:\t\t83.91 %\n",
      "Epoch 9 of 100 took 0.466s\n",
      "  training loss (in-iteration):\t\t0.008989\n",
      "  train accuracy:\t\t99.73 %\n",
      "  validation accuracy:\t\t83.87 %\n",
      "Epoch 10 of 100 took 0.451s\n",
      "  training loss (in-iteration):\t\t0.006810\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 11 of 100 took 0.453s\n",
      "  training loss (in-iteration):\t\t0.005293\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 12 of 100 took 0.451s\n",
      "  training loss (in-iteration):\t\t0.004283\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.73 %\n",
      "Epoch 13 of 100 took 0.452s\n",
      "  training loss (in-iteration):\t\t0.003558\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.65 %\n",
      "Epoch 14 of 100 took 0.453s\n",
      "  training loss (in-iteration):\t\t0.003022\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.62 %\n",
      "Epoch 15 of 100 took 0.466s\n",
      "  training loss (in-iteration):\t\t0.002604\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.65 %\n",
      "Epoch 16 of 100 took 0.452s\n",
      "  training loss (in-iteration):\t\t0.002277\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.71 %\n",
      "Epoch 17 of 100 took 0.452s\n",
      "  training loss (in-iteration):\t\t0.002016\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.80 %\n",
      "Epoch 18 of 100 took 0.450s\n",
      "  training loss (in-iteration):\t\t0.001800\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.76 %\n",
      "Epoch 19 of 100 took 0.448s\n",
      "  training loss (in-iteration):\t\t0.001620\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.76 %\n",
      "Epoch 20 of 100 took 0.451s\n",
      "  training loss (in-iteration):\t\t0.001469\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 21 of 100 took 0.452s\n",
      "  training loss (in-iteration):\t\t0.001342\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 22 of 100 took 0.449s\n",
      "  training loss (in-iteration):\t\t0.001233\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.75 %\n",
      "Epoch 23 of 100 took 0.450s\n",
      "  training loss (in-iteration):\t\t0.001137\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.80 %\n",
      "Epoch 24 of 100 took 0.451s\n",
      "  training loss (in-iteration):\t\t0.001054\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 25 of 100 took 0.450s\n",
      "  training loss (in-iteration):\t\t0.000981\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 26 of 100 took 0.446s\n",
      "  training loss (in-iteration):\t\t0.000917\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 27 of 100 took 0.445s\n",
      "  training loss (in-iteration):\t\t0.000859\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 28 of 100 took 0.449s\n",
      "  training loss (in-iteration):\t\t0.000807\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 29 of 100 took 0.451s\n",
      "  training loss (in-iteration):\t\t0.000761\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 30 of 100 took 0.446s\n",
      "  training loss (in-iteration):\t\t0.000718\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 31 of 100 took 0.445s\n",
      "  training loss (in-iteration):\t\t0.000680\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 32 of 100 took 0.447s\n",
      "  training loss (in-iteration):\t\t0.000645\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 33 of 100 took 0.441s\n",
      "  training loss (in-iteration):\t\t0.000613\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 34 of 100 took 0.439s\n",
      "  training loss (in-iteration):\t\t0.000584\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.80 %\n",
      "Epoch 35 of 100 took 0.447s\n",
      "  training loss (in-iteration):\t\t0.000557\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 36 of 100 took 0.440s\n",
      "  training loss (in-iteration):\t\t0.000532\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 37 of 100 took 0.441s\n",
      "  training loss (in-iteration):\t\t0.000509\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 38 of 100 took 0.443s\n",
      "  training loss (in-iteration):\t\t0.000488\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 39 of 100 took 0.439s\n",
      "  training loss (in-iteration):\t\t0.000468\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 40 of 100 took 0.440s\n",
      "  training loss (in-iteration):\t\t0.000450\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 41 of 100 took 0.442s\n",
      "  training loss (in-iteration):\t\t0.000433\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 42 of 100 took 0.440s\n",
      "  training loss (in-iteration):\t\t0.000417\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.80 %\n",
      "Epoch 43 of 100 took 0.444s\n",
      "  training loss (in-iteration):\t\t0.000402\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.80 %\n",
      "Epoch 44 of 100 took 0.446s\n",
      "  training loss (in-iteration):\t\t0.000387\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.80 %\n",
      "Epoch 45 of 100 took 0.454s\n",
      "  training loss (in-iteration):\t\t0.000374\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.80 %\n",
      "Epoch 46 of 100 took 0.449s\n",
      "  training loss (in-iteration):\t\t0.000362\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 47 of 100 took 0.450s\n",
      "  training loss (in-iteration):\t\t0.000350\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 48 of 100 took 0.452s\n",
      "  training loss (in-iteration):\t\t0.000339\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 49 of 100 took 0.448s\n",
      "  training loss (in-iteration):\t\t0.000328\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 50 of 100 took 0.451s\n",
      "  training loss (in-iteration):\t\t0.000318\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 51 of 100 took 0.454s\n",
      "  training loss (in-iteration):\t\t0.000309\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 52 of 100 took 0.450s\n",
      "  training loss (in-iteration):\t\t0.000300\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 53 of 100 took 0.451s\n",
      "  training loss (in-iteration):\t\t0.000291\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 54 of 100 took 0.453s\n",
      "  training loss (in-iteration):\t\t0.000283\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 55 of 100 took 0.460s\n",
      "  training loss (in-iteration):\t\t0.000275\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 56 of 100 took 0.450s\n",
      "  training loss (in-iteration):\t\t0.000268\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 57 of 100 took 0.442s\n",
      "  training loss (in-iteration):\t\t0.000261\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 58 of 100 took 0.449s\n",
      "  training loss (in-iteration):\t\t0.000254\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 59 of 100 took 0.447s\n",
      "  training loss (in-iteration):\t\t0.000248\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.85 %\n",
      "Epoch 60 of 100 took 0.444s\n",
      "  training loss (in-iteration):\t\t0.000242\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 61 of 100 took 0.442s\n",
      "  training loss (in-iteration):\t\t0.000236\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 62 of 100 took 0.452s\n",
      "  training loss (in-iteration):\t\t0.000230\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 63 of 100 took 0.442s\n",
      "  training loss (in-iteration):\t\t0.000225\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 64 of 100 took 0.450s\n",
      "  training loss (in-iteration):\t\t0.000220\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 65 of 100 took 0.441s\n",
      "  training loss (in-iteration):\t\t0.000215\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 66 of 100 took 0.444s\n",
      "  training loss (in-iteration):\t\t0.000210\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 67 of 100 took 0.448s\n",
      "  training loss (in-iteration):\t\t0.000205\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 68 of 100 took 0.440s\n",
      "  training loss (in-iteration):\t\t0.000201\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 69 of 100 took 0.439s\n",
      "  training loss (in-iteration):\t\t0.000197\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 70 of 100 took 0.444s\n",
      "  training loss (in-iteration):\t\t0.000192\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 71 of 100 took 0.444s\n",
      "  training loss (in-iteration):\t\t0.000189\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 72 of 100 took 0.444s\n",
      "  training loss (in-iteration):\t\t0.000185\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 73 of 100 took 0.446s\n",
      "  training loss (in-iteration):\t\t0.000181\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 74 of 100 took 0.443s\n",
      "  training loss (in-iteration):\t\t0.000178\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.84 %\n",
      "Epoch 75 of 100 took 0.447s\n",
      "  training loss (in-iteration):\t\t0.000174\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.82 %\n",
      "Epoch 76 of 100 took 0.438s\n",
      "  training loss (in-iteration):\t\t0.000171\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.80 %\n",
      "Epoch 77 of 100 took 0.438s\n",
      "  training loss (in-iteration):\t\t0.000168\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 78 of 100 took 0.443s\n",
      "  training loss (in-iteration):\t\t0.000165\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 79 of 100 took 0.445s\n",
      "  training loss (in-iteration):\t\t0.000162\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 80 of 100 took 0.447s\n",
      "  training loss (in-iteration):\t\t0.000159\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 81 of 100 took 0.444s\n",
      "  training loss (in-iteration):\t\t0.000156\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 82 of 100 took 0.444s\n",
      "  training loss (in-iteration):\t\t0.000153\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 83 of 100 took 0.454s\n",
      "  training loss (in-iteration):\t\t0.000150\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 84 of 100 took 0.449s\n",
      "  training loss (in-iteration):\t\t0.000148\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 85 of 100 took 0.451s\n",
      "  training loss (in-iteration):\t\t0.000145\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 86 of 100 took 0.447s\n",
      "  training loss (in-iteration):\t\t0.000143\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 87 of 100 took 0.455s\n",
      "  training loss (in-iteration):\t\t0.000141\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 88 of 100 took 0.450s\n",
      "  training loss (in-iteration):\t\t0.000138\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 89 of 100 took 0.446s\n",
      "  training loss (in-iteration):\t\t0.000136\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.76 %\n",
      "Epoch 90 of 100 took 0.451s\n",
      "  training loss (in-iteration):\t\t0.000134\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.76 %\n",
      "Epoch 91 of 100 took 0.447s\n",
      "  training loss (in-iteration):\t\t0.000132\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.76 %\n",
      "Epoch 92 of 100 took 0.446s\n",
      "  training loss (in-iteration):\t\t0.000130\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 93 of 100 took 0.444s\n",
      "  training loss (in-iteration):\t\t0.000128\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 94 of 100 took 0.440s\n",
      "  training loss (in-iteration):\t\t0.000126\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 95 of 100 took 0.441s\n",
      "  training loss (in-iteration):\t\t0.000124\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.76 %\n",
      "Epoch 96 of 100 took 0.440s\n",
      "  training loss (in-iteration):\t\t0.000122\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.76 %\n",
      "Epoch 97 of 100 took 0.439s\n",
      "  training loss (in-iteration):\t\t0.000120\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.76 %\n",
      "Epoch 98 of 100 took 0.443s\n",
      "  training loss (in-iteration):\t\t0.000119\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 99 of 100 took 0.440s\n",
      "  training loss (in-iteration):\t\t0.000117\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n",
      "Epoch 100 of 100 took 0.444s\n",
      "  training loss (in-iteration):\t\t0.000115\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t83.78 %\n"
     ]
    }
   ],
   "source": [
    "#итерации обучения\n",
    "import time\n",
    "num_epochs = 100#<how many times to iterate over the entire training set>\n",
    "\n",
    "batch_size = 50#<how many samples are processed at a single function call>\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t86.00 %\n",
      "We need more magic!\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "f1score=0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print (\"Achievement unlocked: 80lvl Warlock!\")\n",
    "else:\n",
    "    print (\"We need more magic!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, W, b, W, b, output.W, output.b]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Bad input argument to theano function with name \"<ipython-input-80-f35522de8c28>:89\" at index 0 (0-based).  \nBacktrace when that variable is created:\n\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Applications/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-80-f35522de8c28>\", line 1, in <module>\n    input_X = T.tensor3(\"X\")\nWrong number of dimensions: expected 3, got 2 with shape (50, 39).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-cfdad0f63015>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_err_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mf_tr_micro\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mf_tr_macro\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtrain_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    793\u001b[0m                         s.storage[0] = s.type.filter(\n\u001b[1;32m    794\u001b[0m                             \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                             allow_downcast=s.allow_downcast)\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/theano/tensor/type.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, data, strict, allow_downcast)\u001b[0m\n\u001b[1;32m    176\u001b[0m             raise TypeError(\"Wrong number of dimensions: expected %s,\"\n\u001b[1;32m    177\u001b[0m                             \" got %s with shape %s.\" % (self.ndim, data.ndim,\n\u001b[0;32m--> 178\u001b[0;31m                                                         data.shape))\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maligned\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Bad input argument to theano function with name \"<ipython-input-80-f35522de8c28>:89\" at index 0 (0-based).  \nBacktrace when that variable is created:\n\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Applications/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Applications/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-80-f35522de8c28>\", line 1, in <module>\n    input_X = T.tensor3(\"X\")\nWrong number of dimensions: expected 3, got 2 with shape (50, 39)."
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "\n",
    "input_X = T.lmatrix(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = (None, 39)\n",
    "\n",
    "target_y = T.matrix(\"target Y integer\",dtype='int64')\n",
    "\n",
    "#Input layer (auxilary)\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "emb_layer=lasagne.layers.EmbeddingLayer(input_layer,len(vocab_processor.vocabulary_),50)\n",
    "\n",
    "dim_layer=lasagne.layers.DimshuffleLayer(emb_layer, [0,2,1])\n",
    "\n",
    "conv_1=lasagne.layers.Conv1DLayer(dim_layer, num_filters=3, filter_size=5)#,nonlinearity=lasagne.nonlinearities.softmax)\n",
    "pool_1=lasagne.layers.MaxPool1DLayer(conv_1, pool_size=2)\n",
    "\n",
    "\n",
    "\n",
    "#drop_1=lasagne.layers.DropoutLayer(input_layer,p=0.5)\n",
    "dense_1 = lasagne.layers.DenseLayer(pool_1,num_units=50)#,\n",
    "                                  #nonlinearity = lasagne.nonlinearities.softmax,\n",
    "#                                 name = \"selu\")\n",
    "\n",
    "\n",
    "dense_output = lasagne.layers.DenseLayer(dense_1,num_units = 2,\n",
    "                                        nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='output')\n",
    "\n",
    "y_predicted = lasagne.layers.get_output(dense_output,input_X)\n",
    "\n",
    "\n",
    "#All weights (shared-varaibles)\n",
    "# \"trainable\" flag means not to return auxilary params like batch mean (for batch normalization)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output,trainable=True)\n",
    "print (all_weights)\n",
    "\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "\n",
    "\n",
    "\n",
    "from lasagne.regularization import regularize_layer_params_weighted, l2, l1\n",
    "from lasagne.regularization import regularize_layer_params\n",
    "layers = {conv_1: 0.1, dense_1: 0.5}\n",
    "l2_penalty = regularize_layer_params_weighted(layers, l2)\n",
    "l1_penalty = regularize_layer_params(dense_1, l1) * 1e-4\n",
    "#loss=loss+l2_penalty#+l1_penalty\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.adagrad(loss, all_weights,learning_rate=0.01)\n",
    "\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#A function that just computes accuracy given X and y\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)\n",
    "\n",
    "#итерации обучения\n",
    "import time\n",
    "num_epochs = 20#<how many times to iterate over the entire training set>\n",
    "\n",
    "batch_size = 50#<how many samples are processed at a single function call>\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    f_tr_macro=0\n",
    "    f_tr_micro=0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        f_tr_micro+=f1_score(targets, predict_fun(inputs), average='micro')\n",
    "        f_tr_macro+=f1_score(targets, predict_fun(inputs), average='macro')\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    val_fscore_micro=0\n",
    "    val_fscore_macro=0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_fscore_micro+=f1_score(targets, predict_fun(inputs), average='micro')\n",
    "        val_fscore_macro+=f1_score(targets, predict_fun(inputs), average='macro')\n",
    "        \n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    print(\"  train f1-score-macro:\\t\\t{:.2f} %\".format(\n",
    "        f_tr_macro / train_batches * 100))\n",
    "    print(\"  train f1-score-micro:\\t\\t{:.2f} %\".format(\n",
    "        f_tr_micro / train_batches * 100))\n",
    "    print(\"  val f1-score-macro:\\t\\t{:.2f} %\".format(\n",
    "        val_fscore_macro / val_batches * 100))\n",
    "    print(\"  val f1-score-micro:\\t\\t{:.2f} %\".format(\n",
    "        val_fscore_micro / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "test_fscore_micro=0\n",
    "test_fscore_macro=0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "    test_fscore_micro+=f1_score(targets, predict_fun(inputs), average='micro')\n",
    "    test_fscore_macro+=f1_score(targets, predict_fun(inputs), average='macro')\n",
    "        \n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "print(\"  test f1-score-macro:\\t\\t{:.2f} %\".format(\n",
    "        test_fscore_macro / test_batches * 100))\n",
    "print(\"  test f1-score-micro:\\t\\t{:.2f} %\".format(\n",
    "        test_fscore_micro / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print (\"Achievement unlocked: 80lvl Warlock!\")\n",
    "else:\n",
    "    print (\"We need more magic!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, W, b, W, b, output.W, output.b]\n",
      "Epoch 1 of 10 took 0.337s\n",
      "  training loss (in-iteration):\t\t0.533203\n",
      "  train accuracy:\t\t71.87 %\n",
      "  validation accuracy:\t\t83.13 %\n",
      "Epoch 2 of 10 took 0.378s\n",
      "  training loss (in-iteration):\t\t0.250634\n",
      "  train accuracy:\t\t90.80 %\n",
      "  validation accuracy:\t\t85.35 %\n",
      "Epoch 3 of 10 took 0.418s\n",
      "  training loss (in-iteration):\t\t0.103816\n",
      "  train accuracy:\t\t97.60 %\n",
      "  validation accuracy:\t\t85.45 %\n",
      "Epoch 4 of 10 took 0.354s\n",
      "  training loss (in-iteration):\t\t0.043320\n",
      "  train accuracy:\t\t99.20 %\n",
      "  validation accuracy:\t\t85.53 %\n",
      "Epoch 5 of 10 took 0.351s\n",
      "  training loss (in-iteration):\t\t0.022692\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t85.38 %\n",
      "Epoch 6 of 10 took 0.345s\n",
      "  training loss (in-iteration):\t\t0.016745\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t85.18 %\n",
      "Epoch 7 of 10 took 0.351s\n",
      "  training loss (in-iteration):\t\t0.015013\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t85.25 %\n",
      "Epoch 8 of 10 took 0.347s\n",
      "  training loss (in-iteration):\t\t0.014116\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t85.25 %\n",
      "Epoch 9 of 10 took 0.349s\n",
      "  training loss (in-iteration):\t\t0.013534\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t85.18 %\n",
      "Epoch 10 of 10 took 0.352s\n",
      "  training loss (in-iteration):\t\t0.013081\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t85.27 %\n"
     ]
    }
   ],
   "source": [
    "input_X = T.lmatrix(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = (None, 39)\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int64')\n",
    "\n",
    "\n",
    "#Input layer (auxilary)\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "emb_layer=lasagne.layers.EmbeddingLayer(input_layer,len(vocab_processor.vocabulary_),50)\n",
    "\n",
    "dim_layer=lasagne.layers.DimshuffleLayer(emb_layer, [0,2,1])\n",
    "\n",
    "#lstm=lasagne.layers.LSTMLayer(dense_1,num_units=4)\n",
    "\n",
    "\n",
    "conv_1=lasagne.layers.Conv1DLayer(dim_layer, num_filters=3, filter_size=5)#,nonlinearity=lasagne.nonlinearities.softmax)\n",
    "pool_1=lasagne.layers.MaxPool1DLayer(conv_1, pool_size=2)\n",
    "\n",
    "\n",
    "\n",
    "#drop_1=lasagne.layers.DropoutLayer(pool_1,p=0.5)\n",
    "dense_1 = lasagne.layers.DenseLayer(pool_1,num_units=50)#,\n",
    "                                  #nonlinearity = lasagne.nonlinearities.softmax,\n",
    "#                                 name = \"selu\")\n",
    "\n",
    "dense_output = lasagne.layers.DenseLayer(dense_1,num_units = 2,\n",
    "                                        nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='output')\n",
    "\n",
    "\n",
    "# Network predictions (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(dense_output,input_X)\n",
    "\n",
    "\n",
    "#All weights (shared-varaibles)\n",
    "# \"trainable\" flag means not to return auxilary params like batch mean (for batch normalization)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output,trainable=True)\n",
    "print (all_weights)\n",
    "\n",
    "\n",
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "\n",
    "\n",
    "\n",
    "from lasagne.regularization import regularize_layer_params_weighted, l2, l1\n",
    "from lasagne.regularization import regularize_layer_params\n",
    "layers = {conv_1: 0.1, dense_1: 0.5}\n",
    "l2_penalty = regularize_layer_params_weighted(layers, l2)\n",
    "l1_penalty = regularize_layer_params(dense_1, l1) * 1e-4\n",
    "loss=loss+l1_penalty#+l1_penalty\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.adamax(loss, all_weights,learning_rate=0.01)\n",
    "\n",
    "\n",
    "\n",
    "#A function that accepts X and y, returns loss functions and performs weight updates\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#A function that just computes accuracy given X and y\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)\n",
    "\n",
    "\n",
    "\n",
    "#итерации обучения\n",
    "import time\n",
    "num_epochs = 10#<how many times to iterate over the entire training set>\n",
    "\n",
    "batch_size = 50#<how many samples are processed at a single function call>\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    f_macro=0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t86.00 %\n",
      "We need more magic!\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "    f_macro=f1_score(batch[1],)\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print (\"Achievement unlocked: 80lvl Warlock!\")\n",
    "else:\n",
    "    print (\"We need more magic!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 3, 17)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_1.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, W, b, W, b, selu.W, selu.b, output.W, output.b]\n",
      "Epoch 1 of 100 took 0.400s\n",
      "  training loss (in-iteration):\t\t0.599808\n",
      "  train accuracy:\t\t72.80 %\n",
      "  validation accuracy:\t\t74.05 %\n",
      "Epoch 2 of 100 took 0.397s\n",
      "  training loss (in-iteration):\t\t0.579688\n",
      "  train accuracy:\t\t72.93 %\n",
      "  validation accuracy:\t\t74.05 %\n",
      "Epoch 3 of 100 took 0.399s\n",
      "  training loss (in-iteration):\t\t0.532689\n",
      "  train accuracy:\t\t72.93 %\n",
      "  validation accuracy:\t\t74.05 %\n",
      "Epoch 4 of 100 took 0.392s\n",
      "  training loss (in-iteration):\t\t0.394204\n",
      "  train accuracy:\t\t75.73 %\n",
      "  validation accuracy:\t\t78.49 %\n",
      "Epoch 5 of 100 took 0.396s\n",
      "  training loss (in-iteration):\t\t0.282482\n",
      "  train accuracy:\t\t90.93 %\n",
      "  validation accuracy:\t\t81.93 %\n",
      "Epoch 6 of 100 took 0.393s\n",
      "  training loss (in-iteration):\t\t0.191221\n",
      "  train accuracy:\t\t95.47 %\n",
      "  validation accuracy:\t\t81.62 %\n",
      "Epoch 7 of 100 took 0.396s\n",
      "  training loss (in-iteration):\t\t0.099631\n",
      "  train accuracy:\t\t97.60 %\n",
      "  validation accuracy:\t\t80.75 %\n",
      "Epoch 8 of 100 took 0.394s\n",
      "  training loss (in-iteration):\t\t0.061448\n",
      "  train accuracy:\t\t98.27 %\n",
      "  validation accuracy:\t\t80.64 %\n",
      "Epoch 9 of 100 took 0.388s\n",
      "  training loss (in-iteration):\t\t0.039439\n",
      "  train accuracy:\t\t98.67 %\n",
      "  validation accuracy:\t\t80.13 %\n",
      "Epoch 10 of 100 took 0.389s\n",
      "  training loss (in-iteration):\t\t0.024691\n",
      "  train accuracy:\t\t99.73 %\n",
      "  validation accuracy:\t\t79.11 %\n",
      "Epoch 11 of 100 took 0.394s\n",
      "  training loss (in-iteration):\t\t0.022709\n",
      "  train accuracy:\t\t99.73 %\n",
      "  validation accuracy:\t\t78.49 %\n",
      "Epoch 12 of 100 took 0.399s\n",
      "  training loss (in-iteration):\t\t0.020918\n",
      "  train accuracy:\t\t99.60 %\n",
      "  validation accuracy:\t\t80.64 %\n",
      "Epoch 13 of 100 took 0.579s\n",
      "  training loss (in-iteration):\t\t0.013938\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t80.73 %\n",
      "Epoch 14 of 100 took 0.456s\n",
      "  training loss (in-iteration):\t\t0.015549\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t80.05 %\n",
      "Epoch 15 of 100 took 0.434s\n",
      "  training loss (in-iteration):\t\t0.011027\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t79.05 %\n",
      "Epoch 16 of 100 took 0.393s\n",
      "  training loss (in-iteration):\t\t0.014920\n",
      "  train accuracy:\t\t99.73 %\n",
      "  validation accuracy:\t\t79.13 %\n",
      "Epoch 17 of 100 took 0.396s\n",
      "  training loss (in-iteration):\t\t0.007612\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t80.98 %\n",
      "Epoch 18 of 100 took 0.393s\n",
      "  training loss (in-iteration):\t\t0.004466\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t80.55 %\n",
      "Epoch 19 of 100 took 0.394s\n",
      "  training loss (in-iteration):\t\t0.003151\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t80.22 %\n",
      "Epoch 20 of 100 took 0.560s\n",
      "  training loss (in-iteration):\t\t0.006989\n",
      "  train accuracy:\t\t99.73 %\n",
      "  validation accuracy:\t\t79.73 %\n",
      "Epoch 21 of 100 took 0.458s\n",
      "  training loss (in-iteration):\t\t0.002178\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t79.95 %\n",
      "Epoch 22 of 100 took 0.504s\n",
      "  training loss (in-iteration):\t\t0.003046\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t79.24 %\n",
      "Epoch 23 of 100 took 0.460s\n",
      "  training loss (in-iteration):\t\t0.002505\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t79.09 %\n",
      "Epoch 24 of 100 took 0.459s\n",
      "  training loss (in-iteration):\t\t0.002154\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t79.60 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-265-b9f63871ee3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mtrain_err_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_err_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_X = T.lmatrix(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = (None, 39)\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int64')\n",
    "\n",
    "\n",
    "#Input layer (auxilary)\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "emb_layer=lasagne.layers.EmbeddingLayer(input_layer,len(vocab_processor.vocabulary_),50)\n",
    "\n",
    "dim_layer=lasagne.layers.DimshuffleLayer(emb_layer, [0,2,1])\n",
    "\n",
    "#lstm=lasagne.layers.LSTMLayer(dense_1,num_units=4)\n",
    "\n",
    "\n",
    "conv_1=lasagne.layers.Conv1DLayer(dim_layer, num_filters=3, filter_size=5)#,nonlinearity=lasagne.nonlinearities.softmax)\n",
    "pool_1=lasagne.layers.MaxPool1DLayer(conv_1, pool_size=2)\n",
    "\n",
    "\"\"\"dim_layer_2=lasagne.layers.DimshuffleLayer(pool_1, [0,2,1])\n",
    "conv_2=lasagne.layers.Conv1DLayer(dim_layer_2, num_filters=3, filter_size=2)#,nonlinearity=lasagne.nonlinearities.softmax)\n",
    "pool_2=lasagne.layers.MaxPool1DLayer(conv_2, pool_size=2)\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#drop_1=lasagne.layers.DropoutLayer(pool_1,p=0.5)\n",
    "dense_1 = lasagne.layers.DenseLayer(pool_1,num_units=50)#,\n",
    "                                  #nonlinearity = lasagne.nonlinearities.softmax,\n",
    "#                                 name = \"selu\")\n",
    "\n",
    "\n",
    "drop_1=lasagne.layers.DropoutLayer(dense_1,p=0.5)\n",
    "\n",
    "dense_2 = lasagne.layers.DenseLayer(drop_1,num_units=100,\n",
    " #                                  nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                  name = \"selu\")\n",
    "\n",
    "#dense_3 = lasagne.layers.DenseLayer(dense_2,num_units=100,\n",
    "#                                   nonlinearity = lasagne.nonlinearities.selu,\n",
    "#                                  name = \"selu\")\n",
    "\n",
    "\n",
    "#fully connected output layer that takes dense_1 as input and has 10 neurons (1 for each digit)\n",
    "#We use softmax nonlinearity to make probabilities add up to 1\n",
    "dense_output = lasagne.layers.DenseLayer(dense_2,num_units = 2,\n",
    "                                        nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='output')\n",
    "\n",
    "\n",
    "# Network predictions (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(dense_output,input_X)\n",
    "\n",
    "\n",
    "#All weights (shared-varaibles)\n",
    "# \"trainable\" flag means not to return auxilary params like batch mean (for batch normalization)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output,trainable=True)\n",
    "print (all_weights)\n",
    "\n",
    "\n",
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "\n",
    "\n",
    "\n",
    "from lasagne.regularization import regularize_layer_params_weighted, l2, l1\n",
    "from lasagne.regularization import regularize_layer_params\n",
    "layers = {conv_1: 0.1, dense_1: 0.5}\n",
    "l2_penalty = regularize_layer_params_weighted(layers, l2)\n",
    "l1_penalty = regularize_layer_params(dense_1, l1) * 1e-4\n",
    "#loss=loss+l1_penalty#+l1_penalty\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.adamax(loss, all_weights,learning_rate=0.01)\n",
    "\n",
    "y_binary=theano.tensor.gt(y_predicted, 0.5)\n",
    "predict_fun=theano.function([input_X],y_binary)\n",
    "\n",
    "#A function that accepts X and y, returns loss functions and performs weight updates\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#A function that just computes accuracy given X and y\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)\n",
    "\n",
    "\n",
    "\n",
    "#итерации обучения\n",
    "import time\n",
    "num_epochs = 100#<how many times to iterate over the entire training set>\n",
    "\n",
    "batch_size = 50#<how many samples are processed at a single function call>\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    f_macro=0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t82.47 %\n",
      "We need more magic!\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print (\"Achievement unlocked: 80lvl Warlock!\")\n",
    "else:\n",
    "    print (\"We need more magic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report\n",
    "\n",
    "All creative approaches are highly welcome, but at the very least it would be great to mention\n",
    "* the idea;\n",
    "* brief history of tweaks and improvements;\n",
    "* what is the final architecture and why?\n",
    "* what is the training method and, again, why?\n",
    "* Any regularizations and other techniques applied and their effects;\n",
    "\n",
    "\n",
    "There is no need to write strict mathematical proofs (unless you want to).\n",
    " * \"I tried this, this and this, and the second one turned out to be better. And i just didn't like the name of that one\" - OK, but can be better\n",
    " * \"I have analized these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\" - the ideal one\n",
    " * \"I took that code that demo without understanding it, but i'll never confess that and instead i'll make up some pseudoscientific explaination\" - __not_ok__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#f1-score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_X = T.tensor3(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = (None, 39)\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int64')\n",
    "\n",
    "N_HIDDEN = 100\n",
    "# Number of training sequences in each batch\n",
    "N_BATCH = 50\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .001\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_in = lasagne.layers.InputLayer(shape=(N_BATCH, 39, 1), input_var=input_X)#len(vocab_processor.vocabulary_)\n",
    "    # The network also needs a way to provide a mask for each sequence.  We'll\n",
    "    # use a separate input layer for that.  Since the mask only determines\n",
    "    # which indices are part of the sequence for each batch entry, they are\n",
    "    # supplied as matrices of dimensionality (N_BATCH, MAX_LENGTH)\n",
    "l_forward = lasagne.layers.RecurrentLayer(\n",
    "        l_in, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "        W_in_to_hid=lasagne.init.HeUniform(),\n",
    "        W_hid_to_hid=lasagne.init.HeUniform(),\n",
    "        nonlinearity=lasagne.nonlinearities.tanh, only_return_final=True)\n",
    "l_backward = lasagne.layers.RecurrentLayer(\n",
    "        l_in, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "        W_in_to_hid=lasagne.init.HeUniform(),\n",
    "        W_hid_to_hid=lasagne.init.HeUniform(),\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        only_return_final=True, backwards=True)\n",
    "    # Now, we'll concatenate the outputs to combine them.\n",
    "l_concat = lasagne.layers.ConcatLayer([l_forward, l_backward])\n",
    "    # Our output layer is a simple dense connection, with 1 output unit\n",
    "l_out = lasagne.layers.DenseLayer(\n",
    "        l_concat, num_units=2, nonlinearity=lasagne.nonlinearities.tanh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input_to_hidden.W, input_to_hidden.b, hidden_to_hidden.W, input_to_hidden.W, input_to_hidden.b, hidden_to_hidden.W, W, b]\n"
     ]
    }
   ],
   "source": [
    "# Network predictions (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(l_out,input_X)\n",
    "\n",
    "\n",
    "#All weights (shared-varaibles)\n",
    "# \"trainable\" flag means not to return auxilary params like batch mean (for batch normalization)\n",
    "all_weights = lasagne.layers.get_all_params(l_out,trainable=True)\n",
    "print (all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "\n",
    "\n",
    "\n",
    "from lasagne.regularization import regularize_layer_params_weighted, l2, l1\n",
    "from lasagne.regularization import regularize_layer_params\n",
    "layers = {conv_1: 0.1, dense_1: 0.5}\n",
    "l2_penalty = regularize_layer_params_weighted(layers, l2)\n",
    "l1_penalty = regularize_layer_params(dense_1, l1) * 1e-4\n",
    "#loss=loss+l2_penalty#+l1_penalty\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.adagrad(loss, all_weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A function that accepts X and y, returns loss functions and performs weight updates\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#A function that just computes accuracy given X and y\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_r = X_test.reshape(len(X_test),39, 1)\n",
    "X_train_r=X_train.reshape(len(X_train),39, 1)\n",
    "X_val_r= X_val.reshape(len(X_val),39, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 1.881s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t68.27 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 2 of 100 took 1.789s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 3 of 100 took 2.312s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 4 of 100 took 2.103s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 5 of 100 took 1.885s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 6 of 100 took 2.399s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 7 of 100 took 2.045s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 8 of 100 took 1.866s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 9 of 100 took 1.759s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 10 of 100 took 1.753s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 11 of 100 took 1.734s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 12 of 100 took 1.730s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 13 of 100 took 1.906s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 14 of 100 took 1.816s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 15 of 100 took 1.842s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 16 of 100 took 1.821s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 17 of 100 took 1.854s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 18 of 100 took 1.846s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 19 of 100 took 1.868s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 20 of 100 took 1.745s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 21 of 100 took 1.743s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 22 of 100 took 1.734s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 23 of 100 took 1.735s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 24 of 100 took 1.814s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 25 of 100 took 1.849s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 26 of 100 took 1.849s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 27 of 100 took 1.835s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 28 of 100 took 1.767s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 29 of 100 took 1.736s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 30 of 100 took 1.730s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 31 of 100 took 1.740s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 32 of 100 took 1.752s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 33 of 100 took 1.722s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 34 of 100 took 1.734s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 35 of 100 took 1.767s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 36 of 100 took 1.732s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 37 of 100 took 1.730s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 38 of 100 took 1.730s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 39 of 100 took 1.742s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 40 of 100 took 1.727s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 41 of 100 took 1.736s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 42 of 100 took 1.730s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 43 of 100 took 1.728s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 44 of 100 took 1.729s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 45 of 100 took 1.729s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 46 of 100 took 1.732s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 47 of 100 took 1.932s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 48 of 100 took 2.078s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 49 of 100 took 1.983s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 50 of 100 took 2.054s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 51 of 100 took 1.831s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 52 of 100 took 2.088s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 53 of 100 took 2.121s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 54 of 100 took 2.021s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 55 of 100 took 1.734s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 56 of 100 took 1.827s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 57 of 100 took 1.736s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 58 of 100 took 1.744s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 59 of 100 took 1.745s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 60 of 100 took 1.860s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 61 of 100 took 1.768s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 62 of 100 took 1.838s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 63 of 100 took 1.807s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 64 of 100 took 1.803s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 65 of 100 took 1.833s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 66 of 100 took 1.804s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 67 of 100 took 1.837s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 68 of 100 took 1.898s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 69 of 100 took 1.799s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 70 of 100 took 1.884s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 71 of 100 took 2.135s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 72 of 100 took 1.805s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 73 of 100 took 2.231s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 74 of 100 took 2.155s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 75 of 100 took 1.813s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 76 of 100 took 1.741s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 77 of 100 took 1.738s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 78 of 100 took 1.743s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 79 of 100 took 1.740s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 80 of 100 took 1.778s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 81 of 100 took 2.008s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 82 of 100 took 1.741s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 83 of 100 took 1.744s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 84 of 100 took 1.745s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 85 of 100 took 1.804s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 86 of 100 took 1.994s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 87 of 100 took 2.334s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 88 of 100 took 2.373s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 89 of 100 took 3.007s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 90 of 100 took 2.597s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 91 of 100 took 1.845s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 92 of 100 took 1.803s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 93 of 100 took 1.788s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 94 of 100 took 1.811s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 95 of 100 took 1.810s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 96 of 100 took 1.800s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 97 of 100 took 2.069s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 98 of 100 took 2.365s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 99 of 100 took 2.008s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n",
      "Epoch 100 of 100 took 2.227s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t65.87 %\n",
      "  validation accuracy:\t\t66.65 %\n"
     ]
    }
   ],
   "source": [
    "#итерации обучения\n",
    "import time\n",
    "num_epochs = 100#<how many times to iterate over the entire training set>\n",
    "\n",
    "batch_size = 50#<how many samples are processed at a single function call>\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train_r, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val_r, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t68.10 %\n",
      "We need more magic!\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test_r, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print (\"Achievement unlocked: 80lvl Warlock!\")\n",
    "else:\n",
    "    print (\"We need more magic!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pymorphy2\n",
    "\n",
    "word_vectors = Word2Vec.load_word2vec_format('ruscorpora.model.bin', binary=True) \n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "rus = stopwords.words('russian')\n",
    "def get_word_variants(word, word_vectors):\n",
    "               \n",
    "        try:\n",
    "            return word_vectors[word]   #if 'word' in word_vectors.vocab\n",
    "        except KeyError:\n",
    "             try:\n",
    "                p = morph.parse(word)[0]\n",
    "                return word_vectors[p.normal_form]\n",
    "             except KeyError:\n",
    "                return []\n",
    "\n",
    "def clean_str_vec(string):\n",
    "        \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    \"\"\"\n",
    "        string = re.sub(r\"[^а-яА-Я0-9(),:!?\\'\\`]\", \" \", string)\n",
    "        string = re.sub(r\",\", \" , \", string)\n",
    "        string = re.sub(r\"!\", \" ! \", string)\n",
    "        string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "        string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "        string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "        string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "        return string.strip().lower()\n",
    "\n",
    "def makeVec(text):  \n",
    "        res=[]\n",
    "        text=clean_str_vec(text).split()\n",
    "        for word in text:\n",
    "            if word in rus:\n",
    "                pass\n",
    "            else:\n",
    "                res+=list(get_word_variants(word, word_vectors))\n",
    "        return (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xvec=[]\n",
    "for i in X:\n",
    "    Xvec.append(makeVec(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lenghts=[]\n",
    "for i in Xvec:\n",
    "    lenghts.append(len(i))\n",
    "maximum=max(lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(Xvec)):\n",
    "        while(len(Xvec[i])<maximum):\n",
    "            Xvec[i].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xvec=np.array(Xvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9392, 5700)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xvecre=Xvec.reshape(9392,5700,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(Xvecre, y, test_size=0.33, random_state=42)\n",
    "X_train2, X_val2 = X_train2[:-5500], X_train2[-5500:]\n",
    "y_train2, y_val2 = y_train2[:-5500], y_train2[-5500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, b, W, b, output.W, output.b]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 9.441s\n",
      "  training loss (in-iteration):\t\t2.045409\n",
      "  train accuracy:\t\t73.60 %\n",
      "  validation accuracy:\t\t78.02 %\n",
      "  train f1-score-macro:\t\t64.28 %\n",
      "  train f1-score-micro:\t\t77.33 %\n",
      "  val f1-score-macro:\t\t64.44 %\n",
      "  val f1-score-micro:\t\t78.02 %\n",
      "Epoch 2 of 100 took 9.147s\n",
      "  training loss (in-iteration):\t\t1.065911\n",
      "  train accuracy:\t\t78.53 %\n",
      "  validation accuracy:\t\t78.18 %\n",
      "  train f1-score-macro:\t\t70.45 %\n",
      "  train f1-score-micro:\t\t79.87 %\n",
      "  val f1-score-macro:\t\t62.93 %\n",
      "  val f1-score-micro:\t\t78.18 %\n",
      "Epoch 3 of 100 took 10.069s\n",
      "  training loss (in-iteration):\t\t0.808386\n",
      "  train accuracy:\t\t80.67 %\n",
      "  validation accuracy:\t\t78.93 %\n",
      "  train f1-score-macro:\t\t76.65 %\n",
      "  train f1-score-micro:\t\t83.73 %\n",
      "  val f1-score-macro:\t\t69.22 %\n",
      "  val f1-score-micro:\t\t78.93 %\n",
      "Epoch 4 of 100 took 9.774s\n",
      "  training loss (in-iteration):\t\t0.722363\n",
      "  train accuracy:\t\t84.13 %\n",
      "  validation accuracy:\t\t79.38 %\n",
      "  train f1-score-macro:\t\t80.09 %\n",
      "  train f1-score-micro:\t\t86.00 %\n",
      "  val f1-score-macro:\t\t68.51 %\n",
      "  val f1-score-micro:\t\t79.38 %\n",
      "Epoch 5 of 100 took 10.300s\n",
      "  training loss (in-iteration):\t\t0.667441\n",
      "  train accuracy:\t\t85.33 %\n",
      "  validation accuracy:\t\t80.22 %\n",
      "  train f1-score-macro:\t\t82.50 %\n",
      "  train f1-score-micro:\t\t87.47 %\n",
      "  val f1-score-macro:\t\t71.29 %\n",
      "  val f1-score-micro:\t\t80.22 %\n",
      "Epoch 6 of 100 took 9.737s\n",
      "  training loss (in-iteration):\t\t0.631453\n",
      "  train accuracy:\t\t87.87 %\n",
      "  validation accuracy:\t\t80.51 %\n",
      "  train f1-score-macro:\t\t86.53 %\n",
      "  train f1-score-micro:\t\t90.40 %\n",
      "  val f1-score-macro:\t\t71.55 %\n",
      "  val f1-score-micro:\t\t80.51 %\n",
      "Epoch 7 of 100 took 10.339s\n",
      "  training loss (in-iteration):\t\t0.593848\n",
      "  train accuracy:\t\t90.53 %\n",
      "  validation accuracy:\t\t81.02 %\n",
      "  train f1-score-macro:\t\t88.35 %\n",
      "  train f1-score-micro:\t\t91.73 %\n",
      "  val f1-score-macro:\t\t72.91 %\n",
      "  val f1-score-micro:\t\t81.02 %\n",
      "Epoch 8 of 100 took 9.956s\n",
      "  training loss (in-iteration):\t\t0.558115\n",
      "  train accuracy:\t\t93.47 %\n",
      "  validation accuracy:\t\t80.93 %\n",
      "  train f1-score-macro:\t\t92.03 %\n",
      "  train f1-score-micro:\t\t94.13 %\n",
      "  val f1-score-macro:\t\t72.87 %\n",
      "  val f1-score-micro:\t\t80.93 %\n",
      "Epoch 9 of 100 took 9.745s\n",
      "  training loss (in-iteration):\t\t0.523800\n",
      "  train accuracy:\t\t94.80 %\n",
      "  validation accuracy:\t\t81.18 %\n",
      "  train f1-score-macro:\t\t94.94 %\n",
      "  train f1-score-micro:\t\t96.13 %\n",
      "  val f1-score-macro:\t\t72.54 %\n",
      "  val f1-score-micro:\t\t81.18 %\n",
      "Epoch 10 of 100 took 10.977s\n",
      "  training loss (in-iteration):\t\t0.495940\n",
      "  train accuracy:\t\t96.00 %\n",
      "  validation accuracy:\t\t81.42 %\n",
      "  train f1-score-macro:\t\t96.25 %\n",
      "  train f1-score-micro:\t\t97.07 %\n",
      "  val f1-score-macro:\t\t72.67 %\n",
      "  val f1-score-micro:\t\t81.42 %\n",
      "Epoch 11 of 100 took 10.840s\n",
      "  training loss (in-iteration):\t\t0.469310\n",
      "  train accuracy:\t\t97.20 %\n",
      "  validation accuracy:\t\t81.40 %\n",
      "  train f1-score-macro:\t\t97.38 %\n",
      "  train f1-score-micro:\t\t98.00 %\n",
      "  val f1-score-macro:\t\t72.86 %\n",
      "  val f1-score-micro:\t\t81.40 %\n",
      "Epoch 12 of 100 took 10.279s\n",
      "  training loss (in-iteration):\t\t0.450941\n",
      "  train accuracy:\t\t97.87 %\n",
      "  validation accuracy:\t\t81.29 %\n",
      "  train f1-score-macro:\t\t98.04 %\n",
      "  train f1-score-micro:\t\t98.53 %\n",
      "  val f1-score-macro:\t\t73.74 %\n",
      "  val f1-score-micro:\t\t81.29 %\n",
      "Epoch 13 of 100 took 10.352s\n",
      "  training loss (in-iteration):\t\t0.448560\n",
      "  train accuracy:\t\t98.13 %\n",
      "  validation accuracy:\t\t80.35 %\n",
      "  train f1-score-macro:\t\t97.14 %\n",
      "  train f1-score-micro:\t\t98.00 %\n",
      "  val f1-score-macro:\t\t75.13 %\n",
      "  val f1-score-micro:\t\t80.35 %\n",
      "Epoch 14 of 100 took 10.019s\n",
      "  training loss (in-iteration):\t\t0.447483\n",
      "  train accuracy:\t\t98.40 %\n",
      "  validation accuracy:\t\t80.60 %\n",
      "  train f1-score-macro:\t\t98.58 %\n",
      "  train f1-score-micro:\t\t98.93 %\n",
      "  val f1-score-macro:\t\t69.64 %\n",
      "  val f1-score-micro:\t\t80.60 %\n",
      "Epoch 15 of 100 took 10.084s\n",
      "  training loss (in-iteration):\t\t0.458803\n",
      "  train accuracy:\t\t97.73 %\n",
      "  validation accuracy:\t\t77.95 %\n",
      "  train f1-score-macro:\t\t98.81 %\n",
      "  train f1-score-micro:\t\t99.07 %\n",
      "  val f1-score-macro:\t\t58.88 %\n",
      "  val f1-score-micro:\t\t77.95 %\n",
      "Epoch 16 of 100 took 10.179s\n",
      "  training loss (in-iteration):\t\t0.497309\n",
      "  train accuracy:\t\t95.33 %\n",
      "  validation accuracy:\t\t80.98 %\n",
      "  train f1-score-macro:\t\t96.00 %\n",
      "  train f1-score-micro:\t\t97.07 %\n",
      "  val f1-score-macro:\t\t75.08 %\n",
      "  val f1-score-micro:\t\t80.98 %\n",
      "Epoch 17 of 100 took 9.762s\n",
      "  training loss (in-iteration):\t\t0.422567\n",
      "  train accuracy:\t\t99.47 %\n",
      "  validation accuracy:\t\t81.04 %\n",
      "  train f1-score-macro:\t\t99.27 %\n",
      "  train f1-score-micro:\t\t99.47 %\n",
      "  val f1-score-macro:\t\t70.24 %\n",
      "  val f1-score-micro:\t\t81.04 %\n",
      "Epoch 18 of 100 took 10.046s\n",
      "  training loss (in-iteration):\t\t0.415504\n",
      "  train accuracy:\t\t99.07 %\n",
      "  validation accuracy:\t\t81.20 %\n",
      "  train f1-score-macro:\t\t99.67 %\n",
      "  train f1-score-micro:\t\t99.73 %\n",
      "  val f1-score-macro:\t\t71.19 %\n",
      "  val f1-score-micro:\t\t81.20 %\n",
      "Epoch 19 of 100 took 9.632s\n",
      "  training loss (in-iteration):\t\t0.390254\n",
      "  train accuracy:\t\t99.60 %\n",
      "  validation accuracy:\t\t81.15 %\n",
      "  train f1-score-macro:\t\t99.84 %\n",
      "  train f1-score-micro:\t\t99.87 %\n",
      "  val f1-score-macro:\t\t70.66 %\n",
      "  val f1-score-micro:\t\t81.15 %\n",
      "Epoch 20 of 100 took 9.777s\n",
      "  training loss (in-iteration):\t\t0.384001\n",
      "  train accuracy:\t\t99.73 %\n",
      "  validation accuracy:\t\t81.56 %\n",
      "  train f1-score-macro:\t\t99.84 %\n",
      "  train f1-score-micro:\t\t99.87 %\n",
      "  val f1-score-macro:\t\t71.94 %\n",
      "  val f1-score-micro:\t\t81.56 %\n",
      "Epoch 21 of 100 took 10.230s\n",
      "  training loss (in-iteration):\t\t0.375730\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t80.95 %\n",
      "  train f1-score-macro:\t\t100.00 %\n",
      "  train f1-score-micro:\t\t100.00 %\n",
      "  val f1-score-macro:\t\t70.14 %\n",
      "  val f1-score-micro:\t\t80.95 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-f35522de8c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mval_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mval_fscore_micro\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mval_fscore_macro\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_X = T.tensor3(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = (None,5700,1) #len(vocab_processor.vocabulary_)\n",
    "\n",
    "target_y = T.matrix(\"target Y integer\",dtype='int64')\n",
    "\n",
    "\n",
    "#Input layer (auxilary)\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "#emb_layer=lasagne.layers.EmbeddingLayer(input_layer,len(vocab_processor.vocabulary_),50)\n",
    "\n",
    "dim_layer=lasagne.layers.DimshuffleLayer(input_layer, [0,2,1])\n",
    "\n",
    "#lstm=lasagne.layers.LSTMLayer(input_layer,num_units=4)\n",
    "\n",
    "\n",
    "conv_1=lasagne.layers.Conv1DLayer(dim_layer, num_filters=3, filter_size=5)#,nonlinearity=lasagne.nonlinearities.softmax)\n",
    "pool_1=lasagne.layers.MaxPool1DLayer(conv_1, pool_size=2)\n",
    "\n",
    "\n",
    "\n",
    "#drop_1=lasagne.layers.DropoutLayer(pool_1,p=0.5)\n",
    "dense_1 = lasagne.layers.DenseLayer(pool_1,num_units=300)#,\n",
    "                                  #nonlinearity = lasagne.nonlinearities.softmax,\n",
    "#                                 name = \"selu\")\n",
    "\n",
    "dense_output = lasagne.layers.DenseLayer(dense_1,num_units = 2,\n",
    "                                        nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='output')\n",
    "\n",
    "\n",
    "# Network predictions (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(dense_output,input_X)\n",
    "\n",
    "\n",
    "#All weights (shared-varaibles)\n",
    "# \"trainable\" flag means not to return auxilary params like batch mean (for batch normalization)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output,trainable=True)\n",
    "print (all_weights)\n",
    "\n",
    "\n",
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "\n",
    "\n",
    "\n",
    "from lasagne.regularization import regularize_layer_params_weighted, l2, l1\n",
    "from lasagne.regularization import regularize_layer_params\n",
    "layers = {conv_1: 0.1, dense_1: 0.2}\n",
    "l2_penalty = regularize_layer_params_weighted(layers, l2)\n",
    "l1_penalty = regularize_layer_params(dense_1, l1) * 1e-4\n",
    "loss=loss+l1_penalty#+l1_penalty\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.adamax(loss, all_weights,learning_rate=0.01)\n",
    "\n",
    "\n",
    "\n",
    "#A function that accepts X and y, returns loss functions and performs weight updates\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#A function that just computes accuracy given X and y\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)\n",
    "\n",
    "\n",
    "y_binary=theano.tensor.gt(y_predicted, 0.5)\n",
    "predict_fun=theano.function([input_X],y_binary)\n",
    "\n",
    "\n",
    "#итерации обучения\n",
    "import time\n",
    "num_epochs = 100#<how many times to iterate over the entire training set>\n",
    "\n",
    "batch_size = 50#<how many samples are processed at a single function call>\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    f_tr_macro=0\n",
    "    f_tr_micro=0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train2, y_train2,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        #prediction_new=\n",
    "        #print(predict_fun(inputs))\n",
    "        f_tr_micro+=f1_score(targets, predict_fun(inputs), average='micro')\n",
    "        f_tr_macro+=f1_score(targets, predict_fun(inputs), average='macro')\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    val_fscore_micro=0\n",
    "    val_fscore_macro=0\n",
    "    \n",
    "    for batch in iterate_minibatches(X_val2, y_val2, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_fscore_micro+=f1_score(targets, predict_fun(inputs), average='micro')\n",
    "        val_fscore_macro+=f1_score(targets, predict_fun(inputs), average='macro')\n",
    "        val_batches += 1\n",
    "        \n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    print(\"  train f1-score-macro:\\t\\t{:.2f} %\".format(\n",
    "        f_tr_macro / train_batches * 100))\n",
    "    print(\"  train f1-score-micro:\\t\\t{:.2f} %\".format(\n",
    "        f_tr_micro / train_batches * 100))\n",
    "    print(\"  val f1-score-macro:\\t\\t{:.2f} %\".format(\n",
    "        val_fscore_macro / val_batches * 100))\n",
    "    print(\"  val f1-score-micro:\\t\\t{:.2f} %\".format(\n",
    "        val_fscore_micro / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t81.47 %\n",
      "  test f1-score-macro:\t\t72.18 %\n",
      "  test f1-score-micro:\t\t81.47 %\n",
      "We need more magic!\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "test_fscore_micro=0\n",
    "test_fscore_macro=0\n",
    "for batch in iterate_minibatches(X_test2, y_test2, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "    test_fscore_micro+=f1_score(targets, predict_fun(inputs), average='micro')\n",
    "    test_fscore_macro+=f1_score(targets, predict_fun(inputs), average='macro')\n",
    "        \n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "print(\"  test f1-score-macro:\\t\\t{:.2f} %\".format(\n",
    "        test_fscore_macro / test_batches * 100))\n",
    "print(\"  test f1-score-micro:\\t\\t{:.2f} %\".format(\n",
    "        test_fscore_micro / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print (\"Achievement unlocked: 80lvl Warlock!\")\n",
    "else:\n",
    "    print (\"We need more magic!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
